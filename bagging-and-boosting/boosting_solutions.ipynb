{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(preds: np.array, y: np.array) -> float:\n",
    "    return np.sqrt(np.mean((preds - y) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    clf,\n",
    "    X_train: np.array,\n",
    "    y_train: np.array,\n",
    "    X_test: np.array,\n",
    "    y_test: np.array,\n",
    ") -> None:\n",
    "    print(f\"train rmse: {rmse(clf.predict(X_train), y_train):.2f}\")\n",
    "    print(f\"test rmse: {rmse(clf.predict(X_test), y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the King County house sales dataset, documented here: https://rdrr.io/cran/moderndive/man/house_prices.html\n",
    "\n",
    "Our task is to regress the sale price of houses given their other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from S3 if it doesn't exist locally\n",
    "if not Path('kc_house_data.csv').exists():\n",
    "    !wget https://s3-eu-west-1.amazonaws.com/faculty-client-teaching-materials/bagging-and-boosting/kc_house_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"kc_house_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda d: pd.Series(data=[d.year, d.month, d.day])\n",
    "\n",
    "df[[\"year\", \"month\", \"day\"]] = df[\"date\"].apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"date\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop very large/ outlier house prices for ease of analysis.\n",
    "df = df.loc[df[\"price\"] <= 10 ** 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(\"price\", axis=\"columns\"), df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = partial(\n",
    "    evaluate_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Boosting by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some very simple boosting by hand. Below, you'll find code to train a simple linear model `lasso` (after some feature engineering) to predict house prices. The results aren't very good, but this is a sensible simple model to begin with- it's reasonable to assume that the price of a house should be monotonic in individual variables like the number of bedrooms/bathrooms and the square footage of the of property.\n",
    "1. Train a simple `DecisionTreeRegressor` model, `dt_res`, with `max_depth=3` to predict the residual `y_train - lasso.predict(X_train)`.\n",
    "2. Now combine the models `lasso` and `dt_res` into a class with `.fit` and `.predict` methods representing a single model whose output should be of the form `lasso(x) + dt_res(x)`. Note that the `.fit` method will have to proceed sequentially, first fitting the model `lasso` and then `dt_res` afterwards.\n",
    "3. What is the `rmse` of the combined (boosted) model `lasso + dt_res` as compared to just `lasso` or a single `DecisionTreeRegressor` trained to directly predict `y_train` from `X`?\n",
    "4. It's worth making sure that our improved results aren't because `DecisionTreeRegressor` and `Lasso` are somehow especially well suited to be combined together. Check this by repeating tasks `1-3`, but with the original `lasso` model replaced by a simple `DecisionTreeRegressor` (again with `max_depth=3`), so that you're stacking two decision tree models on top of each other. Feel free to experiment with other regression models or with a higher number of `DecisionTreeRegressor` instances as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute some extra features to help the lasso model learn more accurate monotonic relationships\n",
    "# from the data.\n",
    "def poly_features(X: np.array):\n",
    "    return np.hstack([X, X ** 2, X ** 3])\n",
    "\n",
    "\n",
    "lasso = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"variable_transformation\",\n",
    "            FunctionTransformer(poly_features, validate=False),\n",
    "        ),\n",
    "        (\"lasso_regressor\", Lasso(normalize=True, max_iter=5000)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - I should also have asked you to examine the rmse obtained\n",
    "# when this model's predictions are added to those of lasso!\n",
    "dt_res = DecisionTreeRegressor(max_depth=3).fit(\n",
    "    X_train, y_train - lasso.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"train rmse after manual boosting: {rmse(lasso.predict(X_train) + dt_res.predict(X_train), y_train)}\\n\"\n",
    "    f\"test rmse after manual boosting: {rmse(lasso.predict(X_test) + dt_res.predict(X_test), y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "class ResidualBooster:\n",
    "    def __init__(self, *clfs):\n",
    "        self.clfs = list(clfs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        training_preds = []\n",
    "        for ix, clf in enumerate(self.clfs):\n",
    "            clf.fit(X, y - np.sum(training_preds[:ix], axis=0))\n",
    "            training_preds.append(clf.predict(X_train))\n",
    "        self.training_preds = np.vstack(training_preds)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sum([clf.predict(X) for clf in self.clfs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "res_bstr = ResidualBooster(lasso, DecisionTreeRegressor(max_depth=3))\n",
    "\n",
    "res_bstr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(res_bstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(DecisionTreeRegressor(max_depth=3).fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "dt_bstr = ResidualBooster(\n",
    "    DecisionTreeRegressor(max_depth=3), DecisionTreeRegressor(max_depth=3)\n",
    ")\n",
    "dt_bstr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is worse. Add more trees!\n",
    "evaluate(dt_bstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More like it\n",
    "evaluate(\n",
    "    ResidualBooster(\n",
    "        *(DecisionTreeRegressor(max_depth=3) for _ in range(30))\n",
    "    ).fit(X_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, boosting models are almost always implemented using decision trees as base estimators because of how fast they are to train, and thanks to their applicability in a wide variety of scenarios. You won't have to do this by hand, however! Basic implementations of boosting models using decision trees for classification and regression are made available in `sklearn.ensemble` via the classes `GradientBoostingRegressor` and `GradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Check that a regressor of the form `GradientBoostingRegressor(n_estimators=2, max_depth=3, learning_rate=1)` (don't worry about what the learning rate is right now- we'll discuss this very shortly) gives results which are close to those of the hand-made boosting model consisting of two decision trees that you implemented. The results might not coincide exactly because of some implementation details for how decision trees are trained within boosting models compared to standalone `DecisionTreeRegressor` objects, but they should be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "evaluate(\n",
    "    GradientBoostingRegressor(\n",
    "        n_estimators=2, max_depth=3, learning_rate=1\n",
    "    ).fit(X_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial technique when boosting is that of *shrinkage* or *learning rate*. Given a trained model $f$, and a new model $h$ (the next boosting term) trained to predict the residual $y - f(X)$ (or to minimise a suitable cost function), the idea is to update $f$ by only a small multiple of $h$, $f + \\varepsilon\\cdot h$ (usually with $\\varepsilon$ quite small, say $\\varepsilon\\approx 0.1$ or $\\varepsilon\\approx 0.05$) rather than to perform a full update $f + h$. This is necessary, since  boosting models can overfit otherwise- error on the training set is minimised so quickly and efficiently by the boosted model that performance on the testing set has almost no chance to improve. Applying shrinkage when constructed a boosted model will often mean that more boosting iterations are required, but model performance will usually be better as a result.\n",
    "1. Add an extra boosting stage with shrinkage (use $\\varepsilon = 0.5$) to the 'hand made' boosted models that you have already constructed. That is, take the model `lasso + dt_res` that you have already trained, train a new decision tree, say `dt2`, to predict `y - lasso(X) - dt_res(X)` and then examine the performance of the model `lasso(X) + dt_res(X) + 0.5 * dt2(X)`. Do the same for the other simple sum of decision trees model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "class ResidualBooster:\n",
    "    def __init__(self, *clfs, learning_rates=None):\n",
    "        self.clfs = list(clfs)\n",
    "        self.learning_rates = learning_rates if learning_rates else repeat(1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        training_preds = []\n",
    "        for ix, clf in enumerate(self.clfs):\n",
    "            clf.fit(X, y - np.sum(training_preds[:ix], axis=0))\n",
    "            training_preds.append(clf.predict(X_train))\n",
    "        self.training_preds = np.vstack(training_preds)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        individual_preds = (clf.predict(X) for clf in self.clfs)\n",
    "        return sum(\n",
    "            (\n",
    "                lr * pred\n",
    "                for lr, pred in zip(self.learning_rates, individual_preds)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    ResidualBooster(\n",
    "        lasso,\n",
    "        DecisionTreeRegressor(max_depth=3),\n",
    "        DecisionTreeRegressor(max_depth=3),\n",
    "        learning_rates=(1, 1, 0.5),\n",
    "    ).fit(X_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    ResidualBooster(\n",
    "        *(DecisionTreeRegressor(max_depth=3) for _ in range(3)),\n",
    "        learning_rates=(1, 1, 0.5),\n",
    "    ).fit(X_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rates, overfitting, and early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted models are typically trained with hundreds or thousands of individual shallow decision trees (sometimes known as *decision stumps*). All common implementations of boosting will implement some form of shrinkage by default (both `sklearn` and `XGBoost`'s implementations default to $\\varepsilon = 0.1$)- let's see what would happen if no shrinkage was present:\n",
    "\n",
    "2. Train a `GradientBoostingRegressor` model with `max_depth=3, n_estimators=2_000` and `learning_rate=1` (so that no shrinkage is applied). Now use the method `.staged_predict` to compute the predictions made by the different stages of the model- plot a graph of the `rmse` of your model on `(X_train, y_train)` and `(X_test, y_test)`. What do you notice?\n",
    "3. Now do the same but with the default setting `learning_rate=0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "n_estimators = 2000\n",
    "gb_reg = GradientBoostingRegressor(\n",
    "    max_depth=3, n_estimators=n_estimators, learning_rate=1\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = (rmse(p, y_train) for p in gb_reg.staged_predict(X_train))\n",
    "test_preds = (rmse(p, y_test) for p in gb_reg.staged_predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, n_estimators + 1)), list(zip(train_preds, test_preds)))\n",
    "plt.legend((\"train\", \"test\"))\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "slower_gb_reg = GradientBoostingRegressor(\n",
    "    max_depth=3, n_estimators=n_estimators, learning_rate=0.1\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "train_preds = (rmse(p, y_train) for p in slower_gb_reg.staged_predict(X_train))\n",
    "test_preds = (rmse(p, y_test) for p in slower_gb_reg.staged_predict(X_test))\n",
    "\n",
    "plt.plot(list(range(1, n_estimators + 1)), list(zip(train_preds, test_preds)))\n",
    "plt.legend((\"train\", \"test\"))\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just constructed a relatively extreme example of overfitting, but it's possible to overfit more subtly without realising it.\n",
    "\n",
    "4. Repeat step 2. with the settings `max_depth=5, n_estimators=2_000, learning_rate=0.1`. At which stage of the boosting process does your model achieve its minimum `rmse` on `X_test`? How does this compare to the `rmse` on `X_test` obtained by the final stage of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "slower_but_deeper_gb_reg = GradientBoostingRegressor(\n",
    "    max_depth=5, n_estimators=n_estimators, learning_rate=0.1\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "train_preds = [\n",
    "    rmse(p, y_train) for p in slower_but_deeper_gb_reg.staged_predict(X_train)\n",
    "]\n",
    "test_preds = [\n",
    "    rmse(p, y_test) for p in slower_but_deeper_gb_reg.staged_predict(X_test)\n",
    "]\n",
    "\n",
    "plt.plot(list(range(1, n_estimators + 1)), list(zip(train_preds, test_preds)))\n",
    "plt.legend((\"train\", \"test\"))\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Boosting stage with optimal test performance: {np.argmin(test_preds)}\\n\"\n",
    "    f\"Optimal test performance: {min(test_preds):.2f}\\n\"\n",
    "    f\"Test performance at final boosting stage: {test_preds[-1]:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is simple: we didn't know when to stop training!\n",
    "* The concept of *early stopping* offers us a way around this- before training begins, we set say 20% of the training set aside to use as a validation set and we record the performance of our model on this validation set at each stage of the boosting process.\n",
    "* Tracking validation performance like this lets us know if we're overfitting and, if a certain number of iterations pass without any improvement in the validation `rmse`, we know that it's a good idea to stop training early. \n",
    "* Tuning the learning rate of your boosting algorithm can be difficult- a good strategy is to optimise other hyperparameters (such as tree structure, regularisation) with `learning_rate=0.1` or `learning_rate=0.05` so long as performance is acceptable before then simultaneously decreasing the learning rate by a factor of `0.5` whilst increasing `n_estimators` to see if a performance improvement can be obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been working with `sklearn`'s simple boosting implementation so far- let's now switch to using `XGBoost`'s implementation. `XGBoost` can be much faster than `sklearn`'s `GradientBoostingRegressor` (especially for deep trees), but it also has more convenient tooling for implementing early stopping. `XGBoost`'s `XGBRegressor` has an API that's very similar to that of `sklearn`. For instance, we can fit models which are very similar to the ones we've implemented so far as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the cost function/ objective manually in order to suppress a deprecation warning.\n",
    "xgbr = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    max_depth=3,\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "xgbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is implemented within `XGBoost` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our training data into a smaller training subset and a validation set.\n",
    "X_tr, X_ev, y_tr, y_ev = train_test_split(X_train, y_train, train_size=0.8)\n",
    "eval_set = [(X_tr, y_tr), (X_ev, y_ev)]\n",
    "# eval_metric controls the loss function with respect to which the early stopping decision is made.\n",
    "xgbr.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=300,\n",
    "    verbose=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eval_metric` is set to the root mean squared error here, which is the same as the loss function that we're training to optimise. You could in principle use a different function for `eval_metric` than your training objective, but there is very rarely a good reason to do so. `verbose` controls how often metrics are printed to `stdout` by `.fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A good boosting strategy is to train models with `n_estimators` set to a very high value (say `50_000`) with early stopping (say, `early_stopping_rounds=300`) so that the boosting process effectively goes on forever until performance on the validation set has stopped improving.\n",
    "* Note that, if you decrease the learning rate, you'll need to correspondingly increase `early_stopping_rounds`.\n",
    "* You can find out which boosting iteration gave rise to the best performance by accessing the `.best_ntree_limit` attribute, and you can access the full evaluation history of `eval_metric` across `eval_set` using the `evals_result()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train three different `XGBoostRegressor` objects with the same parameters as above, using early stopping, but with different `train_test_split` random seeds. Record the different values of `model.best_ntree_limit`, and best validation error (you can obtain this using `model.evals_result()` and `model.best_iteration`) that you obtain. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 1 - this will take a while. Approx~ 80s on a 4 core machine.\n",
    "clfs, ntree_limits, validation_scores = [], [], []\n",
    "for i in range(3):\n",
    "\n",
    "    X_tr, X_ev, y_tr, y_ev = train_test_split(X_train, y_train, train_size=0.8)\n",
    "\n",
    "    xgbr = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        max_depth=3,\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.1,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    xgbr.fit(\n",
    "        X_tr,\n",
    "        y_tr,\n",
    "        eval_set=[(X_tr, y_tr), (X_ev, y_ev)],\n",
    "        eval_metric=\"rmse\",\n",
    "        early_stopping_rounds=300,\n",
    "        verbose=None,\n",
    "    )\n",
    "\n",
    "    clfs.append(xgbr)\n",
    "    ntree_limits.append(xgbr.best_ntree_limit)\n",
    "    validation_scores.append(\n",
    "        xgbr.evals_result()[\"validation_1\"][\"rmse\"][xgbr.best_iteration]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's a decent amount of variance here!\n",
    "print(\n",
    "    f\"best_ntree_limits: {ntree_limits}\\n\"\n",
    "    f\"best validation scores: {validation_scores}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You should find that these values vary across different test train splits- this means that you'll need to use cross validation in order to be sure that your values for `.best_ntree_limit` are relatively stable.\n",
    "* One downside of early stopping is that it means that we're not 'making full use' of our training data- we have to hold some of it out for validation. A solution to this problem is to find a good value for `.best_ntree_limit` and other hyper parameters using early stopping and cross validation before then training a new model on all of the training data with `n_estimators` set to the optimal value of `.best_ntree_limit`.\n",
    "2. Train a new `XGBoostRegressor` model on all of `X_train` with `n_estimators` set to a value chosen by considering the `.best_ntree_limit` values that you obtained in step 5. Compare its performance on `X_test` to that of the models obtained in step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# Given how slow overfitting in this parameter regime seems to be here and how\n",
    "# much best_ntree_limit seems to vary, we'll be cautious and train on the highest\n",
    "# n_tree_limit that we found.\n",
    "xgbr = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    max_depth=3,\n",
    "    n_estimators=max(ntree_limits),\n",
    "    learning_rate=0.1,\n",
    "    n_jobs=-1,\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(xgbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clfs used during cross validation:\n",
    "for ix, clf in enumerate(clfs):\n",
    "    print(f\"clf {ix}:\")\n",
    "    evaluate(clf)\n",
    "    print(\"======\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
