{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a dataset to do with protein expression in mice for multiclass classification made available by the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), documented [here](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression).\n",
    "\n",
    "Dataset: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to predict the class of each observation, based on the associated protein expression levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset from S3 if it isn't already in your working directory.\n",
    "if not Path('Data_Cortex_Nuclear.xls').exists():\n",
    "     !wget https://s3-eu-west-1.amazonaws.com/faculty-client-teaching-materials/bagging-and-boosting/Data_Cortex_Nuclear.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Data_Cortex_Nuclear.xls\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no major class imbalance issues to be concerned about\n",
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a restricted set of features for our analysis. I'm not a domain expert,\n",
    "# but I think the point is that we only want to use protein expression information\n",
    "# to determine the class associated to each measurement.\n",
    "feats = [\n",
    "    c for c in df if c not in [\"Genotype\", \"Treatment\", \"Behavior\", \"class\"]\n",
    "]\n",
    "\n",
    "# Fill missing values with an indicator flag. We're not being careful about this,\n",
    "# since the purpose of the notebook is to understand algorithms rather than datasets,\n",
    "# but in a real world context, we would need to make sure that filling in missing\n",
    "# values in this way did not damage the dataset. This could occur if the distribution\n",
    "# of missing values was correlated with the class distribution within the dataset, for\n",
    "# instance.\n",
    "train, test = train_test_split(df.fillna(-1000), test_size=0.2)\n",
    "\n",
    "X_train, y_train = train[feats].values, train[\"class\"].values\n",
    "X_test, y_test = test[feats].values, test[\"class\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bagging trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a classifier consisting of bagged decision trees by hand.\n",
    "1. Using the function `np.random.choice`, train 100 different `DecisionTreeClassifier` objects on 100 different bootstrap samples of the training set (that is, samples obtained using `np.random.choice(range(len(data)), len(data), replace=True)`).\n",
    "2. Use each of these classifiers to generate class predictions for data points in the training set. You should end up with 100 sets of `len(X_test)` predictions.\n",
    "3. Now produce an aggregate/ bagged classification score for each element of the training set by majority vote from the individual `DecisionTreeClassifer` instances. The `scipy.stats` function `mode` will be helpful here.\n",
    "4. Use the function `plt.hist` to plot a histogram of the 100 classifier accuracies from your 100 individually trained `DecisionTreeClassifier` instances. How does the accuracy of bagged classifications produced in `Exercise 3` compare to the individual accuracies of its constituent `DecisionTreeClassifier` members?\n",
    "5. [Optional] Package steps 1-3 into a single class, `BaggedDT`, with `.fit` and `.predict` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` will do the above for you, via the class `sklearn.ensemble.BaggingClassifier`- you won't have to bag supervised machine learning models by hand in real life! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Reimplement `1-3` above using `BaggingClassifier`- you should obtain very similar results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Out Of Bag Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each tree in our bagged classifier was trained on a bootstrapped sample of the training data.\n",
    "* Because bootstrap samples are created by sampling with replacement, this means that not every data point will necessarily have been shown to every base estimator during training.\n",
    "* In fact, in a dataset of size N, the probability that any one data point is not included in the bootstrap sample used to train a given tree is $\\left(1 - \\frac{1}{N}\\right)^N$. So, on average, in a bagged classifier with K base estimators, a given data point will not have been shown to $K\\cdot\\left(1 - \\frac{1}{N}\\right)^N$ base estimators during training. Since $\\left(1 - \\frac{1}{N}\\right)^N\\approx e^{-1}$, we see that any given datum will not have encountered approximately $\\frac{K}{e}\\approx 0.37\\cdot K$ base estimators during training.\n",
    "* We can therefore use these $0.37\\cdot K$ base estimators to generate an essentially unbiased classification score prediction for our data point. Repeating this process N times, we can produce an essentially unbiased score for each point in the training set. Since we know the target variable for points in the training set, this lets us produce what is known as an **out of bag** (or **OOB**) estimate of our bagged classifier's performance.\n",
    "* Out of bag estimates are absolutely fantastic. Like cross validation, they let us estimate our model's generalisation accuracy (which helps us ensure that we don't overfit) using just the training that we have at hand. Unlike cross validation, however, they come for free whenever we train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Optional] Modify the code for the `BaggedDT` classifier that you have already written so that, when `BaggedDT().fit` is called, out of bag classification scores are recorded for each data point in the training set. Use this to estimate the generalisation accuracy of your classifier and compare it to the accuracy you obtain on the test set.\n",
    "* In order to record out of bag classification scores for the training data, you will need to record which data points are shown to which tree during training. One way to do this would be to use your bootstrapping code to bootstrap indices from `list(range(len(X_train))` which can then be used both to select data points from `X_train` for training trees as well as to record which poinst are selected , rather than sampling from `X_train` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, `sklearn` will do this for you and will not need to implement this by hand in the real world: simply set `oob_score=True` and then access `.oob_score_` after having fit your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use out of bag estimates to choose how many trees to use in `BaggingClassifier(DecisionTreeClassifier())`- do this by plotting a graph of a range of `n_estimators` values against `clf.oob_score_` for a classifier trained using that many estimators. You should find that adding more trees will never really make performance worse, but you will encounter diminishing returns that make it not worthwhile eventually. \n",
    "3. Use `plt.plot` to plot a graph comparing the out of bag performance estimates that you obtain this way as well as the true generalisation performance of your classifiers on the test set `(X_test, y_test)` as a function of the number of trees.\n",
    "4. How long does it take to estimate the generalisation performance of `BaggingClassifer(DecisionTreeClassifier())` using 5-fold cross validation? How does the result compare to the OOB estimate? You will want to use the function `sklearn.model_selection.cross_val_score` to implement 5-fold cross validation. Use the jupyter cell magic command %%time to estimate how long these computations take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `BaggingClassifier` objects record the individual out of bag predictions made for each data point in the training set, as well as the overall resulting accuracy. These can be accessed via the (strangely named) `clf.oob_decision_function_` attribute, and can be very useful if you want more granular information than that contained in `clf.oob_score_`.\n",
    "\n",
    "5: Use `clf.oob_decision_function_` together with `sklearn.metrics.confusion_matrix` to to display a confusion matrix computed using out of bag estimates. \n",
    "* You will need to convert the `(len(X_train), n_classes)` shape array `clf.oob_decision_function_` to a length `len(X_train)` list of predicted class labels.\n",
    "* Do this by using `np.argmax` to convert `clf.oob_decision_function_` to a an array with shape `(len(X_train),)` and then use `clf.classes_` to find out which `argmax` values to map to which class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bagging generic estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new classifier by bagging 50 logistic regression classifiers and compare its performance on the testing set with that of a single logistic regression classifier. \n",
    "    * You should find that there is essentially no performance gain - there are only theoretical reasons to believe that bagging will lead to improvements when small changes in the training data can lead to large changes in the structure of the base estimators.\n",
    "    * In fact, there are some arguments that bagging stable estimators could degrade their performance. A discussion of this as well as a simulation in support of the hypothesis features in the [original paper introducing bagging](https://www.stat.berkeley.edu/~breiman/bagging.pdf), but this isn't a phenomenon that the author of this notebook (or anyone he knows) has encountered in the wild. There isn't a good reason to bag stable classifiers, but I've never seen it drastically hurt anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random Forests as bagging + feature subselection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen so far that bagging decision trees is a very powerful technique. This works fundamentally because decision trees trained on different bootstrapped datasets find different pathways to the truth, and so aggregating their predictions results in a 'wisdom of the crowds' type of benefit. Phrased technically: the base estimator decision trees are _decorrelated_. \n",
    "* Leo Breiman's Random Forests take this notion even further by ensuring that the decision trees have access only to randomly selected **partial information** whenever they create new nodes.\n",
    "* In a Random Forest, the base estimator decision trees are made even more decorrelated than in a bagged decision tree classifier by ensuring that they not only have access to different data sets during training but also different **features** at each stage of training.\n",
    "* Partially blinding the decision trees at each stage might impair them as individual learners, but the benefit of decorrelating their predictions makes up for this: indeed, [Concordet's Jury Theorem](https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem) tells us that, if we were able to make the trees fully independent, then a Random Forest would become arbitrarily accurate as the number of trees  becomes larger.\n",
    "* Leo Breiman's original paper subsamples features at each split point by picking $\\mathrm{log}_2(F)$ features at each point, where $F$ is the total number of features available - the field has since found that the right number of feature to subsample can be problem specific. `sklearn` makes a variety of options available through the `max_features` option.\n",
    "* **Note**: the default setting for `max_features` for `RandomForestClassifier` objects in `sklearn` is a sensible default $\\mathrm{floor}(\\sqrt{F})$. However, the default `max_features` setting for `RandomForestRegressor` objects is $F$, so that no subsampling takes place! This is a mistake/ the result of a misunderstanding (as discussed [here](https://stats.stackexchange.com/questions/324370/references-on-number-of-features-to-use-in-random-forest-regression))- if you don't set this argument manaully then you'll simply be bagging decision trees regressors as opposed to training a Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a `RandomForestClassifier` on our training data. If you like, use OOB estimates (as before, you'll need to set `oob_score=True`) to choose the smallest value of `max_features` which doesn't result in degraded performance.\n",
    "2. Use the `%%timeit` Jupyter cell magic command to time how long it takes a RandomForest with 100 trees to train. Compare this to how long it takes a bagged decision tree classifier with the same number of trees to train. What do you notice?\n",
    "    * You should notice that training a RandomForest is faster: this will always be the case, since there are fewer features to consider at each node when a new split is made. This can result in substantial speed improvements when a lot of features are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest failure modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are incredibly robust- they inherit all of the advantages of decision trees (handle mixed data types, correlated features, features on very different scales, fast to train) whilst also being stable, highly performant, needing very little parameter tuning (and even then, OOB examples make this easy!), whilst also providing an estimate of their generalisation accuracy for free. If you had to pick an algorithm to use in order to achieve good performance on a supervised learning task without knowing what the dataset would be beforehand and under time pressure, a Random Forest will almost certainly be the best option. No algorithm is perfect, however, and there are some situations where Random Forests will not perform excellently:\n",
    "* Very sparse or redundant high dimensional data: if a dataset has a huge number of features and very many of them are useless, the fact that Random Forest decision trees subsample features at each node will result in very weak base estimators and a low performing forest. This is relatively easy to diagnose, however: if you obtain poor OOB performance on a training set with very many features, set `max_features=None` to reduce your Random Forest to a standard bagged decision tree estimator and see if performance improves. If this is not computationally feasible, perform dimension reduction (try Principal Component Analysis as a cheap first option) in advance.\n",
    "* Very complicated, unstructured, high dimensional datasets like images and audio: in situations like this, feature engineering is essential and so tree based methods (which don't do any feature engineering) are poorly suited to this kind of situation. Use a neural network instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add `10_000` redundant features to each point in your dataset using `np.random.normal(size=len(df) * 10_000).reshape(N, 10_000)`, before using this data to train a Random Forest in order to explore how Random Forests perform in this situation.\n",
    "* Examine performance first with the default value for `max_features` and then with `max_features=None`. Be aware that it might take a long time to fit a forest of any size when the latter option is used!\n",
    "* You can use `np.hstack` to concatenate your redundant features onto your training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
