{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>__Define Classes__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first step is to define a `KArmedBandit` class. It needs to keep track of:\n",
    "### 1] `k` : Integer $\\rightarrow$ the number of arms that the bandit has\n",
    "\n",
    "### 2] `means` == $\\mu_a$ : List of floats $\\rightarrow$ The mean reward for pulling each arm \n",
    "><font size=3>Begin by choosing these means from a normal distribution.</font>\n",
    "### 3] `stdevs` == $\\sigma_a$ : List of floats $\\rightarrow$ The standard deviations of rewards for pulling each arm\n",
    "><font size=3>To begin with, this can be set to 1 for all arms, but feel free to experiment later!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `KArmedBandit` should also have:\n",
    "### 1] A function that pulls a specified arm and returns a reward drawn from the correct distribution\n",
    "### 2] A function that returns the index of the optimal arm to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit:\n",
    "    def __init__(self, k):\n",
    "        \n",
    "        # We're going to initialise a K-Armed bandit class with k arms by attaching the value of k \n",
    "        # passed through the class constructor to the class instance.\n",
    "        self.k = k\n",
    "        \n",
    "        # Next, let's define the distributions that each arm will draw from. First, let's define the mean of\n",
    "        # each distribution by generating a corresponding list of floats, themselves drawn from a normal distribution.\n",
    "        # The index of this list then indicates which arm is being referred to\n",
    "        \n",
    "        self.means = np.random.normal(loc=0, scale=2, size=k)\n",
    "        \n",
    "        # Finally, let's define the width of each distribution. To start with, we will set the standard deviation of\n",
    "        # each arm to 1.\n",
    "        self.stdevs = [1] * k\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        \"\"\"\n",
    "        Computes the reward for pulling the specified arm by drawing from a normal distribution with the mean and\n",
    "        standard deviation corresponding to that arm as defined in the __init__ function.\n",
    "        \"\"\"\n",
    "        return np.random.normal(loc=self.means[arm], scale=self.stdevs[arm])\n",
    "\n",
    "    def optimal_arm(self):\n",
    "        \"\"\"\n",
    "        Returns the index of the optimal arm by finding the largest entry in the self.means list.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, define an `Agent` class. The agent should keep track of:\n",
    "### 1] `epsilon` : float $\\rightarrow$ Its exploration rate\n",
    "### 2] `Q` == $Q_t(a)$ : list of floats  $\\rightarrow$ The agent's current estimate of the true action value function  \n",
    "### 3] `n` == $n_a$ : list of floats $\\rightarrow$ The number of times the agent has pulled each arm\n",
    "### 4] `num_optimal_pulls` : integer $\\rightarrow$ The number of times the agent has pulled the optimal arm\n",
    "### 5] `reward_history` : list of floats $\\rightarrow$  A list which is initially empty but which is appended to at each timestep, tracking the reward received with each arm pull\n",
    "### 6] `optimal_history` : list of floats $\\rightarrow$ A list which is initially empty but which is appended to at each timestep. This variable tracks the proportion that the agent has pulled the arm with the optimal reward as the number of time steps increases. For example, if the optimal arm is 7, and the agent's first three pulls are [1,  7,  2] , then `optimal_history` should be [0, 0.5, 0.333...] after those three pulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Agent` should also have:\n",
    "\n",
    "### 1] A function to choose an arm to pull $\\epsilon$-greedily\n",
    "### 2] An `act` function, which pulls an arm on the bandit, receives a reward, and updates tracking of rewards and optimal pull %\n",
    "### 3] An `update_Q` function, which updates our estimated action-value function $Q(a)$ given a reward and the index of which arm was pulled\n",
    "><font size=4>The simplest way to do this is to keep track of the rewards assigned to each arm, but there is a more elegant solution</font>\n",
    "### 4] A `run_trial` function, which performs `act` $n_{steps}$ times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, bandit, epsilon):\n",
    "        \n",
    "        # We're going to initialise the agent so that it acts on a specific bandit.\n",
    "        self.bandit = bandit\n",
    "        \n",
    "        # We will also initialise epsilon to the value passed into the class constructor.\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # n tracks the number of times we have pulled each lever, so we initialise this to be a list of zeros with the same \n",
    "        # length as the number of arms that the specified bandit has.\n",
    "        self.n = [0]*self.bandit.k\n",
    "        \n",
    "        # When we initialise the agent we have not made any arm pulls yet so we also haven't yet pulled the optimal arm!\n",
    "        self.num_optimal_pulls = 0\n",
    "        \n",
    "        # These lists track the histories that we want to track so we initialise them as empty lists\n",
    "        self.reward_history = []\n",
    "        self.optimal_history = []\n",
    "        \n",
    "        \n",
    "        # How will you initialise your Q estimates? If we don't know anything about the rewards gained from each arm \n",
    "        # before we start then what should our prior be?\n",
    "        \n",
    "        self.Q = ?\n",
    "        \n",
    "        \n",
    "    \n",
    "    def choose_e_greedy_action(self):\n",
    "        \"\"\"\n",
    "        Chooses an arm to pull epsilon-greedily. Returns the index of the selected arm as an integer.\n",
    "        \"\"\"\n",
    "        # Need to write a function here to select an arm to pull epsilon-greedily.\n",
    "        # Remember that to do this, we sample a number from the uniform distribution between 0 and 1.\n",
    "        # If this number is less than self.epsilon, then we choose a random action (explore).\n",
    "        # Otherwise, we choose the action that we currently estimate has the highest reward by looking at Q(a) (exploit).\n",
    "        \n",
    "        selected_arm = ?\n",
    "                \n",
    "        return selected_arm\n",
    "    \n",
    "    \n",
    "    def act(self):\n",
    "        \n",
    "        # Choose an action e-greedily\n",
    "        arm = self.choose_e_greedy_action()\n",
    "        \n",
    "        # Update the array keeping track of how many times each arm has been pulled\n",
    "        self.n[arm] += 1\n",
    "        \n",
    "        # Now that you know which arm to pull, how will you get the reward? We wrote a function to do this\n",
    "        # in the bandit class\n",
    "        \n",
    "        reward = ?\n",
    "        \n",
    "        # Did the agent pull the optimal arm? If it did, we should update the variable tracking the number of times\n",
    "        # that we have pulled the optimal arm.\n",
    "        if arm == self.bandit.optimal_arm():\n",
    "            self.num_optimal_pulls += 1\n",
    "        \n",
    "        # Each time we pull an arm, we want to update our histories =>\n",
    "        \n",
    "        # Append the reward that we just received\n",
    "        self.reward_history ?\n",
    "        \n",
    "        # Compute the proportion of times that the agent has so far pulled the optimal arm and append this to\n",
    "        # the list\n",
    "        self.optimal_history ?\n",
    "\n",
    "        # Update your Q estimate\n",
    "        self.update_Q(arm, reward)\n",
    "    \n",
    "\n",
    "    # How will you update the Q values? Remember that our Q value for each arm is just the mean reward we have received from \n",
    "    # pulling that arm so far.\n",
    "    def update_Q(self, arm, reward):\n",
    "        ?\n",
    "\n",
    "    def run_trial(self, n_steps):\n",
    "        for step in range(n_steps):\n",
    "            # Which function needs to be called here?\n",
    "            self.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's run some trials!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "num_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Bandit & Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = KArmedBandit(k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(bandit, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a single trial and plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run_trial(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(agent.reward_history)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(\"Steps\", fontsize=16)\n",
    "ax.set_ylabel(\"Reward received\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(agent.optimal_history)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(\"Steps\", fontsize=16)\n",
    "ax.set_ylabel(\"Proportion of optimal arm pulls\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the bandit means to your best estimates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (true_mean, estimated_mean) in enumerate(zip(bandit.means , agent.Q)):\n",
    "    print(f\"Arm {idx}:\")\n",
    "    print(f\"True mean: {true_mean}\")\n",
    "    print(f\"Agent estimated mean: {estimated_mean}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare runs with different values of epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 1000\n",
    "epsilons_to_test = [0.01, 0.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward_history_array = []\n",
    "mean_optimal_history_array = []\n",
    "\n",
    "for eps in epsilons_to_test:\n",
    "\n",
    "    print(f\"Testing epsilon = {eps}\")\n",
    "    # Initialise containers for histories\n",
    "    reward_history_array = []\n",
    "    optimal_history_array = []\n",
    "\n",
    "    # Run num_trials trials and average the results\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "        bandit = KArmedBandit(k=k)\n",
    "        ag = Agent(bandit, eps)\n",
    "\n",
    "        ag.run_trial(num_steps)\n",
    "\n",
    "        # After each trial, add the reward and optimal % history to an array\n",
    "        reward_history_array.append(ag.reward_history)\n",
    "        optimal_history_array.append(ag.optimal_history)\n",
    "\n",
    "    # After running num_trials trials, take the mean of the histories and store them\n",
    "    # in an array\n",
    "    mean_reward_history_array.append(\n",
    "        np.mean(np.array(reward_history_array), axis=0)\n",
    "    )\n",
    "    mean_optimal_history_array.append(\n",
    "        np.mean(np.array(optimal_history_array), axis=0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for idx, eps in enumerate(epsilons_to_test):\n",
    "    ax.plot(mean_reward_history_array[idx])\n",
    "ax.legend([str(e) for e in epsilons_to_test], fontsize=16)\n",
    "ax.set_xlabel(\"Steps\", fontsize=16)\n",
    "ax.set_ylabel(\"Mean reward\", fontsize=16)\n",
    "ax.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for idx, eps in enumerate(epsilons_to_test):\n",
    "    ax.plot(mean_optimal_history_array[idx])\n",
    "ax.legend([str(e) for e in epsilons_to_test], fontsize=16)\n",
    "ax.set_xlabel(\"Steps\", fontsize=16)\n",
    "ax.set_ylabel(\"Optimal arm %\", fontsize=16)\n",
    "ax.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How do your estimates Q_t of the true action-value function q* converge over time? As a function of epsilon?\n",
    "\n",
    "### 2. Optimistic initialisation: what happens when you initialise your Q values at 5 instead of 0? why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Non-stationary q*(a): try adding a function that slightly modifies your bandit means after every time the agent acts. What happens now? (Hint: your Q_n update function should value nearer rewards to those further away. How can we achieve this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
