{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning taxi game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =4> First, let's ensure we have the OpenAI gym package.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.21.0\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Let's create an environment for the taxi game. This environment object keeps\n",
    "track its state, and provides functionality for performing actions in the environment and returning\n",
    "rewards for those actions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>The observation_space instance variable shows that there are 500 possible discrete states in this environment.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>The reset() method sets the environment to a random state and returns the index of that state.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()  # This is a useful function - you will need it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> The render() function can show us what the environmental states look like...\n",
    " - The yellow rectangle is the taxi\n",
    " - The |'s are walls\n",
    " - The :'s are road\n",
    " - The goal is to pick up someone at blue letter, then drop off at pink letter</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Note that there are 6 possible actions the taxi can take at any time step with the indices:\n",
    " - 0: down\n",
    " - 1: up\n",
    " - 2: right\n",
    " - 3: left\n",
    " - 4: pick up\n",
    " - 5: drop off</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>To sample a random action from the action space, which you will need to do, you can use the `env.action_space.sample()` method.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Experiment with taking actions...\n",
    "    \n",
    "- The argument to the `step( )` function is the index of the action you want to take\n",
    "\n",
    "- The `new_state` returned by the `step( )` function is the index of the new state we are in after performing that action. \n",
    " \n",
    "- `reward` is the reward received for taking that action\n",
    "\n",
    "- `info` and `done` are other flags that are not required for this exercise\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The argument to this function is the index of the action you want to take.\n",
    "# 0: down, 1: up, 2: left, 3: right, 4: pickup, 5: dropoff\n",
    "\n",
    "ACTION_INDEX = 0\n",
    "\n",
    "new_state, reward, done, info = env.step(ACTION_INDEX) # This function is important!\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform tabular Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Initialize the action-value function `Q(state, action)` and choose a learning rate $\\alpha$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise your Q(s,a) table. This should be a 2-D array of floats - use a numpy array for this.\n",
    "# What shape should the table be?\n",
    "# You can find the size of the state space with env.observation_space.n\n",
    "# The action space is env.action_space. Find its shape also\n",
    "# Try initialising all values to 0 at first, or play with random initialisation\n",
    "\n",
    "\n",
    "Q = ?\n",
    "\n",
    "\n",
    "# Choose a sensible learning rate\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Play the game for a certain number of episodes, updating the Q-function after each action</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of episodes to run\n",
    "n_episodes = 100000\n",
    "print_freq = 10  # logarithmic\n",
    "prev_freq = 0\n",
    "\n",
    "avg_step_count = 0\n",
    "for episode in tqdm(range(1, n_episodes + 1)):\n",
    "\n",
    "    # Initialise the environmental state by resetting the environment. \n",
    "    # Make sure to save the index of the state returned by the reset function\n",
    "    # so that you know where you are starting.\n",
    "    state = ?\n",
    "\n",
    "    # Let's create a variable to track how many steps each episode takes.\n",
    "    step_count = 0\n",
    "\n",
    "    # Let epsilon-greediness decay with episode. Try experimenting with this!\n",
    "    epsilon = 10 / episode\n",
    "\n",
    "    # Continue until taxi performs the correct pick-up and drop-off (thus earning 20 points)\n",
    "    reward = 0\n",
    "    while reward != 20:\n",
    "\n",
    "        # Choose action epsilon-greedily. You might have a function left over from the k-armed bandit \n",
    "        # notebook that can help you do this. Remember that Q(s,a) is a 2D table - when deciding\n",
    "        # which action to use when exploting, we need to pick the row of 6 Q(s,a) values corresponding \n",
    "        # to the state that the environment is currently in\n",
    "        action = ?\n",
    "\n",
    "        # Perform the action you chose, and receive the new state and reward\n",
    "        # what function should go here?\n",
    "        ? = env.?\n",
    "\n",
    "        step_count += 1\n",
    "\n",
    "        # Update action-value function according to Q-learning algorithm\n",
    "        # Refer to slide for update equation!\n",
    "\n",
    "        # Q-update equation goes here\n",
    "\n",
    "        # Update state and proceed to next action\n",
    "        state = new_state\n",
    "\n",
    "    # Track stats\n",
    "    # This will print out how many steps on average it takes your q-learning\n",
    "    # agent to complete the task.\n",
    "    if episode % print_freq == 0:\n",
    "        avg_step_count += 1 / print_freq * (step_count - avg_step_count)\n",
    "        print(\n",
    "            \"Episode: {}, Average Step Count: {:.2f}\".format(\n",
    "                episode, avg_step_count\n",
    "            )\n",
    "        )\n",
    "        avg_step_count = 0\n",
    "        prev_freq = print_freq\n",
    "        print_freq *= 10\n",
    "    else:\n",
    "        avg_step_count += (\n",
    "            1 / (episode - prev_freq) * (step_count - avg_step_count)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualise the behaviour of the agent before and after learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used for interactive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(Q, epsilon):\n",
    "    states = []\n",
    "    actions = []\n",
    "    state = env.reset()\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "        actions.append(action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        states.append(state)\n",
    "    return states, actions\n",
    "\n",
    "\n",
    "def snapshot(t):\n",
    "    if t == 0:\n",
    "        env.reset()\n",
    "        env.env.s = states[0]\n",
    "    else:\n",
    "        env.env.s = states[t - 1]\n",
    "        env.step(actions[t - 1])\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the initial random strategy in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions = run_episode(Q, 1)\n",
    "interact(\n",
    "    snapshot, t=widgets.IntSlider(min=0, max=len(states) - 1, step=1, value=0)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the learned optimal policy in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions = run_episode(Q, 0)\n",
    "interact(\n",
    "    snapshot, t=widgets.IntSlider(min=0, max=len(states) - 1, step=1, value=0)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens to the convergence rate if you randomly initialise your Q table instead of setting it to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try repeating this for a different game in the OpenAI gym! https://gym.openai.com/envs/FrozenLake8x8-v0/ is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
