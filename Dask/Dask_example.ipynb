{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask dataframe example\n",
    "\n",
    "This is an introduction to using Dask to run out-of-core computations on datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get number of CPUs allocated to your server\n",
    "NUM_CPUS = int(os.environ['NUM_CPUS'])\n",
    "print(f'Number of CPUs in server: {NUM_CPUS}')\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import CacheProfiler, ResourceProfiler, Profiler, ProgressBar, visualize\n",
    "\n",
    "# Set up default number of workers and enable global progress bar\n",
    "dask.config.set(num_workers=NUM_CPUS)\n",
    "progress_bar = ProgressBar()\n",
    "progress_bar.register()\n",
    "\n",
    "# Set up Bokeh for visualisation\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight data from USA domestic flights 2001 - 2008\n",
    "\n",
    "We will explore a dataset that contains information on every domestic flight in the USA between 2001 and 2008. Every year is a CSV file of ~600MB, so the compilation of all the years is at the level of annoying data: data that will typically not fit on a single node but is not large enough to warrant the overhead of <font size=\"3\">Big Data™</font> technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all CSVs from S3 with a wildcard into a Dask dataframe and select a few columns. Note that this is a delayed operation, the whole file has not been actually read when calling `read_csv`, but only the column names and types have been inferred from the first few lines of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = ['Year', 'Month', 'DayOfWeek', 'DepDelay',\n",
    "        'CRSDepTime', 'UniqueCarrier', 'Origin', 'Dest']\n",
    "df = dd.read_csv('s3://fellowship-teaching-materials/airline-data/200*.csv',\n",
    "                 storage_options={'anon': True}, usecols=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df` object contains the graph that dask will execute when the operation is computed. You can visualize it by calling the `visualize()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.visualize(format='svg', size='100,50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to define operations that will be run on the dataframe. As above, these commands do not execute the analysis, only define an execution graph.\n",
    "\n",
    "We build two new columns: \n",
    "- `hour`: the hour at which the flight departed\n",
    "- `delayed`: whether the flight had more than 15 minutes delay.\n",
    "    \n",
    "On this new dataframe, we perform three aggregations: delay fraction by month, delay fraction by hour, and delay fraction by carrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = (df.drop(['DepDelay', 'CRSDepTime'], axis=1)\n",
    "        .assign(hour=df.CRSDepTime.clip(upper=2399)//100,\n",
    "                delayed=(df.DepDelay.fillna(16) > 15).astype('i8')))\n",
    "# Define some aggregations to plot\n",
    "aggregations = [\n",
    "    df2.groupby('Year').delayed.mean(),\n",
    "    df2.groupby('Month').delayed.mean(),\n",
    "    df2.groupby('hour').delayed.mean(),\n",
    "    df2.groupby('UniqueCarrier').delayed.mean().nlargest(20),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the execution graph of all the above operations that will be needed to carry out the computation of delayed fraction by carrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delayed_by_carrier_dask = aggregations[3]\n",
    "delayed_by_carrier_dask.visualize(format='png', size='80,50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can perform the actual computation! On any dask object you can call `.compute()` to obtain the computed value and execute the graph. Because here we have three aggregations that will use the same data, we group them in a single call to `dask.compute`, which helps optimize the reuse of intermediate data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof, ProgressBar():\n",
    "    # Compute them all in a single pass over the data\n",
    "    (delayed_by_hour,\n",
    "     delayed_by_month,\n",
    "     delayed_by_year,\n",
    "     delayed_by_carrier) = dask.compute(*aggregations, num_workers=NUM_CPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having wrapped the execution in the `with Profiler() ... ResourceProfiler()` context manager now allows us to visualize the resource usage of the execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize([prof, rprof, cprof])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can take a look at the aggregated metrics, which are regular pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_df in (delayed_by_hour, delayed_by_month, delayed_by_year):\n",
    "    f, ax = plt.subplots()\n",
    "    result_df.sort_index(inplace=True)\n",
    "    ax.plot(result_df.index, result_df.values, '-o')\n",
    "    ax.set_title(f'Delay fraction grouped by {result_df.index.name}')\n",
    "    ax.set_xlabel(result_df.index.name)\n",
    "    ax.set_ylabel(f'Fraction {result_df.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_by_carrier.sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `EV`: Atlantic Southeast Airlines\n",
    "* `YV`: Mesa Airlines\n",
    "* `MQ`: Envoy Air\n",
    "* `AS`: Alaska Airlines\n",
    "* `DH`: Discovery Airways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other medium sized datasets\n",
    "\n",
    "\n",
    "These are some medium sized data that are good examples of when dask is needed.\n",
    "\n",
    "NYCTaxi\n",
    "------\n",
    "\n",
    "[Download link](http://www.andresmh.com/nyctaxitrips/)\n",
    "\n",
    "Taxi trips taken in 2013 released by a FOIA request.  Around 20GB CSV uncompressed.\n",
    "\n",
    "**Try the following:**\n",
    "\n",
    "*  Use `dask.dataframe` with pandas-style queries\n",
    "*  Store in HDF5 both with and without categoricals, measure the size of the file and query times\n",
    "*  Set the index by one of the date-time columns and store in castra (also using categoricals).  Perform range queries and measure speed.  What size and complexity of query can you perform while still having an \"interactive\" experience?\n",
    "\n",
    "Github Archive\n",
    "----------------\n",
    "\n",
    "[Download link](https://www.githubarchive.org/)\n",
    "\n",
    "Every public github event for the last few years stored as gzip compressed line-delimited JSON data.  Watch out, the schema switches at the 2014-2015 transition.\n",
    "\n",
    "**Try the following:**\n",
    "\n",
    "*  Use `dask.bag` to inspect the data\n",
    "*  Drill down using functions like `pluck` and `filter`\n",
    "*  Find who the most popular committers were in 2015\n",
    "\n",
    "Reddit Comments\n",
    "-----------------\n",
    "\n",
    "[Download link](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/)\n",
    "\n",
    "Every publicly available reddit comment, available as a large torrent\n",
    "\n",
    "**Try the following:**\n",
    "\n",
    "*  Use `dask.bag` to inspect the data\n",
    "*  Combine `dask.bag` with `nltk` or `gensim` to perform textual analyis on the data\n",
    "*  Reproduce the work of [Daniel Rodriguez](http://danielfrg.com/blog/2015/07/21/reproduceit-reddit-word-count-dask/) and see if you can improve upon his speeds when analyzing this data.\n",
    "\n",
    "NYC 311\n",
    "---------\n",
    "\n",
    "[Download link](https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9)\n",
    "\n",
    "All 311 service requests since 2010 in New York City\n",
    "\n",
    "European Centre for Medium Range Weather Forecasts\n",
    "----------------------------------------------------------\n",
    "\n",
    "[Download script](https://gist.github.com/mrocklin/26d8323f9a8a6a75fce0)\n",
    "\n",
    "Download historical global weather data from the ECMWF.\n",
    "\n",
    "**Try the following:**\n",
    "\n",
    "*  What is the variance in temperature over time?\n",
    "*  What areas experienced the largest temperature swings in the last month relative to their previous history?\n",
    "*  Plot the temperature of the earth as a function of latitude and then as longitude"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
