{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an MNIST Neural Net using Numpy (Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's install everything we need for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to show you how to build and train a very simple feedforward neural network to classify MNIST purely in numpy. In practice, today we use packages such as `tensorflow` or `Pytorch`, but is useful nevertheless to demystify what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import itertools\n",
    "import gzip\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mnist(dataset=\"train\", flatten=True):\n",
    "    MNIST_REMOTE = (\n",
    "        \"https://s3-eu-west-1.amazonaws.com/faculty-client-teaching-materials\"\n",
    "        \"/neural-networks/mnist.pkl.gz\"\n",
    "    )\n",
    "    response = requests.get(MNIST_REMOTE)\n",
    "    mnist = BytesIO(response.content)\n",
    "\n",
    "    with gzip.open(mnist, \"rb\") as f:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "    if \"train\" in dataset.lower():\n",
    "        images, labels = train_set\n",
    "    elif \"valid\" in dataset.lower():\n",
    "        images, labels = valid_set\n",
    "    elif \"test\" in dataset.lower():\n",
    "        images, labels = test_set\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"dataset must be 'train', 'valid' or 'test'. \"\n",
    "            \"Got '{}'\".format(dataset)\n",
    "        )\n",
    "    if not flatten:\n",
    "        images = images.reshape(-1, 28, 28)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(image, label=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    plot = ax.imshow(image.reshape(28, 28), cmap=plt.cm.gray)\n",
    "    plot.set_interpolation(\"nearest\")\n",
    "    ax.xaxis.set_ticks_position(\"top\")\n",
    "    ax.yaxis.set_ticks_position(\"left\")\n",
    "    # Ensure label 0 is not passed as False\n",
    "    if label or label == 0.0:\n",
    "        ax.set_xlabel(\"Label: {}\".format(label), size=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_mnist(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(X_train[0], Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Define the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex:** Complete the softmax and neural net prediction functions below. Recall for this simple example we're assuming a neural net that simply softmaxes its inputs i.e.\n",
    "y = softmax(W.x + b) where $ \\textrm{softmax}(z_i)= \\frac{exp(z_i)}{\\sum_j exp(z_j)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / exp_z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_prediction(x, W, b):\n",
    "    return softmax(np.dot(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Define a Loss Function for the Network to Minimise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex:** Implement the cross entropy loss $H_p(q) = -\\sum_x p(x) \\log q(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(p, q):\n",
    "    return -(p * np.log(q)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) One-Hot Encode Labels to Compare With Predictions\n",
    "The neural network takes a `28x28` pixel image (flattened to a vector of length 784) and outputs a prediction for digit shown in the image. The output is a vector of length ten, which forms a probability distribution over the possible classes `0-9` for the input image. Obviously the predicited class is just the class assigned the largest probability. \n",
    "\n",
    "But our loss function wants to compare distributions, and so expects vectors of equal length. Currently the true labels in the dataset are digits, `0`, or `1`, or ... `9`, not vectors.\n",
    "\n",
    "**Ex:** Let's encode labels as a vector, such that `3` -> `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    ohe = np.zeros(10, dtype=int)\n",
    "    ohe[label] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Gradients of Loss wrt Parameters\n",
    "During gradient descent, we 'learn' by updating the parameters W, b so as to minimise the loss function. This requires calculating the gradient of the loss function w.r.t. the parameters.\n",
    "\n",
    "I've done this bit for you (it can be done analytically in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_dw(y_true, x, y_pred):\n",
    "    \"\"\"Analytic gradient of cross-entropy with reference to matrix W.\"\"\"\n",
    "    return np.outer(x, -y_true * (1 - y_pred))\n",
    "\n",
    "\n",
    "def dloss_db(y_true, x, y_pred):\n",
    "    \"\"\"Analytic gradient of cross-entropy with reference to vector b\"\"\"\n",
    "    return -y_true * (1 - y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Network Training Loop\n",
    "\n",
    "To learn the neural network parameters, we have to\n",
    "1. Loop through the data\n",
    "2. Make a prediction for each data point \n",
    "3. Caluclate the losses and gradients for a batch of data points - note the gradient for a batch should be the average of the gradients of the points\n",
    "4. Update the network parameters given these gradients \n",
    "5. Repeat until converged (plotting metrics)\n",
    "\n",
    "**Ex:** Complete the above steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(W, b, max_iters=100000, learning_rate=0.1, batch_size=100):\n",
    "    \"\"\"Update W, b to reduce cross-entropy on using gradient descent.\"\"\"\n",
    "    W = W.copy()  # don't overwrite original parameters\n",
    "    b = b.copy()\n",
    "    grad_w = np.zeros(W.shape)  # Initialise gradients\n",
    "    grad_b = np.zeros(b.shape)\n",
    "\n",
    "    # keep track of some metrics\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    start = time.time()\n",
    "    print_freq = max_iters / 10  # print progress 10 times during learning\n",
    "\n",
    "    # learn by cycling repeatedly through data for max_iters iterations\n",
    "    X_train, y_train = read_mnist(\"train\")\n",
    "    X_val, y_val = read_mnist(\"valid\")\n",
    "\n",
    "    for i, (x, label) in enumerate(itertools.cycle(zip(X_train, y_train))):\n",
    "        # predictions\n",
    "        y_pred = neural_network_prediction(x, W, b)\n",
    "        y_true = one_hot_encode(label)\n",
    "\n",
    "        # gradients and loss\n",
    "        grad_w += dloss_dw(y_true, x, y_pred) / batch_size\n",
    "        grad_b += dloss_db(y_true, x, y_pred) / batch_size\n",
    "        total_loss += cross_entropy_loss(y_true, y_pred)\n",
    "\n",
    "        if i % batch_size == 0:\n",
    "            W -= learning_rate * grad_w\n",
    "            b -= learning_rate * grad_b\n",
    "            grad_w = np.zeros(W.shape)\n",
    "            grad_b = np.zeros(b.shape)\n",
    "\n",
    "        # Calculate error metrics\n",
    "        if i % print_freq == 0:\n",
    "            train_error, val_error = get_metrics(\n",
    "                i, total_loss, W, b, X_train, y_train, X_val, y_val, print_freq\n",
    "            )\n",
    "\n",
    "            # accumulate the metrics\n",
    "            train_errors.append(train_error)\n",
    "            val_errors.append(val_error)\n",
    "            losses.append(total_loss)\n",
    "            total_loss = 0\n",
    "\n",
    "        if i > max_iters:\n",
    "            return W, b, losses, train_errors, val_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(\n",
    "    iteration, total_loss, W, b, X_train, y_train, X_val, y_val, print_freq\n",
    "):\n",
    "    total_loss /= print_freq\n",
    "    train_error = error_rate_mnist(W, b, X_train, y_train)\n",
    "    val_error = error_rate_mnist(W, b, X_val, y_val)\n",
    "    print(\n",
    "        (\n",
    "            \"Iteration {iteration} | Loss: {loss:.4f} | \"\n",
    "            \"Train error: {train:.4f} | Validation error: {val:.4f} | \".format(\n",
    "                iteration=iteration,\n",
    "                loss=total_loss,\n",
    "                train=train_error,\n",
    "                val=val_error,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return train_error, val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate_mnist(W, b, X, y):\n",
    "    y_pred = np.argmax(neural_network_prediction(X, W, b), axis=1)\n",
    "    return np.mean(y_pred != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning(losses, train_errors, val_errors):\n",
    "    \"\"\"Plot the train/val error rate and the loss\"\"\"\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(14, 5), squeeze=True)\n",
    "\n",
    "    ax[0].plot(\n",
    "        range(len(train_errors)), train_errors, \"-o\", label=\"Training error\"\n",
    "    )\n",
    "    ax[0].plot(\n",
    "        range(len(val_errors)), val_errors, \"-o\", label=\"Validation error\"\n",
    "    )\n",
    "    ax[0].set_ylim(0, 1)\n",
    "    ax[0].legend(loc=\"upper right\")\n",
    "    ax[0].set_title(\"Error rate\")\n",
    "\n",
    "    ax[1].plot(range(len(losses)), losses, \"-o\")\n",
    "    ax[1].set_title(\"Losses\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Train Your Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.random((784, 10)) - 0.5\n",
    "b1 = np.random.random((1, 10)) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, losses, train_errors, val_errors = learn(W1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning(losses=losses, train_errors=train_errors, val_errors=val_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
