{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how a traditional machine learning approach compares to the Bayesian approach. As we will see, setting up a Bayesian model will be less straightforward. However, the predictions made with the Bayesian approach will generalise better and will be easier to inspect.\n",
    "To keep the discussion simple, we will focus in a supervised, binary classification task, where the model of choice is a Logistic regression.\n",
    "\n",
    "Before anything else, let's import all the necessary dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os                        \n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "from jax import random\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "\n",
    "NUM_CPUS = int(os.environ.get(\"NUM_CPUS\", os.cpu_count()))\n",
    "numpyro.set_host_device_count(NUM_CPUS)\n",
    "\n",
    "# set a random seed for later use\n",
    "seed = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data that we wish to model, looks as follows:\n",
    "X = np.array([\n",
    "    [  0.84,  -1.48],\n",
    "    [ -4.64,  -4.08],\n",
    "    [  1.32,  -7.64],\n",
    "    [ -3.04,  -6.64],\n",
    "    [ -8.8 , -10.48],\n",
    "    [ -0.84,   1.48],\n",
    "    [  4.64,   4.08],\n",
    "    [ -1.32,   7.64],\n",
    "    [  3.04,   6.64],\n",
    "    [  8.8 ,  10.48],\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our test set is \"every\" point in the [-15, 15]x[-15, 15] 2d space.\n",
    "x_space = np.linspace(-15, 15, num=100)\n",
    "X1, X2 = np.meshgrid(x_space, x_space)\n",
    "X_test = np.array([X1.ravel(), X2.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set is every \"pixel\" in the grid.\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(*X[:5].T, 'o', ms=12, mec='w', label='y = 0')\n",
    "ax.plot(*X[5:].T, \"o\", ms=12, mec='w', label='y = 1')\n",
    "ax.set_xticks(x_space[::])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticks(x_space[::])\n",
    "ax.set_yticklabels([])\n",
    "ax.grid()\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional ML approach: fit-predict\n",
    "Here's how you quickly solve this task using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict_proba(X_test)[:, 0] # keep the 1st column, which corresponds to label \"y=0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's going on under the hood?\n",
    "\n",
    "The class LogisticRegression  contains a hard-coded cost function given by:\n",
    "\n",
    "$$ Cost(y, x, w) = \\frac{1}{2} w\\cdot w + C\\sum_{i=1}^{n} (y_i \\log p_i) + (1 - y_i)\\log(1 - p_i)) $$\n",
    "\n",
    "where, $p = \\mathrm{logistic}(X_i\\cdot w)$. From a Bayesian point of view, the second term is saying that the data follows a Bernoulli distribution, because that term is equal to the log-likelihood of a Bernoulli distribution with proabability $p$ (convice yourself of this!).\n",
    "\n",
    "At the same time, scikit-learn has placed some regularisation in place for you (without your consent, but whatever). The default regularisation is an L2 regularisation with $C=1$ . From a Bayesian interpretaion, this is saying that the weights are expected to follow a _standard_ normal distribution (why?)\n",
    "\n",
    "The line `model.fit(X, y)` is finding the value of theta that minimizes the cost function for the given data. As we saw, from the Bayesian point of view, this value of theta is the maximum of the posterior distribution (a.k.a. the mode). We can inspect such value by looking at the model.coeffs_  attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind though, that the mode of the distribution has no special place in the bayesian framework. It's actual probability is zero, just like every other point!\n",
    "\n",
    "What happens when we do `model.predict_proba`  The name given to this method might suggest that the result you're getting is the probability of `y=0`  given the data seen so far, but that's not true (sorry to be the bringer of sad news). Such value required us to evaluate some complicated integrals which I'm sorry to say sklearn is not doing. A more appropriate name for this method would be eval_likelihood , because what is really happening is that the expression\n",
    "\n",
    "$$1 - \\mathrm{logistic}(X_{new}\\cdot w))$$\n",
    "\n",
    "is being evaluated, with $w$ being replaced by the value that minimised the cost. Let's check that this statement is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star = model.coef_.flatten()\n",
    "y_pred_manual = 1 - logistic(X_test @ w_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check:\n",
    "np.isclose(y_pred_manual, y_pred).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case, there's any confusion, I'll say it clearly: **the above answer is wrong!**. That's not the answer we are really looking for, and for it to be a valid approximation to the mathematically correct answer one would have to make some very strong assumptions. To be fair, the assumptions are not unrealistic and they often happen in practice. But the key point is that they often don't.\n",
    "\n",
    "To conclude this section, let's visualise the decision boundary predicted by the ML model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contour = ax.contourf(X1, X2, y_pred.reshape(*X1.shape), cmap='RdYlBu', levels=11)\n",
    "ax.plot(*X[:5].T, 'o', ms=12, mec='w')\n",
    "ax.plot(*X[5:].T, \"o\", ms=12, mec='w')\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "cbar.ax.set_ylabel('Probability of $y=0$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the regularisation, predictions made by a ML model are doomed to perform poorly away from the data. In this case, the decision boundary extends in a straight line despite the fact that we don't have any data in the \"edges\" of the grid. Hopefully this bothers you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian approach\n",
    "\n",
    "Rather than evaluating the likelihood with a single value of theta, the Bayesian approach aims to compute the correct answer which, as we saw in the slides, should be the average likelihood (where the average is taken over the posterior distribution). The average, is calculated via a complicated integral, which we approximate with samples using MCMC. To draw the samples, we first write our NumPyro model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: there are more \"elegant\" ways of writing the model below, \n",
    "# but I'm aiming for readibility\n",
    "def logistic_regression(X, y=None):\n",
    "    n_obs, n_dims = X.shape\n",
    "    # Let's use the same prior as the one sklearn uses:\n",
    "    w = numpyro.sample(\"w\", dist.Normal(0, 1).expand((n_dims,)))\n",
    "    \n",
    "    # This is the likelihood. The `obs` argument allows this model\n",
    "    # to be used on unseen data. But we will not cover that syntax here.\n",
    "    numpyro.sample(\"y\", dist.BernoulliLogits(X @ w), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_kwargs = dict(num_warmup=2000, num_samples=2000, num_chains=NUM_CPUS)\n",
    "mcmc = MCMC(NUTS(logistic_regression), **mcmc_kwargs)\n",
    "mcmc.run(seed, X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples are stored inside the mcmc  object. We can print a summary of the samples drew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the coefficients does not agree with the values obtained by sklearn, but they shouldn't anyway: The mean  and the mode  are different quantities.\n",
    "When we run MCMC, though, we don't just get the means. We obtain the full distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_samples = mcmc.get_samples()[\"w\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(13, 5), sharey=True, sharex=True)\n",
    "axes[0].hist(w_samples[:,0], density=True, bins=30)\n",
    "axes[1].hist(w_samples[:, 1], density=True, bins=30)\n",
    "axes[0].set_title(\"$W_1$ distribution\")\n",
    "axes[1].set_title(\"$W_2$ distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you want to see the joint distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import jointplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jointplot(*w_samples.T, alpha=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, numpyro provides a nice api for making predictions on new data once the samples are available. However, for the sake of making this notebook as didactical as possible, I'm going to manually do what numpyro would do for you.\n",
    "\n",
    "Say, for instance, that we want to make a prediction for a new point `x=(2, 0.5)`\n",
    "\n",
    "Then we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_ = (2, 0.5)\n",
    "likelihoods = []\n",
    "for w in w_samples:\n",
    "    lkhood = 1 - logistic(x_new_ @ w)\n",
    "    likelihoods.append(lkhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value we should predict is then given by np.mean(likelihoods) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do it for every point on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_y_pred = np.zeros(X_test.shape[0])\n",
    "for i, x_new in enumerate(X_test):\n",
    "    p = 1 - logistic(x_new @ w_samples.T) # resulting likelihood for every sample\n",
    "    bayesian_y_pred[i] = np.mean(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the prediction boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_title(\"Bayesian Logistic Regression\")\n",
    "contour = ax.contourf(X1, X2, bayesian_y_pred.reshape(*X1.shape), cmap='RdYlBu', levels=11)\n",
    "ax.plot(*X[:5].T, 'o', ms=12, mec='w')\n",
    "ax.plot(*X[5:].T, \"o\", ms=12, mec='w')\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "cbar.ax.set_ylabel('Average Probability of $y=0$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cool feature available to Bayesian models, is that we can also inspect the how much the predictions vary across samples -- hence serving as indicator for \"model uncertainty\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_uncertainty = np.zeros(X_test.shape[0])\n",
    "for i, x_new in enumerate(X_test):\n",
    "    p = 1 - logistic(x_new @ w_samples.T) # resulting likelihood for every sample\n",
    "    y_pred_uncertainty[i] = np.std(p) # notice the difference to previous calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_title(\"Bayesian Logistic Regression\")\n",
    "contour = ax.contourf(X1, X2, y_pred_uncertainty.reshape(*X1.shape), cmap='magma_r', levels=10)\n",
    "ax.plot(*X[:5].T, 'o', ms=12, mec='w')\n",
    "ax.plot(*X[5:].T, \"o\", ms=12, mec='w')\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "cbar.ax.set_ylabel('Std. deviation of predicted proba')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is showing the regions of space where predictions are reliable. Basically, away from the data there is too much uncertainty and you shouldn't trust the predictions. It is extremely difficult to get a similar insight from a traditional ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
