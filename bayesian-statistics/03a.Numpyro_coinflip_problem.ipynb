{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Inference via NumPyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want try out what we learned about computational inference and conduct the inferences we did in the previous notebook now with the use of sampling methods. We will be using the NumPyro library, to which we will do a short introduction before digging into the inference part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model specification in NumPyro\n",
    "\n",
    "As before we want to estimate the probability of a coin landing heads based on a number of observed flips. \n",
    "\n",
    "We assume like before a Beta-distributed prior with parameters $\\alpha=\\beta=20$.\n",
    "\n",
    "With that assumption, the model takes the form:\n",
    "\n",
    "$$\n",
    "    p(\\theta) = \\mathrm{Beta}(20, 20) \\\\\n",
    "    y_i \\sim \\mathrm{Bernoulli}(\\theta), \\quad i = 1, \\dots, N \\\\\n",
    "    p(y|N, \\theta) = \\mathrm{Binomial(y,N, \\theta)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 20\n",
    "BETA = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building a probabilistic model as a Python function just using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `np.random` to sample from distributions, e.g., from the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.beta(ALPHA, BETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(n=1):\n",
    "    theta = np.random.beta(ALPHA, BETA)  # prior\n",
    "    y = np.random.binomial(1, theta, size=n)  # likelihood\n",
    "    return theta, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or indeed from the model. We simply sample theta, then n samples of y given that theta and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpyro\n",
    "\n",
    "The corresponding model in NumPyro will look very similar, but instead of using distributions from `numpy.random` we use `numpyro.sample` statements and pass the corresponding distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "# tell numpyro to use multiple cores\n",
    "numpyro.set_host_device_count(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(n=1):\n",
    "    theta = numpyro.sample(\"theta\", dist.Beta(ALPHA, BETA))  # prior\n",
    "    y = numpyro.sample(\"y\", dist.Bernoulli(theta).expand((n,)))  # likelihood\n",
    "    return theta, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final change is that NumPyro does not maintain global random state, so we need to manually seed the model. Don't worry about why this is here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "rng_key = random.PRNGKey(42)\n",
    "\n",
    "with numpyro.handlers.seed(rng_seed=rng_key):\n",
    "    theta, y = model(n=20)\n",
    "\n",
    "theta, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the model\n",
    "\n",
    "We can make a few improvements to the model.\n",
    "\n",
    "- We will allow the user to pass in y values for the model to condition on. This ability to condition on observed data is crucial in order to make inferences about the latent variables ( $\\theta$ in this case).\n",
    "- We will replace `.expand` with a `numpyro.plate` context manager. This NumPyro primitive is another way of specifying batch dimensions inspired by [plate notation](https://en.wikipedia.org/wiki/Plate_notation) and makes for more readable code + additional useful output in the model trace. It's a construct for annotating conditionally independent variables. Within a plate context manager, sample sites will be automatically broadcasted to the size of the plate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(y=None):\n",
    "\n",
    "    n = y.shape[0] if y is not None else 20  # if you are sampling the prior\n",
    "\n",
    "    # prior\n",
    "    theta = numpyro.sample(\"theta\", dist.Beta(ALPHA, BETA))\n",
    "\n",
    "    # likelihood\n",
    "    with numpyro.plate(\"n\", n):  # think of it as range in python\n",
    "        numpyro.sample(\"y\", dist.Bernoulli(theta), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpyro lets us create neat visualizations of our probabilistic  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpyro.render_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the prior\n",
    "NumPyro provides a convenient interface for drawing multiple samples from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 4_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Predictive(model, num_samples=N_SAMPLES)\n",
    "\n",
    "rng_key, subkey = random.split(rng_key)\n",
    "prior_samples = prior(subkey)\n",
    "\n",
    "prior_samples[\"theta\"].shape, prior_samples[\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prior_samples[\"theta\"], bins=50, label=\"Prior samples\")\n",
    "plt.xlabel(\"Theta\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian inference via NumPyro\n",
    "\n",
    "Once we've built the model inference is pretty straight-forward. Let's fit the model to the data of our lazy 3-coin toss trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data described as 3 outcomes from Bernoulli trials\n",
    "y = np.array([1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import MCMC, NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = MCMC(NUTS(model), num_warmup=500, num_samples=int(N_SAMPLES / 4), num_chains=4)\n",
    "\n",
    "rng_key, subkey = random.split(rng_key)\n",
    "mcmc.run(subkey, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPyro provides a summary of your MCMC sampling, including statistics on the produced samples and an MCMC diagnostic: `r_hat`. We won't go into the details of `r_hat` here but note that `r_hat=1` indicates that our chains have converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples[\"theta\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples[\"theta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot prior and posterior histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.hist(np.array(posterior_samples[\"theta\"]), alpha=0.5, bins=50, label=\"posterior\")\n",
    "ax.hist(np.array(prior_samples[\"theta\"]), alpha=0.5, bins=50, label=\"prior\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare these histograms to the true pdf-plots from the previous notebook we notice they look very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Statistics\n",
    "\n",
    "#### A. Expecation value\n",
    "\n",
    "**Example** Compute the Monte Carlo estimate of the expectation of $\\theta$ under the posterior. For posterior samples $\\theta_i$, $i=1, ..., N$, this estimate is defined by\n",
    "\n",
    "$$\\mathbb{E}\\left[\\theta \\vert y\\right] \\approx \\frac{1}{N}\\sum_{i=1}^N \\theta_i.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = posterior_samples[\"theta\"].mean()\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Event Probabilities\n",
    "\n",
    "Compute the Monte Carlo estimate for the probability of $\\theta \\in [\\theta_1, \\theta_2]$ under the posterior. For posterior samples $\\theta_i$, $i=1, ..., N$, this estimate is defined by,\n",
    "\n",
    "$$ \\int_{\\theta_1}^{\\theta_2} p(\\theta \\vert y) \\mathrm{d} \\theta \\approx \\frac{ \\#\\{\\theta_i \\in [\\theta_1, \\theta_2]\\} }{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What's the estimated probability of the coin being fair with a tolerance of 0.01? \n",
    "\n",
    "$$\\text{Probability of fair} = \\int_{0.49}^{0.51} p(\\theta | y) d\\theta \\approx \\frac{ \\#\\{\\theta_i \\in [0.49, 0.51]\\} }{N} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples[\"theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_fair = ...\n",
    "print(proba_fair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given probability $P \\in [0,1]$ the associated posterior quantile can be approximated via posterior samples $\\theta_i$, $i=1, ..., N$ by\n",
    "\n",
    "$$ \\arg\\max_x \\left\\{ \\int_0^{x} \\ p(\\theta \\vert y) d\\theta \\le P \\right\\} \\approx \\arg\\max_x \\left\\{ \\frac{1}{N}\\sum_{i=1}^N \\chi(\\theta_i \\in [0, x]) \\le P \\right\\} $$\n",
    "\n",
    "**Exercise:** What's the 5-th and 95-th percentile estimate?\n",
    "\n",
    "(Tip: you can use ```np.quantile``` instead of trying to implement the  formula of the approximation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_5 = ...\n",
    "perc_95 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perc_5)\n",
    "print(perc_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
