{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e72d3a8",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d444ea4",
   "metadata": {},
   "source": [
    "In this notebook we show how a logistic regression compares between the traditional machine learning approach and the Bayesian approach. As we will see, setting up a Bayesian model will be less straightforward. However, the predictions made with the Bayesian approach will generalise better and will be easier to inspect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8092d27-e5ae-4ecd-8d49-54930f60ead7",
   "metadata": {},
   "source": [
    "The data that we wish to model is hard coded, centered at zero, and reads as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f0dd8-d694-46db-9e9d-2a2209dba68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [0.84, -1.48],\n",
    "        [-4.64, -4.08],\n",
    "        [1.32, -7.64],\n",
    "        [-3.04, -6.64],\n",
    "        [-8.8, -10.48],\n",
    "        [-0.84, 1.48],\n",
    "        [4.64, 4.08],\n",
    "        [-1.32, 7.64],\n",
    "        [3.04, 6.64],\n",
    "        [8.8, 10.48],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618da0f-e745-4f3a-8906-0387d6385837",
   "metadata": {},
   "source": [
    "The data looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87ca52-dae0-4198-af76-a9308ffa32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.plot(*X[:5].T, \"o\", ms=8, mec=\"w\", label=\"y = 0\")\n",
    "ax.plot(*X[5:].T, \"o\", ms=8, mec=\"w\", label=\"y = 1\")\n",
    "ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "ax.legend()\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f9d0d-95c0-45c0-bc85-25c9dc9ce4ba",
   "metadata": {},
   "source": [
    "## Conventional ML\n",
    "\n",
    "First let's fit a regular logistic regression to this data using Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defee5a8-4747-47a4-8b55-349194bfb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(fit_intercept=False, C=1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3136d1",
   "metadata": {},
   "source": [
    "#### What's going on under the hood?\n",
    "\n",
    "The class LogisticRegression contains a hard-coded cost function given by:\n",
    "\n",
    "$$ Cost(y, x, \\theta) = \\frac{1}{2} \\theta \\cdot \\theta + \\sum_{i=1}^{n} (y_i \\log p_i) + (1 - y_i)\\log(1 - p_i)) $$\n",
    "\n",
    "where, $p_i = \\mathrm{logistic}(X_i\\cdot \\theta)$. \n",
    "\n",
    "The second term is saying that the data follows a Bernoulli distribution, because that term is equal to the log-likelihood of a Bernoulli distribution with proabability $p$ (convice yourself of this!).\n",
    "\n",
    "The first term is a regularisation which Scikit-learn automatically added. The default regularisation is an L2 regularisation.\n",
    "\n",
    "The line `model.fit(X, y)` is finding the value of $\\theta$ that minimizes the cost function for the given data. The fitted parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c561af-6453-4e6b-81ac-49666835a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24b9c3",
   "metadata": {},
   "source": [
    "Keep in mind though, that a single value of $\\theta$ has no special place in the Bayesian framework. It's actual probability is zero, just like every other point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41f913-3758-4d27-9d5d-5b55364517ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_space = np.linspace(-15, 15, num=100)\n",
    "X1, X2 = np.meshgrid(x_space, x_space)\n",
    "X_test = np.array([X1.ravel(), X2.ravel()]).T\n",
    "\n",
    "y_pred = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbdec9",
   "metadata": {},
   "source": [
    "What happens when we do ```model.predict_proba```? The name given to this method might suggest that the result you're getting is the probability of y=0 given the data seen so far, but that's not true. Such value required us to evaluate some complicated integrals which Scikit-learn is not doing. A more appropriate name for this method would be `eval_likelihood`, because what is really happening is that the expression\n",
    "\n",
    "$$ \\mathrm{logistic}( X_{\\text{new}} \\cdot \\hat{\\theta})) \\tag{1}$$\n",
    " \n",
    "is being evaluated, where $ X_{\\text{new}}$ is an unseesn data point and $\\hat{\\theta}$ is the value of $\\theta$ that minimised the cost. Let's check that this statement is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62bae5",
   "metadata": {},
   "source": [
    "#### Exercise: Check that ```model.predict_proba``` evalutes equation (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_manual = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e380780",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(y_pred_manual, y_pred).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6ac16",
   "metadata": {},
   "source": [
    "Let's plot the heatmap of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebe59f-993e-46a3-be6c-d34350a79678",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "contour = ax.contourf(X1, X2, y_pred.reshape(*X1.shape), cmap=\"YlOrRd\", levels=101)\n",
    "\n",
    "# training data\n",
    "ax.plot(*X[:5].T, \"o\", ms=8, mec=\"w\")\n",
    "ax.plot(*X[5:].T, \"o\", ms=8, mec=\"w\")\n",
    "\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "cbar.ax.set_ylabel(\"Probability of $y=1$\")\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78cfd2d",
   "metadata": {},
   "source": [
    "The decision boundary is a realtively sharp straight line. The fact that on the far left or right we don't have any data point is not reflected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a29aa1c-db99-46d5-bcda-24a08f17f6ac",
   "metadata": {},
   "source": [
    "## Bayesian Logistic regression\n",
    "\n",
    "Rather than evaluating the likelihood with a single value of theta, the Bayesian approach aims to compute the correct answer which, as we saw in the slides, should be the average likelihood, where the average is taken over the posterior distribution. The average, is calculated via a complicated integral, which we approximate with samples using MCMC. \n",
    "\n",
    "We start by defining a simple, 2 parameter logistic regression model,\n",
    "\n",
    "$$ \n",
    "y_i \\sim \\text{Bernoulli}(p_i) \\\\ \n",
    "p_i = \\text{Logistic}(x_i \\cdot \\theta) \\\\\n",
    "\\theta_i \\sim \\mathcal{N}(0, 1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec2345",
   "metadata": {},
   "source": [
    "To draw the samples, we first write our model in NumPyro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34147b52",
   "metadata": {},
   "source": [
    "#### Exercise: Specify the above model in NumPyro.\n",
    "\n",
    "(Tip: You can use `dist.Normal` for the prior and `dist.BernoulliLogits(X @ theta)` for the sampling distribution, and make use of the syntax used to specify the model in the previous notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X=None, y=None):\n",
    "    n_obs, n_dims = X.shape if X is not None else (10, 2)\n",
    "    \n",
    "    # prior\n",
    "    ...\n",
    "\n",
    "    # likelihood\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d266a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from jax import random\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "# tell numpyro to use multiple cores\n",
    "numpyro.set_host_device_count(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562afc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(42)\n",
    "\n",
    "mcmc = MCMC(NUTS(model), num_warmup=1000, num_samples=500, num_chains=4)\n",
    "mcmc.run(rng_key, X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ee5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd9820",
   "metadata": {},
   "source": [
    "The mean of the coefficients does not agree with the values obtained by sklearn, but they shouldn't anyway: The mean and the mode are different quantities. When we run MCMC, though, we don't just get the means. We obtain the full distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c02f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_samples = mcmc.get_samples()[\"theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638014b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0b0b3",
   "metadata": {},
   "source": [
    "Now, numpyro provides a nice API for making predictions on new data once the samples are available. However, for the sake of making this notebook as didactical as possible, let's manually do what numpyro would do for us.\n",
    "\n",
    "Say, for instance, that we want to make a prediction for a new point `x_new`. Then we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a058d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.array([2, 0.5])\n",
    "likelihoods = []\n",
    "for theta in theta_samples:\n",
    "    lkhood = logistic(x_new @ theta)\n",
    "    likelihoods.append(lkhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531b90b",
   "metadata": {},
   "source": [
    "The value we should predict is then given by ```np.mean(likelihoods)```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ffab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd267e15",
   "metadata": {},
   "source": [
    "Let's now do it for every point on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107855e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_y_pred = np.zeros(X_test.shape[0])\n",
    "for i, x_new in enumerate(X_test):\n",
    "    p = logistic(x_new @ theta_samples.T)  # resulting likelihood for every sample\n",
    "    bayesian_y_pred[i] = np.mean(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef4e24",
   "metadata": {},
   "source": [
    "Let's look at the prediction boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "contour = ax.contourf(X1, X2, bayesian_y_pred.reshape(*X1.shape), cmap=\"YlOrRd\", levels=101)\n",
    "\n",
    "# training data\n",
    "ax.plot(*X[:5].T, \"o\", ms=8, mec=\"w\")\n",
    "ax.plot(*X[5:].T, \"o\", ms=8, mec=\"w\")\n",
    "\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "cbar.ax.set_ylabel(\"Probability of $y=1$\")\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f225f8",
   "metadata": {},
   "source": [
    "A cool feature available to Bayesian models, is that we can also inspect the how much the predictions vary across samples -- hence serving as indicator for \"model uncertainty\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69b92b",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the uncertainty predictions as the standard deviation in the predicted probabilties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0dea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_uncertainty = np.zeros(X_test.shape[0])\n",
    "for i, x_new in enumerate(X_test):\n",
    "    \n",
    "    y_pred_uncertainty[i] = ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a20021",
   "metadata": {},
   "source": [
    "Let's look at the uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "contour = ax.contourf(X1, X2, y_pred_uncertainty.reshape(*X1.shape), cmap=\"magma_r\", levels=100)\n",
    "\n",
    "# training data\n",
    "ax.plot(*X[:5].T, \"o\", ms=8, mec=\"w\")\n",
    "ax.plot(*X[5:].T, \"o\", ms=8, mec=\"w\")\n",
    "\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "cbar.ax.set_ylabel(\"Probability of $y=1$\")\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78791680",
   "metadata": {},
   "source": [
    "This is showing the regions of space where predictions are reliable. Basically, away from the data there is too much uncertainty and you shouldn't trust the predictions. It is extremely difficult to get a similar insight from a traditional ML model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
