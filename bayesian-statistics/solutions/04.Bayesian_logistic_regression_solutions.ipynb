{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e72d3a8",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d444ea4",
   "metadata": {},
   "source": [
    "In this notebook we show how a logistic regression compares between the traditional machine learning approach and the Bayesian approach. As we will see, setting up a Bayesian model will be less straightforward. However, the predictions made with the Bayesian approach will generalise better and will be easier to inspect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8092d27-e5ae-4ecd-8d49-54930f60ead7",
   "metadata": {},
   "source": [
    "The data that we wish to model is hard coded, centered at zero, and reads as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f0dd8-d694-46db-9e9d-2a2209dba68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [0.84, -1.48],\n",
    "        [-4.64, -4.08],\n",
    "        [1.32, -7.64],\n",
    "        [-3.04, -6.64],\n",
    "        [-8.8, -10.48],\n",
    "        [-0.84, 1.48],\n",
    "        [4.64, 4.08],\n",
    "        [-1.32, 7.64],\n",
    "        [3.04, 6.64],\n",
    "        [8.8, 10.48],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618da0f-e745-4f3a-8906-0387d6385837",
   "metadata": {},
   "source": [
    "The data looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87ca52-dae0-4198-af76-a9308ffa32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.plot(*X[:5].T, \"o\", ms=8, mec=\"w\", label=\"y = 0\")\n",
    "ax.plot(*X[5:].T, \"o\", ms=8, mec=\"w\", label=\"y = 1\")\n",
    "ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "ax.legend()\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f9d0d-95c0-45c0-bc85-25c9dc9ce4ba",
   "metadata": {},
   "source": [
    "## Conventional ML\n",
    "\n",
    "First let's fit a regular logistic regression to this data using Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defee5a8-4747-47a4-8b55-349194bfb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(fit_intercept=False, C=1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3136d1",
   "metadata": {},
   "source": [
    "#### What's going on under the hood?\n",
    "\n",
    "The class LogisticRegression contains a hard-coded cost function given by:\n",
    "\n",
    "$$ Cost(y, x, \\theta) = \\frac{1}{2} \\theta \\cdot \\theta - \\sum_{i=1}^{n} (y_i \\log p_i) + (1 - y_i)\\log(1 - p_i)) $$\n",
    "\n",
    "where, $p_i = \\mathrm{logistic}(X_i\\cdot \\theta)$. \n",
    "\n",
    "See logistic function here: https://en.wikipedia.org/wiki/Logistic_function\n",
    "\n",
    "The second term is saying that the data follows a Bernoulli distribution, because that term is equal to the log-likelihood of a Bernoulli distribution with probability $p$ (convice yourself of this!).\n",
    "\n",
    "The first term is a regularisation which Scikit-learn automatically added. The default regularisation is an L2 regularisation.\n",
    "\n",
    "The line `model.fit(X, y)` is finding the value of $\\theta$ that minimizes the cost function for the given data. The fitted parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c561af-6453-4e6b-81ac-49666835a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24b9c3",
   "metadata": {},
   "source": [
    "Keep in mind though, that a single value of $\\theta$ has no special place in the Bayesian framework. It's actual probability is zero, just like every other point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f94c0-d063-4fed-9764-d1a1b92f812d",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572fb9d4-97c4-4573-8c7e-fe9cc530b0c0",
   "metadata": {},
   "source": [
    "Our test set is \"every\" point in the _[-15, 15] x [-15, 15]_ 2d space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84801dfd-69da-4d89-aaa3-94c5ddf45dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_space = np.linspace(-15, 15, num=100)\n",
    "X1, X2 = np.meshgrid(x_space, x_space)\n",
    "X_test = np.array([X1.ravel(), X2.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13036b-f7cf-4a9a-af58-929a88a9eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set is every \"pixel\" in the grid.\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(*X[:5].T, 'o', ms=12, mec='w', label='y = 0')\n",
    "ax.plot(*X[5:].T, \"o\", ms=12, mec='w', label='y = 1')\n",
    "ax.set_xticks(x_space[::])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticks(x_space[::])\n",
    "ax.set_yticklabels([])\n",
    "ax.grid()\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b8a7c-2514-4e11-a83a-3052f5432f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(X_test)[:, 1] # keep the 2nd column, which corresponds to label \"y=1\"\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e4138-7931-424c-8481-c8d58103428d",
   "metadata": {},
   "source": [
    "What happens when we do ```model.predict_proba```? The name given to this method might suggest that the result you're getting is the probability of y=0 given the data seen so far, but **that's not true.**\n",
    "\n",
    "Such value required us to evaluate some complicated integrals which Scikit-learn is not doing. A more appropriate name for this method would be `eval_likelihood`, because what is really happening is that the expression\n",
    "\n",
    "$$ \\mathrm{logistic}( X_{\\text{new}} \\cdot \\hat{\\theta})) \\tag{1}$$\n",
    " \n",
    "is being evaluated, where $ X_{\\text{new}}$ is an unseen data point and $\\hat{\\theta}$ is the value of $\\theta$ that minimised the cost. Let's check that this statement is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62bae5",
   "metadata": {},
   "source": [
    "#### Check that ```model.predict_proba``` evalutes equation (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as logistic # this is the logistic sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_manual = logistic(np.dot(X_test, model.coef_.flatten()))\n",
    "np.isclose(y_pred_manual, y_pred).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6ac16",
   "metadata": {},
   "source": [
    "Let's plot the heatmap of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebe59f-993e-46a3-be6c-d34350a79678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase/decrease the number of levels to bin the probability\n",
    "levels=11\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "contour = ax.contourf(X1, X2, y_pred.reshape(*X1.shape), cmap=\"YlOrRd\", levels=levels)\n",
    "\n",
    "# training data\n",
    "ax.plot(*X[:5].T, \"o\", ms=8, mec=\"w\")\n",
    "ax.plot(*X[5:].T, \"o\", ms=8, mec=\"w\")\n",
    "\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "cbar.ax.set_ylabel(\"Probability of $y=1$\")\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225656e0",
   "metadata": {},
   "source": [
    "The decision boundary is a realtively sharp straight line. The fact that on the far left or right we don't have any data point is not reflected.\n",
    "\n",
    "Despite the regularisation, predictions made by a ML model are doomed to perform poorly away from the data. In this case, the decision boundary extends in a straight line despite the fact that we don't have any data in the \"edges\" of the grid. Hopefully this bothers you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a29aa1c-db99-46d5-bcda-24a08f17f6ac",
   "metadata": {},
   "source": [
    "## Bayesian Logistic regression\n",
    "\n",
    "Rather than evaluating the likelihood with a single value of theta, the Bayesian approach aims to compute the correct answer which, as we saw in the slides, should be the average likelihood, where the average is taken over the posterior distribution. \n",
    "\n",
    "The average, is calculated via a complicated integral, which we approximate with samples using MCMC. \n",
    "\n",
    "We start by defining a simple, 2 parameter logistic regression model, with a standard normal prior distribution on the theta parameter:\n",
    "\n",
    "<div align=\"center\"><b> prior: </b></div> \n",
    "$$ \n",
    "\\theta_i \\sim \\mathcal{N}(0, 1)\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "<div align=\"center\"><b> likelihood: </b></div> \n",
    "$$ \n",
    "y_i \\sim \\text{Bernoulli}(p_i) \\\\ \n",
    "p_i = \\text{Logistic}(x_i \\cdot \\theta) \\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec2345",
   "metadata": {},
   "source": [
    "To draw the samples, we first write our model in NumPyro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34147b52",
   "metadata": {},
   "source": [
    "#### Exercise: Specify the above model in NumPyro.\n",
    "\n",
    "(Tip: You'll need to use `dist.Normal` and `dist.BernoulliLogits`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2622359-a0c2-4925-a5f7-4662dfda928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y=None):\n",
    "    n_obs, n_dims = X.shape\n",
    "    \n",
    "    # prior\n",
    "    with numpyro.plate(\"theta_plate\", n_dims):\n",
    "        theta = numpyro.sample(\"theta\", dist.Normal(0, 1))\n",
    "\n",
    "    # likelihood\n",
    "    with numpyro.plate(\"n\", n_obs):\n",
    "        numpyro.sample(\"y\", dist.BernoulliLogits(X @ theta), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d266a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from jax import random\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "# tell numpyro to use multiple cores\n",
    "numpyro.set_host_device_count(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562afc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(42)\n",
    "\n",
    "mcmc = MCMC(NUTS(model), num_warmup=1000, num_samples=500, num_chains=4)\n",
    "mcmc.run(rng_key, X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ee5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd9820",
   "metadata": {},
   "source": [
    "The mean of the coefficients does not agree with the values obtained by sklearn, but they shouldn't anyway: The mean and the mode are different quantities. When we run MCMC, though, we don't just get the means. We obtain the full distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c02f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_samples = mcmc.get_samples()[\"theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638014b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a43ab-9495-4960-9015-ae74ffb71e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(13, 5), sharey=True, sharex=True)\n",
    "axes[0].hist(np.array(theta_samples)[:,0], density=True, bins=30)\n",
    "axes[1].hist(np.array(theta_samples)[:, 1], density=True, bins=30)\n",
    "axes[0].set_title(\"$Theta_1$ distribution\")\n",
    "axes[1].set_title(\"$Theta_2$ distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177adb56-5c14-49b4-892c-2a9e6ddaaa7c",
   "metadata": {},
   "source": [
    "Or if you want to see the joint distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e18fc4-5bf5-420e-b0c3-4c6255945da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import jointplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0145f67-4d50-4cdb-8d4d-f3c85c5965b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jointplot(*np.array(theta_samples).T, alpha=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6499ef3-6f30-4c5c-b192-5915a245d1b4",
   "metadata": {},
   "source": [
    "### New predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0b0b3",
   "metadata": {},
   "source": [
    "Now, numpyro provides a nice API for making predictions on new data once the samples are available. However, for the sake of making this notebook as didactical as possible, let's manually do what numpyro would do for us.\n",
    "\n",
    "Say, for instance, that we want to make a prediction for a new point `x_new`. \n",
    "\n",
    "We know that:\n",
    "\n",
    "$$\n",
    "p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta)p(\\theta|y)d\\theta\n",
    "$$\n",
    "\n",
    "which is basically the integral over all possible values of $\\theta$ of our posterior distribution times the likelihood of a new observation for that value of $\\theta$.\n",
    "\n",
    "\n",
    "We know we can rewrite that as the conditional mean of the likelihood function on the new data:\n",
    "\n",
    "$$\n",
    "p(\\tilde{y}|y) = \\mathbb{E}_{\\theta} (p(\\tilde{y}|\\theta)|y)\n",
    "$$\n",
    "\n",
    "which basically mean that our Monte Carlo approximation of the probability for the new prediction is simply the **mean of the likelihoods** over the samples of \\theta generated by the posterior!\n",
    "\n",
    "In other words, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a058d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.array([2, 0.5])\n",
    "likelihoods = []\n",
    "for theta in theta_samples:\n",
    "    lkhood = logistic(x_new @ theta)\n",
    "    likelihoods.append(lkhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531b90b",
   "metadata": {},
   "source": [
    "The value we should predict is then given by ```np.mean(likelihoods)```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ffab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd267e15",
   "metadata": {},
   "source": [
    "Let's now do it for every point on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107855e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_y_pred = np.zeros(X_test.shape[0])\n",
    "for i, x_new in enumerate(X_test):\n",
    "    p = logistic(x_new @ theta_samples.T)  # resulting likelihood for every sample\n",
    "    bayesian_y_pred[i] = np.mean(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef4e24",
   "metadata": {},
   "source": [
    "Let's look at the prediction boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase/decrease the number of levels to bin the probability\n",
    "levels=11\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "contour = ax.contourf(X1, X2, bayesian_y_pred.reshape(*X1.shape), cmap=\"YlOrRd\", levels=levels)\n",
    "\n",
    "# training data\n",
    "ax.plot(*X[:5].T, \"o\", ms=8, mec=\"w\")\n",
    "ax.plot(*X[5:].T, \"o\", ms=8, mec=\"w\")\n",
    "\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "cbar.ax.set_ylabel(\"Probability of $y=1$\")\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f225f8",
   "metadata": {},
   "source": [
    "A cool feature available to Bayesian models, is that we can also inspect the how much the predictions vary across samples -- hence serving as indicator for \"model uncertainty\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d11f5",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the uncertainty predictions as the standard deviation in the predicted probabilties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0dea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_uncertainty = np.zeros(X_test.shape[0])\n",
    "for i, x_new in enumerate(X_test):\n",
    "    p = logistic(x_new @ theta_samples.T)  # resulting likelihood for every sample\n",
    "    y_pred_uncertainty[i] = np.std(p)  # notice the difference to previous calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102406a4",
   "metadata": {},
   "source": [
    "Let's look at the uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase/decrease the number of levels to bin the probability\n",
    "levels=11\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "contour = ax.contourf(X1, X2, y_pred_uncertainty.reshape(*X1.shape), cmap=\"magma_r\", levels=levels)\n",
    "\n",
    "# training data\n",
    "ax.plot(*X[:5].T, \"o\", ms=8, mec=\"w\")\n",
    "ax.plot(*X[5:].T, \"o\", ms=8, mec=\"w\")\n",
    "\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\")\n",
    "cbar.ax.set_ylabel(\"Probability of $y=1$\")\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78791680",
   "metadata": {},
   "source": [
    "This is showing the regions of space where predictions are reliable. Basically, away from the data there is too much uncertainty and you shouldn't trust the predictions. It is extremely difficult to get a similar insight from a traditional ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d52c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9abb2c7244cce8e9aa0789530a7803f4d07abee4d2941ae83e506281c9cdab36"
  },
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
