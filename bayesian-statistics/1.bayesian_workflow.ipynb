{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the three steps of the Bayesian workflow in the context of the coin tossing example. More precisely, we have a coin and wish to determine the probability $\\theta$ of getting heads by flipping it $n$ times and observing $y$ heads. Let us assume that we were lazy and only tossed the coin 3 times. Every time we observed head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HEADS = 3\n",
    "N_TRIALS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining a joint probability model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior\n",
    "\n",
    "Let’s suppose we think all values of $\\theta$ are equally likely, then our prior is simple a uniform distribution, i.e., $\\theta \\sim \\text{Unif}(0,1)$. \n",
    "\n",
    "<!-- The associated probability density function (pdf) is,  -->\n",
    "\n",
    "<!-- $$ p(\\theta) = \\begin{cases} \n",
    "      1 & 0\\leq \\theta \\leq 1 \\\\\n",
    "      0 & \\text{otherwise}. \n",
    "   \\end{cases}$$ -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(theta):\n",
    "    return uniform.pdf(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling distribution\n",
    "\n",
    "If the probability of heads is known, then the probability of $y$-times heads in $n$ flips is described by the binomial distribution, i.e., $y \\sim \\text{Binomial}(\\theta, n)$. \n",
    "<!-- The associated probability density function is, -->\n",
    "\n",
    "<!-- $$ p(y \\vert n, \\theta) = {n \\choose y} \\theta^y (1 - \\theta)^{(n - y)}.$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(theta, y=N_HEADS, n=N_TRIALS):\n",
    "    return binom.pmf(y, n, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conditioning on the observed data\n",
    "\n",
    "We want to evaluate the distribution of parameters given observed data\n",
    "\n",
    "$$p(\\theta | y) $$\n",
    "\n",
    "This is known as the posterior distribution and represents our beliefs about the parameters after having observed the data.\n",
    "Bayes’ rule tells us how to calculate the posterior from the sampling distribution and prior\n",
    "\n",
    "$$\n",
    "p(\\theta | y)  \\propto p(\\theta)p(y|\\theta)  \\propto \\theta^y (1-\\theta)^{(n-y)}\\label{eq1}\\tag{1}\n",
    "$$\n",
    "\n",
    "The right hand side has the form of an unnormalised Beta distribution, so we can immediately jump to the answer, \n",
    "\n",
    "$$\n",
    "\\theta|y \\sim \\text{Beta}(y+1, n-y+1).\\label{eq2}\\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition for Beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph plots the density of Beta($\\alpha$, $\\beta$) where $\\alpha$ and $\\beta$ are controlled by input fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import numpy as np\n",
    "from scipy.stats import beta as beta_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# turn off interactive mode so plots aren't auto-displayed\n",
    "plt.ioff()\n",
    "out = widgets.Output()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "x = np.linspace(0, 1, 1001)\n",
    "y = beta_dist.pdf(x, 5, 5)\n",
    "(line,) = ax.plot(x, y)\n",
    "# invisible point to make sure y axis always starts at zero\n",
    "ax.plot(0, 0, alpha=0)\n",
    "ax.set_xlabel(\"x\", fontsize=16)\n",
    "ax.set_ylabel(\"Density\", fontsize=16)\n",
    "\n",
    "\n",
    "def update_plot(alpha, beta):\n",
    "    line.set_ydata(beta_dist.pdf(x, alpha, beta))\n",
    "    # recompute the ax.dataLim\n",
    "    ax.relim()\n",
    "    # update ax.viewLim using the new dataLim\n",
    "    ax.autoscale_view()\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        display(f)\n",
    "\n",
    "\n",
    "def make_input(desc):\n",
    "    return widgets.BoundedIntText(\n",
    "        value=5,\n",
    "        min=0,\n",
    "        max=1000,\n",
    "        step=1,\n",
    "        description=desc,\n",
    "    )\n",
    "\n",
    "\n",
    "widgets.interact(update_plot, alpha=make_input(\"Alpha\"), beta=make_input(\"Beta\"))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close figure window so that new plots won't show previous results\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the posterior\n",
    "\n",
    "**Exercise:** Define a function that computes the posterior pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(theta, y=N_HEADS, n=N_TRIALS):\n",
    "    return beta_dist.pdf(theta, y + 1, n - y + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot prior, likelihood and posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_vals = np.linspace(0, 1, 100)\n",
    "prior_vals = prior(theta_vals)\n",
    "likelihood_vals = likelihood(theta_vals)\n",
    "posterior_vals = posterior(theta_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_vals, prior_vals, label=\"Prior\")\n",
    "plt.plot(theta_vals, likelihood_vals, label=\"Likelihood\")\n",
    "plt.plot(theta_vals, posterior_vals, label=\"Posterior\")\n",
    "plt.xlabel(\"Theta\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the posterior is massively skewed towards 1, following the tendency of the likelihood. The value of $\\theta$ that maximises the posterior is the same that maximises the likelihood, since we assumed a uniform prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to frequentist approach\n",
    "\n",
    "Despite having observed only heads, our posterior still allows for the probability of tails being greater than 0, which seems sensible. How does this compare to the frequentist approach?\n",
    "\n",
    "The MLE is a classic frequentist estimate: it is the parameter value that maximizes the likelihood $p(y \\vert \\theta)$ (the $n$ for number of trials is omitted), that is,\n",
    "\n",
    "$$\\hat\\theta_{\\text{MLE}} =  \\arg \\max_{\\theta} p(y \\vert \\theta) $$\n",
    "\n",
    "From the likelihood plot we see that $\\hat\\theta_{\\text{MLE}} = 1$, hence suggesting to estimate the outcome of any toin toss to always be heads! This is an extreme example, but in general small sample sizes can lead to bad inferences. Moderating our inferences with a well chosen prior can protect against this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate model fit and then assess the implications\n",
    "\n",
    "Now what we have the posterior, we can compute means, quantiles and more. But first let us check whether we are happy with our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the probability of $\\theta|y > 0.5$.\n",
    "\n",
    "(Tip: you can use ```quad``` to integrate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result, numerical_error = quad(posterior, 0, 0.5)\n",
    "print(1.0 - result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a mighty large probability given we had only 3 observations! Are we really confident we should trust this model? In reality we might be pretty sure that the probability of heads is close to $1/2$ even without flipping the coin. We can build this belief into the model by changing the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine prior\n",
    "\n",
    "But by what choice should we replace our current prior? We know that $\\theta$ is bounded to $[0,1]$. The only distribution we know that satisfies this is the Beta distribution (with the uniform distribution being the special case of $\\alpha=\\beta=1$). Since we know that most coins are fairly symmetric (hence fair), we are going to choose a Beta distribution with mean at $1/2$.\n",
    "\n",
    "But how concentrated around 0.5 do we want it to be? Surely more than for the uniform distribution since we already tried that, and weren't satisfies with our model. \n",
    "\n",
    "**Exercise:** Play around with the parameters of $\\alpha$ and $\\beta$ in Section 2.1 until you find a distribution that captures what you think it's true about $\\theta$. Hint: For it to have mean 0.5, you will need both parameters to be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 20\n",
    "BETA = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_prior(theta, alpha=ALPHA, beta=BETA):\n",
    "    return beta_dist(alpha, beta).pdf(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute posterior\n",
    "\n",
    "It turns out that if we replace the uniform prior in equation (1) by Beta$(\\alpha, \\beta)$, the posterior is still Beta-distributed. More precisely,\n",
    "\n",
    "$$\n",
    "\\theta|y \\sim \\text{Beta}(y+\\alpha, n-y+\\beta).\n",
    "$$\n",
    "\n",
    "Note that formula (1) can be seen as a special case of this (for when $\\alpha=\\beta=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function which computes the posterior pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_posterior(theta, y=N_HEADS, n=N_TRIALS, alpha=ALPHA, beta=BETA):\n",
    "    return beta_dist(y + alpha, n - y + beta).pdf(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prior_vals = new_prior(theta_vals)\n",
    "new_posterior_vals = new_posterior(theta_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_vals, new_prior_vals, label=\"Prior\")\n",
    "plt.plot(theta_vals, likelihood_vals, label=\"Likelihood\")\n",
    "plt.plot(theta_vals, new_posterior_vals, label=\"Posterior\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Theta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior is still skewed to 1 due to the likelihood, but now the prior has a considerably higher influence. Let us check whether the model fits our assumptions better than the previous one. Let us compute the probability of $\\theta|y > 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result, numerical_error = quad(new_posterior, 0, 0.5)\n",
    "print(1.0 - result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already considerably better than the previous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Statistics\n",
    "\n",
    "Now that we are happy with our model we want to compute some statistics which describe $\\theta$'s distribution.\n",
    "\n",
    "#### A. Expecation value\n",
    "\n",
    "**Exercise:** Compute the expected value of $\\theta$ under the posterior. Note that this is defined by\n",
    "\n",
    "$$\\mathbb{E}\\left[\\theta \\vert y\\right] = \\int_0^{\\infty} \\theta\\ p(\\theta \\vert y) d\\theta $$\n",
    "\n",
    "(Tip: you can do the integration by using ```quad```.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = quad(\n",
    "    lambda theta: theta * new_posterior(theta),\n",
    "    0,\n",
    "    np.inf,\n",
    ")[0]\n",
    "\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Event Probabilities\n",
    "\n",
    "The probability of $\\theta \\in [\\theta_1, \\theta_2]$ under the posterior can be computed by\n",
    "\n",
    "$$ \\int_{\\theta_1}^{\\theta_2} p(\\theta \\vert y) \\mathrm{d} \\theta.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to find out what the chances are that the coin is fair, given our observations. But what do we mean by fair? In practice we know that there's no such thing as a fair coin. Every coin will have some minor imperfections that make it not symmetric. We usually do not care about those imperfections since we have some \"error tolerance\". More technically: Being a continuous variable, there is 0% probability that $\\theta$ is exactly $0.5$. What we should care about is the probability mass around a neighbourhood of 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Set a tolerance and evaluate the probability of $\\theta$ is fair within your tolerance. \n",
    "\n",
    "(Tip: this requires again integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tolerance = 0.01\n",
    "result, numerical_error = quad(new_posterior, 0.5 - tolerance, 0.5 + tolerance)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given probability $P \\in [0,1]$ the associated posterior quantile is defined by\n",
    "\n",
    "$$ \\arg\\max_x \\left\\{ \\int_0^{x} \\ p(\\theta \\vert y) d\\theta \\le P \\right\\}$$\n",
    "\n",
    "**Exercise:** Define a function which computes the posterior quantiles. Use it to compute the 5-th and 95-th percentile.\n",
    "\n",
    "(Tip: You can use the inverse cdf method ```.ppf``` from scipy's Beta distribution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_function(p, y=N_HEADS, n=N_TRIALS, alpha=ALPHA, beta=BETA):\n",
    "    return beta_dist(y + alpha, n - y + beta).ppf(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantile_function(0.05))\n",
    "print(quantile_function(0.95))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
