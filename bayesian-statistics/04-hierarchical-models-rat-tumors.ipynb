{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical models\n",
    "\n",
    "Often we find ourselves working with hierarchical data: broadly speaking a collection of observations, which can be categorized as belonging to one of a number of groups. The cancer deaths by county data discussed in the slides is an example.\n",
    "\n",
    "The question is, how best to build a model in such a situation? There are two obvious approaches:\n",
    "  1. Combine all of the data and build a single model used to make predictions / inferences for every group (pooling).\n",
    "  2. Model each group independently (no pooling).\n",
    "\n",
    "The first approach is not really satisfactory because we lose group level information, we don't really want to model all groups as the same. The second approach is not great either because we often run into small data problems when dealing with each group independently.\n",
    "\n",
    "The ideal approach would be a hybrid one where we build seperate models for each group, but ones which aren't totally independent. I.e. they produce predictions and inferences specific to that group, but they learn from each other as well. \n",
    "\n",
    "Let's look at a particular example. We have historical data on the rate of tumor incidence in control groups of lab rats. We want to estimate the probability of a lab rate developing tumors in order to better be able to measure whether a given drug has preventative power. We are particularly interested in comparing the most recent experiment to the others, where 4 tumours were observed in a group of size 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystan\n",
    "import seaborn as sns\n",
    "\n",
    "%load_ext jupyterstan\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path('rat_tumors.csv').exists():\n",
    "    !wget https://s3-eu-west-1.amazonaws.com/faculty-client-teaching-materials/bayesian-statistics/rat_tumors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rat_tumors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just had one control group, we would model this in the same way that we modelled the Paris births data, i.e.\n",
    "\n",
    "$$\n",
    "    y \\: | \\: \\theta \\sim \\text{Binomial}(n, \\theta) \\\\\n",
    "    \\theta \\sim \\text{Beta}(\\alpha, \\beta)\n",
    "$$\n",
    "\n",
    "where $n$ is the number of rats, $y$ is the number of rats that develop a tumour, and $\\theta$ is the probability of a given rat developing a tumor (which we model as independent, identically distributed events).\n",
    "\n",
    "There's a big unanswered question here, how do we choose $\\alpha$ and $\\beta$? I certainly have no idea what a reasonable prior is on the probability of a rat developing a tumour. Unlike the Paris births example where we had enough data that our choice of prior didn't really matter, in this case the data is small (about 20 rats in each control group), so our inferences here are much more sensitive to the choice we make.\n",
    "\n",
    "To resolve this, we use the fact that with 71 groups in our data, we can learn this prior from the data! Nothing comes for free, we now need to specify a hyperprior on the parameters of our prior. However, this hyperprior applies to all of the data (~1700 observations) and so our inferences are much less senstive to the choice we make here. Hence we can safely choose a relatively flat, non-informative prior. Here is our new model, with $j = 1, \\dots, 70$ indexing the groups.\n",
    "\n",
    "$$\n",
    "    y_j \\: | \\: \\theta_j\\sim \\text{Binomial}(n_j, \\theta_j) \\\\\n",
    "    \\theta_j \\: | \\: \\alpha, \\: \\beta \\sim \\text{Beta}(\\alpha, \\beta) \\\\\n",
    "    \\alpha, \\beta \\sim \\text{Half-Cauchy}(0, 2.5)\n",
    "$$\n",
    "\n",
    "We specify this model in Stan as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%stan tumour_model\n",
    "data {\n",
    "  int<lower=0> J;  // number of groups\n",
    "  int<lower=0> n[J];  // number of rats in each group\n",
    "  int<lower=0> y[J];  // number of rats that developed tumours\n",
    "}\n",
    "parameters {\n",
    "  real<lower=0> a;\n",
    "  real<lower=0> b;\n",
    "  vector<lower=0, upper=1>[J] theta;  // tumour incidence rates\n",
    "}\n",
    "model {\n",
    "  // hyperprior\n",
    "  a ~ cauchy(0, 2.5);\n",
    "  b ~ cauchy(0, 2.5);\n",
    "  \n",
    "  // prior (vectorised assignement)\n",
    "  theta ~ beta(a, b);\n",
    "  \n",
    "  // sampling distribution\n",
    "  for (j in 1:J) {\n",
    "    y[j] ~ binomial(n[j], theta[j]);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our model has compiled we can fit it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"J\": df.shape[0],\n",
    "    \"n\": df.N,\n",
    "    \"y\": df.y,\n",
    "}\n",
    "\n",
    "fit = tumour_model.sampling(data=data, iter=2000, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in the tumour incidence rates in each group, i.e. $\\theta_j$, which we can extract with the `extract` method of `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = fit.extract()[\"theta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use seaborn to estimate the density of the posterior probability distribution for each $\\theta_j$ from the samples. We plot the estimated density of the most recent experiment in red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "for i in np.random.randint(0, 70, 20):\n",
    "    sns.distplot(theta[:, i], hist=False, ax=ax, color=\"#9C9C9C\")\n",
    "\n",
    "sns.distplot(theta[:, 70], hist=False, ax=ax, color=\"#FA7268\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the posterior distribution of the death rate for the most recent group puts the death rate relatively high, but not unusually so. In fact the posterior mean is much lower than the sample rate (maximum likelihood estimate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle = df.loc[70, \"y\"] / df.loc[70, \"N\"]\n",
    "pm = theta[:, 70].mean()\n",
    "\n",
    "print(f\"MLE of tumour rate for group 70: {mle:.3f}\")\n",
    "print(f\"Posterior mean of tumour rate for group 70: {pm:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the 95% posterior intervals and posterior means for all groups to their sample rates. Notice that there is some regression to the mean as the extreme outcomes are pulled towards the centre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = np.percentile(theta, [2.5, 97.5], axis=0)\n",
    "means = np.mean(theta, axis=0)\n",
    "sample_rates = df.y / df.N\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.plot([0, 0.4], [0, 0.4], \"k--\", linewidth=0.5)\n",
    "\n",
    "ax.scatter(sample_rates[:70], means[:70], color=\"#0099ff\", s=10)\n",
    "\n",
    "for i in range(70):\n",
    "    ax.plot(\n",
    "        [sample_rates[i], sample_rates[i]],\n",
    "        [intervals[0, i], intervals[1, i]],\n",
    "        \"--\",\n",
    "        color=\"#0099ff\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "\n",
    "ax.scatter(sample_rates[70], means[70], color=\"#ff0098\", s=12)\n",
    "ax.plot(\n",
    "    [sample_rates[70], sample_rates[70]],\n",
    "    [intervals[0, 70], intervals[1, 70]],\n",
    "    \"--\",\n",
    "    color=\"#ff0098\",\n",
    "    linewidth=0.7,\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Sample rate\", fontdict={\"fontsize\": \"14\"})\n",
    "ax.set_ylabel(\"Posterior sample rate\", fontdict={\"fontsize\": \"14\"})\n",
    "ax.set_title(\"Posterior means and 95 intervals for tumour incidence rate\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the small sample sizes correspond to wider credible intervals, reflecting the greater uncertainty associated to those groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit\n",
    "\n",
    "This notebook makes extensive use of PyStan which is licensed under the GPL license, a requirement of which is that derivative works are also licensed under the GPL license. Hence this notebook is distributed under the GPL license. There is a copy of the full text of the license in this directory.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<small style=\"font-size:12px\">\n",
    "This notebook is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This notebook is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with Foobar.  If not, see <https://www.gnu.org/licenses/>.\n",
    "</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
