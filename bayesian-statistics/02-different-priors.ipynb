{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial model with Beta priors\n",
    "\n",
    "This notebook plots the beta distribution curves so that we can see the effect of observing data on posterior for the binomial model.\n",
    "\n",
    "Recall that the density of the $\\text{Beta}(\\alpha, \\beta)$ distribution is defined as\n",
    "\n",
    "$$\n",
    "    p(\\theta | \\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbeta(a, b, n_samples=10000):\n",
    "    \"\"\"\n",
    "    Evaluate beta density with parameters a and b on a grid, normalise to have integral 1\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 1, n_samples)\n",
    "    y = x ** (a - 1) * (1 - x) ** (b - 1)\n",
    "    return x, y * n_samples / np.sum(y)\n",
    "\n",
    "\n",
    "def plot_beta(a, b, n_samples=10000, ax=None):\n",
    "    \"\"\"\n",
    "    Make a plot of the beta distribution with parameters a and b. Note that the\n",
    "    values of the density are normalised to lie in the range 0, 1, and so the\n",
    "    integral will not be 1.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots()\n",
    "    else:\n",
    "        f = plt.gcf()\n",
    "    ax.plot(*dbeta(a, b, n_samples=n_samples))\n",
    "    return f, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation \n",
    "\n",
    "Assume that $y \\sim \\mathrm{Binom}(n, \\theta)$. Then we saw that $\\theta \\sim \\mathrm{Beta}(\\alpha, \\beta)$ leads to $\\theta \\:|\\: y \\sim \\mathrm{Beta}(\\alpha+y, \\beta+n-y)$.\n",
    "\n",
    "The uniform distribution on $[0,1]$ can be characterised as $\\mathrm{Beta}(1,1)$, and so with a uniform prior, the posterior distribution for $\\theta$ having observed $y$ successes from $n$ trials is $\\mathrm{Beta}(y+1, n+1-y)$.\n",
    "\n",
    "Below we plot $\\mathrm{Beta}(1,1)$, $\\mathrm{Beta}(1,2)$ and $\\mathrm{Beta}(1,5)$ which correspond to the prior; the posterior having observed one trial which was unsuccessful; and the posterior having observed 4 trials, all of which were unsuccessful. Note that the posterior probability of $\\theta$ becomes more and more concentrated at $0$, as the data suggests success is unlikely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "plot_beta(1, 1, ax=ax[0])\n",
    "plot_beta(1, 2, ax=ax[1])\n",
    "plot_beta(1, 5, ax=ax[2])\n",
    "ax[0].set_title(\"Beta(1, 1)\", fontdict={\"fontsize\": 16})\n",
    "ax[0].set_ylim(0, 1.1)\n",
    "ax[1].set_title(\"Beta(1, 2)\", fontdict={\"fontsize\": 16})\n",
    "ax[2].set_title(\"Beta(1, 5)\", fontdict={\"fontsize\": 16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe more data, our uncertainty drops. Here are plots of $\\mathrm{Beta}(1+1, 3+1)$, $\\mathrm{Beta}(10+1, 30+1)$, $\\mathrm{Beta}(100+1, 300+1)$. In each case we observe three times as many failures as successes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "plot_beta(2, 4, ax=ax[0])\n",
    "plot_beta(11, 31, ax=ax[1])\n",
    "plot_beta(101, 301, ax=ax[2])\n",
    "ax[0].set_title(\"Beta(2, 4)\", fontdict={\"fontsize\": 16})\n",
    "ax[1].set_title(\"Beta(11, 31)\", fontdict={\"fontsize\": 16})\n",
    "ax[2].set_title(\"Beta(101, 301)\", fontdict={\"fontsize\": 16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now imagine we specify an informative prior. We can see visually that as we observe more and more data, the prior's influence reduces.\n",
    "\n",
    "The below plots show the same data observed for two different priors. First a uniform prior (top left), then the posterior after observing 10 successes from 40 trials, and the posterior after observing 100 successes from 400 trials. The second row shows a non-uniform prior that represents the belief that the probability of success is high. As the data is observed, we see that very quickly the mass of the distribution moves to lower values of $\\theta$, and after observing 400 samples the two posteriors are almost visually indistinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "\n",
    "plot_beta(1, 1, ax=ax[0, 0])\n",
    "plot_beta(11, 31, ax=ax[0, 1])\n",
    "plot_beta(101, 301, ax=ax[0, 2])\n",
    "ax[0, 0].set_title(\"Beta(1, 1)\", fontdict={\"fontsize\": 16})\n",
    "ax[0, 0].set_ylim(0, 1.1)\n",
    "ax[0, 1].set_title(\"Beta(11, 31)\", fontdict={\"fontsize\": 16})\n",
    "ax[0, 2].set_title(\"Beta(101, 301)\", fontdict={\"fontsize\": 16})\n",
    "\n",
    "plot_beta(10, 2, ax=ax[1, 0])\n",
    "plot_beta(20, 32, ax=ax[1, 1])\n",
    "plot_beta(110, 302, ax=ax[1, 2])\n",
    "ax[1, 0].set_title(\"Beta(10, 2)\", fontdict={\"fontsize\": 16})\n",
    "ax[1, 1].set_title(\"Beta(20, 32)\", fontdict={\"fontsize\": 16})\n",
    "ax[1, 2].set_title(\"Beta(110, 302)\", fontdict={\"fontsize\": 16});"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
