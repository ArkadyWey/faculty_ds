{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to take you through some AI explainability tooling, starting with simple models that can be easily understood, moving right up to Shapley values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load and Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the tree-based-methods course, we'll be working with a dataset called adult census which can be found [here](https://archive.ics.uci.edu/ml/datasets/adult). This contains US census information from 1994. The task is to predict whether or not an individual in the dataset earns more than $50k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path('adult-census.csv').exists():\n",
    "    !wget https://s3-eu-west-1.amazonaws.com/faculty-client-teaching-materials/explainability/adult-census.csv\n",
    "df = pd.read_csv(\"adult-census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we'll need to clean the salary column to make it suitable for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_salary(salary):\n",
    "    \"\"\"\n",
    "    salary: str\n",
    "        This should be an entry from df[\"salary\"]\n",
    "    \"\"\"\n",
    "\n",
    "    if salary == \" <=50K\" or salary == \" <=50K.\":\n",
    "        output = 0\n",
    "    elif salary == \" >50K\" or salary == \" >50K.\":\n",
    "        output = 1\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid input {salary}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"salary\"] = df[\"salary\"].map(convert_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the classes are imbalanced, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"salary\"].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 24% of people have a salary above $50k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split off the target column from our features. We'll also drop the `education` column, as this has already been encoded in a sensible way numerically using `education-num`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\n",
    "    [\"salary\", \"education\"], axis=1\n",
    ").copy()  # this stops X being a reference to df\n",
    "y = df[\"salary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, and just as before, we need to convert the categorical features into numercial columns. These categorical columns are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    \"workclass\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"native-country\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use `pd.get_dummies`, let's have a look at the different values within each category. The reason for this is we want to be slightly careful about dropping the redundant categories - our goal is to understand how important different features are to a model and this might become difficult if we discard an arbitrary category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, drop_first=False)\n",
    "\n",
    "categories = {}\n",
    "for col in cat_cols:\n",
    "    categories[col] = [c for c in X.columns if c.split(\"_\")[0] == col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through this it is clear that most categories have an \"Other\" column, denoted either by a \"?\" or \"Other\". The exceptions are `sex`, `marital-status` and `relationship`. For `sex` we'll just keep the male column as it's a binary variable. For `marital-status` and `relationship` we'll keep all of their categories, we'll drop `native-country` entirely as it is very high cardinality, and for the other columns we'll remove the \"other\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"sex_ Female\"]\n",
    "for col in X.columns:\n",
    "    if (\n",
    "        col.split(\"_\")[-1] in [\" ?\", \" Other\"]\n",
    "        or col.split(\"_\")[0] == \"native-country\"\n",
    "    ):\n",
    "        to_drop.append(col)\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a training and testing set. We won't be hyperparamter tuning here so we can do without a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it will be helpful later to normalize our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)  # note we fit the scaler with just the training set\n",
    "\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Intrinsically Interpretably Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit an intrinsically interpretable model, namely logistic regression. You haven't come across logistic regression in a lot of detail, but as simple way of thinking about it is it's the classification counterpart to linear regression. It's defined as follows:\n",
    "$$ p_i = \\sigma\\left( \\beta_0 + \\sum_{j=1}^k \\beta_j x_{ij} \\right)$$\n",
    "where $p_i$ is the probability assigned to predicting the $i$-th value, there are $k$ features, and\n",
    "$$\\sigma(x):= \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients naturally give a way of explaining the model and also therefore of measuring feature importance. As we've normalised our features, we can compare the relative sizes of the coefficients to each other to measure importance.\n",
    "\n",
    "Let's now fit an train our logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the accuracies across the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logistic.score(X_train, y_train))\n",
    "print(logistic.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling appears to have helped the model greatly! Let's plot a graph of its coefficients. You can access the coefficients of logistic regression by looking at the `.coef` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a 2-dimensional array with one row, so in most of what follows we'll want to plug in `logistic.coef_[0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex:** Write a function that given a list of feature importances and labels for them plots them on a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(scores, labels, normalize=False):\n",
    "    \"\"\"\n",
    "    scores: array-like\n",
    "        A list of feature scores.\n",
    "    labels: array-like\n",
    "        A list of feature labels, the same length of scores.\n",
    "    normalize: bool\n",
    "        Whether or not to normalize the scores by L1-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # put the scores in a dataframe\n",
    "    to_plot = pd.DataFrame({\"score\": scores, \"label\": labels})\n",
    "\n",
    "    if normalize:\n",
    "        to_plot[\"score\"] /= np.absolute(to_plot[\"score\"].values).sum()\n",
    "\n",
    "    # sort the dataframe by score\n",
    "    to_plot = to_plot.sort_values(\"score\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.bar(to_plot[\"label\"], to_plot[\"score\"])\n",
    "    plt.xticks(rotation=\"vertical\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(np.absolute(logistic.coef_[0]), X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a decent first attempt, but it is pretty hard to interpret due to the sheer number of features. One way we could combat this is by combining the dummied-categorical columns together. Let's do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will try and combine categorical columns scores together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def combine_importances(scores, labels):\n",
    "    \"\"\"\n",
    "    Combine categorical scores together.\n",
    "    \n",
    "    WARNING:\n",
    "        Definition of a cat-col is precence of an underscore.\n",
    "        This works for adult-census but won't necessarily\n",
    "        for other datasets.\n",
    "    \"\"\"\n",
    "    score_dict = defaultdict(float)\n",
    "    for (score, label) in zip(scores, labels):\n",
    "        cat_col_split = label.split(\"_\")[0]\n",
    "        score_dict[cat_col_split] += abs(score)\n",
    "    return list(score_dict.values()), list(score_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try plotting that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(\n",
    "    *combine_importances(logistic.coef_[0], X.columns), normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a lot better. One downside is we've lost the ability to tell the directionality of how features effect predictions (which we could have done in the first plot), i.e. it's no longer possible to tell if being more educated has a positive or a negative effect on your earnings, we just know it's significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Model Specific Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models are not intrinsically interpetable, but due to their structure come with a way of calculating feature importances. We'll have a look at Random Forests in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(\n",
    "    n_estimators=100, random_state=42, min_samples_leaf=10\n",
    ")\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forest.score(X_train, y_train))\n",
    "print(forest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees have a natural way of measuring feature importance; one can calculate how much a feature decreases the gini impurity/entropy. To apply this to a random forest, you simply average this across trees in the forest. You can access these by looking at the `.feature_importances_` attribute; note these will sum up to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex:** Plot the feature importance graph where the individual dummied-columns are combined together like the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(\n",
    "    *combine_importances(forest.feature_importances_, X.columns), True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest seems to be using different features to the logistic regression, in particular Logistic Regression makes a lot of use of `capital-gain`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Model Agnostic Methods: Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big upside of the above approaches with random forests and logistic regression is that we essentially got the feature importances \"for free\". A downside is that we can't easily compare them to each other; we can normalize them both but they are essentially very different methods. One way of overcoming this difficulty is to use \"permutation\" feature importance on both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate this, you simply shuffle a column (or a group of columns) and see how it affects the models pefromance with respect to some scoring metric. Often we will repeate this process many times and output the average. In fact this \"group\" point is very useful, we can easily see the importance of a group of categorical columns by shuffling its dummies as a group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex:** Write a function that will calculate the shuffle feature importance of a (group of) columns, you may find `np.random.permutation` to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shuffle_importance(scoring_function, data, target, column, n_iters=5):\n",
    "    \"\"\"\n",
    "    scoring_function: function\n",
    "        A function with which to measure model, should take\n",
    "        arguments in the form scoring_function(data, target).\n",
    "    data: pandas.DataFrame\n",
    "        The data with which to calculate feature importance.\n",
    "    target: array-like\n",
    "        The target that was being predicted\n",
    "    column: str or array-like\n",
    "        If str this should be the column to calculate feature importance for.\n",
    "        O/w a list (or similar) of columns.\n",
    "    n_iters: int, optional\n",
    "        The number of times to do the shuffling, default is 5.\n",
    "    \"\"\"\n",
    "\n",
    "    perm_scores = []\n",
    "    opt_score = scoring_function(data, target)\n",
    "    for _ in range(n_iters):\n",
    "        data0 = data.copy()  # to make sure we don't change the original data\n",
    "        data0[column] = np.random.permutation(data0[column])\n",
    "\n",
    "        # calculate the perm score\n",
    "        perm_score = scoring_function(data0, target)\n",
    "\n",
    "        perm_scores.append(perm_score)\n",
    "\n",
    "    perm_scores = np.array(perm_scores)\n",
    "\n",
    "    perm_scores = np.absolute(perm_scores - opt_score) / opt_score\n",
    "    return perm_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using it for the logistic and random forest models using accuracy as our metric. To do this we'll first need to group the dummied columns back together. This is done below for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_groups = defaultdict(list)\n",
    "for col in X.columns:\n",
    "    if col.split(\"_\")[0] != col:  # then it's a categorical\n",
    "        column_groups[col.split(\"_\")[0]].append(col)\n",
    "    else:\n",
    "        column_groups[col].append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex:** Use the `column_groups` dictionary to create a dataframe with three columns; `label` being the original column names, `forest_acc` being the forest accuracy feature importance, and `logistic_acc` being the logistic accuracy feature importance. Calculate these importances on the training set. Call the dataframe `importances_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_dict = {\"label\": [], \"forest_acc\": [], \"logistic_acc\": []}\n",
    "\n",
    "for feature, group in tqdm(column_groups.items()):\n",
    "    importances_dict[\"label\"].append(feature)\n",
    "\n",
    "    logistic_acc = get_shuffle_importance(\n",
    "        logistic.score, X_train, y_train, group\n",
    "    )\n",
    "    forest_acc = get_shuffle_importance(forest.score, X_train, y_train, group)\n",
    "\n",
    "    importances_dict[\"forest_acc\"].append(forest_acc)\n",
    "    importances_dict[\"logistic_acc\"].append(logistic_acc)\n",
    "\n",
    "importances_df = pd.DataFrame(importances_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex:** Compare the feature importance graphs made by this method to the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(importances_df[\"forest_acc\"], importances_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(\n",
    "    importances_df[\"logistic_acc\"], importances_df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that they're a bit different! The above demonstrates a clear problem with feature importance; different metrics which *a priori* sound reasonable give different answers. Another downside is that they can't explain anything about local feature importance; `education-num` may be important in general but it might be more or less significant for individual points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, you may want to think about the following:\n",
    "- In order to aggregate the importance of categorical features to random forests using `.feature_importances_`, we simply summed the individual components. However, a more correct way to do this would have been to have done the summing before taking the average across trees. You may want to see if you can do this yourself.\n",
    "- A big advantage of the permutation approach is that it allows us to calculate feature importance on a validation set distinct from the training set, try doing that above and see if it changes things much.\n",
    "- We chose accuracy above, but we could have chosen log loss. You could try rerunning the above using logloss as your scoring function instead, using the functions defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_logloss(data, target):\n",
    "    return log_loss(target, forest.predict_proba(data)[:, 1])\n",
    "\n",
    "\n",
    "def logistic_logloss(data, target):\n",
    "    return log_loss(target, logistic.predict_proba(data)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Model Agnostic Methods: Shapley Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapley values are a way of explaining ML models that come from cooperative game theory. They allow us to give local explanations of data points, as well as getting global information by summing over individual points. Let's try it out below for the random forest; we use the `shap` package found [here](https://github.com/slundberg/shap).\n",
    "\n",
    "One disadvantage of shap is that it can be quite slow. To speed things up, we'll use a subsample below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X_train.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.SamplingExplainer(\n",
    "    lambda x: forest.predict_proba(x)[:, 1], sample\n",
    ")\n",
    "shap_values = explainer.shap_values(\n",
    "    sample, nsamples=100, l1_reg=f\"num_features({sample.shape[1]})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point now has its own explanation (or feature importance). In order to aggregate these together, we first take the absolute value and then take the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_shaps = np.absolute(shap_values).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now plot the feature importances for the individual categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(*combine_importances(global_shaps, sample.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are quite similar, though `hours-per-week` and `age` look more important here. The `shap` package actually has its own plotting built in, let's check it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, features=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows how much variability there is in features importance for different points, and the importance of local explanations. If you have some time, try repeating the above with the logistic regression, and looking at the `shap` package in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shap` package has downsides though, some of which are unique to it and others which are a problem for Shapley values more generally:\n",
    "- It's quite slow and often requires some sampling\n",
    "- You may recall from the slides that Shapley values are meant to \"sum to the accuracy of the model. This cannot be true with `shap` though, as we haven't passed the true labels anywhere\n",
    "- It would have been better if we could have passed our column groups to the the explainer class directly as opposed to having to do the aggregation afterwards and treating them as independent features.\n",
    "\n",
    "This is actually a topic of current reseach, and is something which the Faculty R&D team are actively involved with. For more details on AI Safety, see the blogs on our website, for example [here](https://faculty.ai/blog/machine-learning-model-explainability-through-shapley-values/). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
