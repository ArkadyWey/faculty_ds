{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing\n",
    "## 2. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Get Vader data for sentiment analysis\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "PARTY_COLOURS = {\"trump\": \"#E91D0E\", \"obama\": \"#00A6EF\"}\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # encode tweets as utf-8 strings\n",
    "    text = text.decode(\"utf-8\")\n",
    "    # remove commas in numbers (else vectorizer will split on them)\n",
    "    text = re.sub(r\",([0-9])\", \"\\\\1\", text)\n",
    "    # sort out HMTL formatting of &\n",
    "    text = re.sub(r\"&amp\", \"and\", text)\n",
    "    # strip urls\n",
    "    return re.sub(r\"http[s]{0,1}://[^\\s]*\", \"\", text)\n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"tweets.pkl\")\n",
    "df[\"text\"] = df[\"text\"].map(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we allow our vectorizer to infer a vocabulary from the corpus, then this will typically result in a huge number of sparesely populated features. We can often dimension reduce and retain relevant information (albeit sacrificing some interpretability), and improve the efficiency of our models and analysis.\n",
    "\n",
    "Let's visualise our tfidf vectors in a few different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Principal component analysis (PCA) aims find a coordinate system where correlation between features is minimized. By keeping only the coordinate directions in the new system that explain the most variance, we can reduce the dimensions of our feature space. This transformation is linear, so each of the principal components is a linear combination of the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# dimension reduction algorithms can be pretty slow, so let's work with a sample\n",
    "# try on the whole data set if you want!\n",
    "sample_trump = df.loc[df[\"label\"] == 0, [\"text\", \"label\"]].sample(500)\n",
    "sample_obama = df.loc[df[\"label\"] == 1, [\"text\", \"label\"]].sample(500)\n",
    "sample = sample_trump.append(sample_obama)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Use `PCA` to reduce `tfidf_vectors` to two dimensions, then plot the results using the `scatter_1` function. Pass the labels as colours.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_1(x, colors):\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect=\"equal\")\n",
    "    ax.scatter(\n",
    "        x[(colors == 0), 0],\n",
    "        x[(colors == 0), 1],\n",
    "        c=PARTY_COLOURS[\"trump\"],\n",
    "        label=\"Trump\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        x[(colors == 1), 0],\n",
    "        x[(colors == 1), 1],\n",
    "        c=PARTY_COLOURS[\"obama\"],\n",
    "        label=\"Obama\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    plt.legend()\n",
    "    return f, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduce and plot here\n",
    "pca = PCA(n_components=2)\n",
    "# dimensional reduction techniques often make use of the `fit_transform` method\n",
    "# it both fits the model and then transforms the vectors you inputted\n",
    "X_pca = pca.fit_transform(<your-tfidf-vector>)\n",
    "# plot your graph now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "t-SNE is another dimension reduction algorithm, but one that is generally better at preserving the global structure of the data. In the case of our twitter data it does a much better job than PCA. This transformation is highly non-linear though, so it is hard to understand what the 2-dimensional representation means in reference to the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have suggested some parameters below, feel free to experiment\n",
    "tsne = TSNE(perplexity=800, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Transform `tfidf_vectors` using `tsne` and plot them using `scatter_1`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduce and plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "UMAP is another dimensional reduction algorithm. It is in many ways similar to t-SNE but it is a lot faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapper = UMAP(n_neighbors=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Transform `tfidf_vectors` using `UMAP` and plot them using `scatter_1`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduce and plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a numeric representation of our data, there are many clustering algorithms we can try out. Since our feature vectors are extremely high dimensional, it is a good idea to first dimension reduce so that we do not fall foul of the curse of dimensionality. To demonstrate this, we will cluster the original high-dimensional vectors and then the dimension-reduced vectors.\n",
    "\n",
    "**Exercise: Use KMeans to cluster your tfidf vectors into two classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# cluster tfidf vectors using K-Means\n",
    "km = KMeans(n_clusters=2, init='k-means++', n_init=20)\n",
    "km.fit(<your-tfidf-vectors>)\n",
    "\n",
    "sample['kmeans_labels'] = # your kmeans labels, check the documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some reorganizing for plotting clusters\n",
    "df_trump = sample[sample[\"label\"] == 0]\n",
    "df_obama = sample[sample[\"label\"] == 1]\n",
    "\n",
    "trump_counts = (\n",
    "    df_trump[[\"kmeans_labels\", \"label\"]]\n",
    "    .groupby(\"kmeans_labels\")\n",
    "    .count()\n",
    "    .values.flatten()\n",
    ")\n",
    "obama_counts = (\n",
    "    df_obama[[\"kmeans_labels\", \"label\"]]\n",
    "    .groupby(\"kmeans_labels\")\n",
    "    .count()\n",
    "    .values.flatten()\n",
    ")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "bars11 = ax.bar(\n",
    "    np.arange(2) - 0.15,\n",
    "    trump_counts,\n",
    "    0.3,\n",
    "    color=PARTY_COLOURS[\"trump\"],\n",
    "    label=\"Trump\",\n",
    ")\n",
    "bars12 = ax.bar(\n",
    "    np.arange(2) + 0.15,\n",
    "    obama_counts,\n",
    "    0.3,\n",
    "    color=PARTY_COLOURS[\"obama\"],\n",
    "    label=\"Obama\",\n",
    ")\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=15)\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel(\"Cluster\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Now use k-means clustering to cluster the tsne-vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means on your tfidf vectors\n",
    "\n",
    "sample['kmeans_tsne'] = # your k-means labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some reorganizing for plotting clusters\n",
    "\n",
    "df_trump = sample[sample[\"label\"] == 0]\n",
    "df_obama = sample[sample[\"label\"] == 1]\n",
    "\n",
    "trump_counts = (\n",
    "    df_trump[[\"kmeans_tsne\", \"label\"]]\n",
    "    .groupby(\"kmeans_tsne\")\n",
    "    .count()\n",
    "    .values.flatten()\n",
    ")\n",
    "obama_counts = (\n",
    "    df_obama[[\"kmeans_tsne\", \"label\"]]\n",
    "    .groupby(\"kmeans_tsne\")\n",
    "    .count()\n",
    "    .values.flatten()\n",
    ")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "bars11 = ax.bar(\n",
    "    np.arange(2) - 0.15,\n",
    "    trump_counts,\n",
    "    0.3,\n",
    "    color=PARTY_COLOURS[\"trump\"],\n",
    "    label=\"Trump\",\n",
    ")\n",
    "bars12 = ax.bar(\n",
    "    np.arange(2) + 0.15,\n",
    "    obama_counts,\n",
    "    0.3,\n",
    "    color=PARTY_COLOURS[\"obama\"],\n",
    "    label=\"Obama\",\n",
    ")\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=15)\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel(\"Cluster\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Print a selection of Obama tweets that ended up in the Trump cluster, and a selection of Trump tweets that ended up in the Obama cluster. If the clustering is working well, the Obama tweets should look Trumpian, and the Trump tweets should look Obama-like.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print tweets that ended up in the wrong clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove vectors - Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first notebook we saw how GloVe vectors could be used to determine how similar words are to each other. They can also be used in a similar way to find topics, by first performing dimensionality reduction and then using a clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataset contains short survery responses to the question: \"what's your passion?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = pd.read_csv(\"survey_responses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = np.concatenate(\n",
    "    [nlp(response).vector.reshape(1, 300) for response in survey[\"response\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use UMAP to perform dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapper = UMAP(n_neighbors=25)\n",
    "umap_vectors = umapper.fit_transform(glove_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use a clustering algorithm, in this case one called hdbscan, to cluster the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscanner = HDBSCAN()\n",
    "hdbscanner.fit(umap_vectors)\n",
    "\n",
    "labels = hdbscanner.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualise the clustering. The following code uses an interactive graphing library called `plotly`. It allows you to interact with the plot; try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "def scatter_2(x, y, labels, text):\n",
    "    data = [\n",
    "        go.Scatter(\n",
    "            x=x[labels == label],\n",
    "            y=y[labels == label],\n",
    "            mode=\"markers\",\n",
    "            opacity=0.7,\n",
    "            text=text[labels == label],\n",
    "            name=label,\n",
    "            marker={\"size\": 15, \"line\": {\"width\": 0.5, \"color\": \"white\"}},\n",
    "        )\n",
    "        for label in set(labels)\n",
    "    ]\n",
    "    layout = go.Layout(\n",
    "        xaxis={\"showgrid\": False, \"showticklabels\": False, \"zeroline\": False},\n",
    "        yaxis={\"showgrid\": False, \"showticklabels\": False, \"zeroline\": False},\n",
    "        hovermode=\"closest\",\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig, config={\"displayModeBar\": False})\n",
    "\n",
    "\n",
    "scatter_2(\n",
    "    umap_vectors[:, 0],\n",
    "    umap_vectors[:, 1],\n",
    "    labels=labels.astype(str),\n",
    "    text=survey.response,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common task in NLP is sentiment anaylsis, this is often an unsupervised problem. We show off a tool called `vader` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# find a sample of 400 tweets, split between Obama and Trump\n",
    "df_sentiment = (\n",
    "    df[df[\"label\"] == 0]\n",
    "    .sample(200)\n",
    "    .append(df[df[\"label\"] == 1].sample(200))\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "for i, tweet in df_sentiment[\"text\"].iteritems():\n",
    "    ss = sid.polarity_scores(str(tweet))\n",
    "    for k in sorted(ss):\n",
    "        df_sentiment.loc[i, k] = ss[k]\n",
    "\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentance gets a negative, neutral and positive score, as well as a compound score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a plot here comparing the sentiment distribution of Trump vs. Obama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_sentiment = df_sentiment[df_sentiment[\"label\"] == 0][\n",
    "    [\"compound\", \"neg\", \"pos\", \"text\"]\n",
    "]\n",
    "obama_sentiment = df_sentiment[df_sentiment[\"label\"] == 1][\n",
    "    [\"compound\", \"neg\", \"pos\", \"text\"]\n",
    "]\n",
    "\n",
    "trump_neg_sentiment = (\n",
    "    trump_sentiment[\"neg\"].sort_values().reset_index(drop=True)\n",
    ")\n",
    "obama_neg_sentiment = (\n",
    "    obama_sentiment[\"neg\"].sort_values().reset_index(drop=True)\n",
    ")\n",
    "\n",
    "trump_pos_sentiment = (\n",
    "    trump_sentiment[\"pos\"].sort_values().reset_index(drop=True)\n",
    ")\n",
    "obama_pos_sentiment = (\n",
    "    obama_sentiment[\"pos\"].sort_values().reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "ax[0, 0].hist(trump_pos_sentiment, color=PARTY_COLOURS[\"trump\"])\n",
    "ax[0, 0].set_title(\"Trump Positive Sentiment\")\n",
    "ax[0, 1].hist(trump_neg_sentiment, color=PARTY_COLOURS[\"trump\"])\n",
    "ax[0, 1].set_title(\"Trump Negative Sentiment\")\n",
    "ax[1, 0].hist(obama_pos_sentiment, color=PARTY_COLOURS[\"obama\"])\n",
    "ax[1, 0].set_title(\"Obama Positive Sentiment\")\n",
    "ax[1, 1].hist(obama_neg_sentiment, color=PARTY_COLOURS[\"obama\"])\n",
    "ax[1, 1].set_title(\"Obama Negative Sentiment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's compare Trump's positive and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_pos_sentiment = trump_sentiment.sort_values(\n",
    "    by=\"pos\", ascending=False\n",
    ").copy()\n",
    "trump_neg_sentiment = trump_sentiment.sort_values(\n",
    "    by=\"neg\", ascending=False\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in trump_pos_sentiment.head()[\"text\"].values:\n",
    "    print(tweet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in trump_neg_sentiment.head()[\"text\"].values:\n",
    "    print(tweet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
