{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing\n",
    "## 1. Data Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take you through many of the concepts we have introduced in this session. We will use the same dataset for many of the examples, namely a collection of 6000 or so tweets from @realDonaldTrump and @BarackObama. \n",
    "\n",
    "Wherever possible we will use `sklearn`, Python's machine learning library that you are most likely already familiar with. For a few tasks we will turn to `nltk` (natural language toolkit) a Python library for Nautural Language Procession (NLP), and a few other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"tweets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things to consider when cleaning text data. Some problems are common to other data types, such as how to deal with missing values. Others are unique to text data, and include things like removing HTML tags or urls. We don't want to focus too much on data cleaning for the purposes of this course, we've done a little bit of cleaning below to give you a taste. Generally speaking regular expressions (available in Python in the `re` module) will get you pretty far. For specific tasks there are often existing libraries you can use. For example `feedparser` is good for getting data from an RSS feed, `beautifulsoup` is good for parsing HTML/XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # encode tweets as utf-8 strings\n",
    "    text = text.decode(\"utf-8\")\n",
    "    # remove commas in numbers (else vectorizer will split on them)\n",
    "    text = re.sub(r\",([0-9])\", \"\\\\1\", text)\n",
    "    # sort out HMTL formatting of &\n",
    "    text = re.sub(r\"&amp\", \"and\", text)\n",
    "    # strip urls\n",
    "    return re.sub(r\"http[s]{0,1}://[^\\s]*\", \"\", text)\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].map(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of NLP contains a lot of jargon from linguistics. We don't want to get too bogged down in defining lots of new terms, but the following two are helpful:\n",
    "\n",
    "- Type: An element of the vocabulary. May be a word, may be an n-gram (ordered sequence of words)\n",
    "- Token: An instance of a type in running text.\n",
    "\n",
    "Any given language has a large enough vocabulary that trying to do data science on the set of all possible sentences is totally impractical. Instead it helps to break text up into smaller chunks, a process called tokenizing.\n",
    "\n",
    "Exactly how we do this will depend on the problem, but some common ways include splitting on whitespace, or splitting on non-alphanumeric characters. In general, the method of tokenizing will be informed by the format of the text data being studied.\n",
    "\n",
    "**Exercise: Tokenizers are accessed in a slightly roundabout way in `sklearn`, as below. Run this cell a few times to tokenize random tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from random import randint\n",
    "\n",
    "# tokenize a random tweet\n",
    "i = randint(0, len(df) - 1)\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "tokenizer(df[\"text\"].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing breaks our raw text data down into more manageable chunks, but it's still not in a form that is particularly useful for training models. Let's look at a few common, simple ways of vectorizing text data. We will use `sklearn` which can efficiently vectorize text data and stores everything as `scipy` sparse arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the simplest way to vectorize is to simply create a vector of counts of the number of times any type appears in a given piece of text.\n",
    "\n",
    "To get some intuition, let's try it on a small test corpus of 10 random tweets.\n",
    "\n",
    "**Exercise: Use `sample` on the series `df['text']` to get a random selection of 10 tweets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "test_corpus = df[\"text\"].sample(n=10, random_state=42).copy()\n",
    "\n",
    "test_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample, we can create count vectors using `CountVectorizer` from `sklearn`. We set `max_features=5` so as to work with a small vocabulary of only the most common terms.\n",
    "\n",
    "See the next cell for usage of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer with our desired parameters\n",
    "count_vectorizer = CountVectorizer(max_features=5)\n",
    "\n",
    "# first 'fit' the vectorizer to the corpus\n",
    "# this step automatically determines the vocabulary\n",
    "count_vectorizer.fit(test_corpus)\n",
    "\n",
    "# then 'transform' the corpus to count vectors (a matrix)\n",
    "count_vectors = count_vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = count_vectorizer.get_feature_names()\n",
    "# we use .toarray() to convert from sparse\n",
    "# array to dense numpy array\n",
    "for i, row in enumerate(count_vectors.toarray()):\n",
    "    print(test_corpus.iloc[i])\n",
    "    print(\n",
    "        pd.DataFrame({\"Terms\": features, \"Counts\": row}).to_string(index=False)\n",
    "    )\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count vectors are very sensitive to document length. In our case we expect all tweets to be similar lengths, but in general we might be dealing with documents of varying lengths, so it makes sense to normalise the count vectors. This results in so-called frequency vectors.\n",
    "\n",
    "**Exercise: Using `TfidfVectorizer`, compute term frequency vectors for the test corpus and print them out as we did for the count vectors. Make sure you set `use_idf=False` when initialising your `TfidfVectorizer`. As before limit the vocabulary to 5 types.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a term frequency vectorizer\n",
    "# note that use_idf=False, more on tf-idf in a second\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_features=5, use_idf=False)\n",
    "\n",
    "# 'fit' the vectorizer to the corpus\n",
    "# this step automatically determins the vocabulary\n",
    "tf_vectorizer.fit(test_corpus)\n",
    "\n",
    "# then 'transform' the corpus\n",
    "# this computes the term frequency vectors\n",
    "tf_vectors = tf_vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_vectorizer.get_feature_names()\n",
    "for i, row in enumerate(tf_vectors.toarray()):\n",
    "    print(test_corpus.iloc[i])\n",
    "    print(\n",
    "        pd.DataFrame({\"Terms\": features, \"Term frequencies\": row}).to_string(\n",
    "            index=False\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf stands for 'term frequency - inverse document frequencys. Given our term frequencys, we re-weight by the inverse of the document frequency. Therefore a given term will have a larger value if it both appears many times in the document, but appears infrequently across the corpus. In this sense it automatically detects and upweights terms which are likely to be able to help us distinguish between documents.\n",
    "\n",
    "**Exercise: Compute tfidf vectors for your test_corpus. You can once again use `TfidfVectorizer`, but this time set `use_idf=True`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a term frequency - inverse document frequency vectorizer with our desired parameters\n",
    "# in this case let us limit the vocabulary (max features) to 10\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5, use_idf=True)\n",
    "\n",
    "# we 'fit' the vectorizer to the corpus, this step automatically determines the vocabulary\n",
    "tfidf_vectorizer.fit(test_corpus)\n",
    "\n",
    "# then we 'transform' the corpus, this is the vectorizing step\n",
    "tfidf_vectors = tfidf_vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer.get_feature_names()\n",
    "for i, row in enumerate(tfidf_vectors.toarray()):\n",
    "    print(test_corpus.iloc[i])\n",
    "    print(\n",
    "        pd.DataFrame(\n",
    "            {\"Terms\": features, \"Term Frequency (weighted)\": row}\n",
    "        ).to_string(index=False)\n",
    "    )\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only considered individual words and their frequencies. We lose a lot of information doing so, because we discard word order and grammar etc.\n",
    "\n",
    "A simple solution to this is to use n-grams, that is sequences of words of length n, when we tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can tokenize/vectorize with n-grams using the parameter\n",
    "# ngram_range. It takes a tuple of ints that specify min and max\n",
    "# n-gram lengths\n",
    "ngram_vectorizer = CountVectorizer(max_features=5, ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Use `ngram_vectorizer` to compute bigram count vectors for your test corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the vectorizer to the test_corpus\n",
    "ngram_vectorizer.fit(test_corpus)\n",
    "\n",
    "# transform to get the vectors\n",
    "ngram_vectors = ngram_vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ngram_vectorizer.get_feature_names()\n",
    "for i, row in enumerate(ngram_vectors.toarray()):\n",
    "    print(test_corpus.iloc[i])\n",
    "    print(\n",
    "        pd.DataFrame({\"Terms\": features, \"Term frequencies\": row}).to_string(\n",
    "            index=False\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same principles as before, namely vectorizing using term frequencies or term frequency-inverse document frequencies, apply here too.\n",
    "\n",
    "A big advantage of tokenizing using n-grams is that models can learn some basic information about which words tend to appear together, and which words follow on from other sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `spacy` for accessing GloVe vectors in Python. It's a really nice package for natural language processing that we definitely recommend checking out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(\"This is a sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all nouns from the sentance\n",
    "[t.text for t in nlp(\"This is a sentence\") if t.pos_ == \"NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatise each of the words in the sentance\n",
    "[t.lemma_ for t in nlp(\"Some knives, forks and spoons\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the GloVe vector's shape associcated to the word \"octopus\"\n",
    "nlp(\"octopus\").vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use GloVe vectors to compare how similar words are to each other. The code below computes the \"cosine similarity\" between the GloVe vectors, which is defined as follows:\n",
    "$$\\mathrm{simularity}(\\mathbf{v}, \\mathbf{w}) = \\cos(\\theta) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\|\\mathbf{v}\\| \\| \\mathbf{w}\\|}$$\n",
    "where $\\theta$ is the angle between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc = nlp(\"dog cat banana apple\")\n",
    "\n",
    "similarities = np.zeros((4, 4))\n",
    "\n",
    "for i, token1 in enumerate(doc):\n",
    "    for j, token2 in enumerate(doc):\n",
    "        similarities[i, j] = token1.similarity(\n",
    "            token2\n",
    "        )  # computes the cosine similarity\n",
    "\n",
    "pd.DataFrame(\n",
    "    similarities,\n",
    "    index=[\"Dog\", \"Cat\", \"Banana\", \"Apple\"],\n",
    "    columns=[\"Dog\", \"Cat\", \"Banana\", \"Apple\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll read in similarities comparing other words to \"octopus\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = pd.read_pickle(\"similarities.pkl\")\n",
    "\n",
    "sim.sample(5, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.sort_values(by=\"similarity\", ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.sort_values(by=\"similarity\", ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe vectors preserve relationships between similar words leading to relationships like $$\\mathrm{king} \\approx \\mathrm{queen} - \\mathrm{woman} + \\mathrm{man}$$\n",
    "We'll investigate this further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"man\", \"woman\", \"king\", \"queen\", \"uncle\", \"aunt\", \"sir\", \"madam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "glove_vectors = np.concatenate(\n",
    "    [nlp(word).vector.reshape(1, 300) for word in words]\n",
    ")\n",
    "\n",
    "glove_pca = pca.fit_transform(glove_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "def scatter(x, y, labels, text):\n",
    "    data = [\n",
    "        go.Scatter(\n",
    "            x=x[labels == label],\n",
    "            y=y[labels == label],\n",
    "            mode=\"markers\",\n",
    "            opacity=0.7,\n",
    "            text=text[labels == label],\n",
    "            name=label,\n",
    "            marker={\"size\": 15, \"line\": {\"width\": 0.5, \"color\": \"white\"}},\n",
    "        )\n",
    "        for label in set(labels)\n",
    "    ]\n",
    "    layout = go.Layout(\n",
    "        xaxis={\"showgrid\": False, \"showticklabels\": False, \"zeroline\": False},\n",
    "        yaxis={\"showgrid\": False, \"showticklabels\": False, \"zeroline\": False},\n",
    "        hovermode=\"closest\",\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig, config={\"displayModeBar\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast words to numpy array so scatter can create boolean mask\n",
    "scatter(glove_pca[:, 0], glove_pca[:, 1], np.array(words), np.array(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_ceo = [\n",
    "    \"dorsey\",\n",
    "    \"twitter\",\n",
    "    \"zuckerberg\",\n",
    "    \"facebook\",\n",
    "    \"ballmer\",\n",
    "    \"microsoft\",\n",
    "    \"bezos\",\n",
    "    \"amazon\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "glove_vectors_ceo = np.concatenate(\n",
    "    [nlp(word).vector.reshape(1, 300) for word in companies_ceo]\n",
    ")\n",
    "\n",
    "glove_pca_ceo = pca.fit_transform(glove_vectors_ceo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast companies_ceo to numpy array so scatter can create boolean mask\n",
    "scatter(\n",
    "    glove_pca_ceo[:, 0],\n",
    "    glove_pca_ceo[:, 1],\n",
    "    np.array(companies_ceo),\n",
    "    np.array(companies_ceo),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparatives = [\n",
    "    \"slow\",\n",
    "    \"slower\",\n",
    "    \"slowest\",\n",
    "    \"fast\",\n",
    "    \"faster\",\n",
    "    \"fastest\",\n",
    "    \"long\",\n",
    "    \"longer\",\n",
    "    \"longest\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "glove_comparatives = np.concatenate(\n",
    "    [nlp(word).vector.reshape(1, 300) for word in comparatives]\n",
    ")\n",
    "\n",
    "glove_comparatives = pca.fit_transform(glove_comparatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast comparatives to numpy array so scatter can create boolean mask\n",
    "scatter(\n",
    "    glove_comparatives[:, 0],\n",
    "    glove_comparatives[:, 1],\n",
    "    np.array(comparatives),\n",
    "    np.array(comparatives),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
