{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.special import binom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Our goal in Bayesian inference is to compute the posterior $p(\\theta \\mid D)$ which represents our updated beliefs about the parameters after we have seen data. This is, by Bayes' rule:\n",
    "\n",
    "$$p(\\theta \\mid D) = \\frac{p(D \\mid \\theta) p(\\theta)}{p(D)}$$\n",
    "\n",
    "We will start by focusing on just one of those terms, the \"likelihood\", $p(D \\mid \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just what is a likelihood anyway?\n",
    "\n",
    "Let's call $\\theta$ the probability of landing heads from a coin (or the \"bias\" of the coin). We toss the coin $n$ times and observe $y$ heads. \n",
    "\n",
    "Let's assume we toss the coin 10 times ($n=10$) and see 8 heads ($y=8$). Using just the data, what is our best guess for the bias of the coin, $\\theta$?\n",
    "\n",
    "\n",
    "In this example the (correct) likelihood (or model) is given by a Binomial distribution:\n",
    "\n",
    "$$p(D \\mid \\theta ) = \\text{Bin}(n, \\theta) = {n \\choose y} \\theta^y (1-\\theta)^{n -y}$$\n",
    "\n",
    "We can write this in Python as:\n",
    "\n",
    "```python\n",
    "def likelihood(n, y, theta):\n",
    "    return binom(n, y) * theta**y * (1 - theta)**(n - y)\n",
    "```\n",
    "\n",
    "and evaluate it for different choices of $\\theta$. Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(n, y, theta):\n",
    "    return binom(n, y) * theta**y * (1 - theta) ** (n - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0, 1, 101)\n",
    "plt.plot(x_grid, [likelihood(10, 8, x) for x in x_grid], linewidth=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Why is the peak where it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-Binomial model\n",
    "\n",
    "We can turn this into a fully Bayesian model by using a prior distribution, $p(\\theta)$ to represent our prior belief about the bias of the coin before we have seen any data (i.e. tossed it). This section walks through how to calculate the posterior distribution in closed-form.\n",
    "\n",
    "As before, let's call $\\theta$ the probability of tossing heads from a coin. We toss the coin $n$ times and observe $y$ heads. We are interested in calculating $p(\\theta \\mid D)$.\n",
    "\n",
    "The likelihood (or model) is the following:\n",
    "\n",
    "$$p(D \\mid \\theta ) = \\text{Bin}(n, \\theta) = {n \\choose y} \\theta^y (1-\\theta)^{n -y}$$\n",
    "\n",
    "and we know: \n",
    "\n",
    "$${n \\choose y} = \\frac{y!}{n! (n-y)!}$$\n",
    "\n",
    "If we assume a uniform prior, $p(\\theta) = 1$ then we have the posterior proportional to:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\theta \\mid D) &\\propto p(D \\mid \\theta) \\, p(\\theta) \\\\\n",
    "&\\propto \\frac{y!}{n! (n-y)!} \\theta^y (1-\\theta)^{n -y} \\cdot 1 \\\\\n",
    "&\\propto \\theta^y (1-\\theta)^{n -y}\n",
    "\\end{align}\n",
    "\n",
    "where we dropped all terms that didn't depend on $\\theta$, including $p(D)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a Beta distribution has PDF:\n",
    "\n",
    "$$\\text{Beta}(\\alpha, \\beta) = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)}$$\n",
    "\n",
    "where $$\\text{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\, \\Gamma(\\beta)} {\\Gamma(\\alpha + \\beta)}$$\n",
    "\n",
    "Note we only care about terms involving $\\theta$, everything else is treated as a constant.\n",
    "\n",
    "We can thus compare the terms in the posterior expression with the PDF for a Beta distribution and deduce that the posterior is given by:\n",
    "\n",
    "$$p(\\theta \\mid D) = \\text{Beta}(y + 1, n - y + 1)$$\n",
    "\n",
    "when we assume a uniform prior. A helpful fact is that $\\text{Beta}(1, 1)$ is actually the uniform distribution. \n",
    "\n",
    "By following the same reasoning as above, if we switched our prior from uniform to a $\\text{Beta}(\\alpha, \\beta)$ distribution we would have a posterior of:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\theta \\mid D) &\\propto p(D \\mid \\theta) \\, p(\\theta) \\\\\n",
    "&\\propto \\underbrace{\\theta^y (1-\\theta)^{n -y}}_{\\text{Likelihood}} \\underbrace{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}_{\\text{Beta prior}} \\\\\n",
    "&\\propto \\theta^{y + \\alpha - 1} (1-\\theta)^{n - y + \\beta - 1} \\\\\n",
    "&= \\text{Beta}(y + \\alpha, n - y + \\beta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples!\n",
    "\n",
    "Let's now look at some code examples to make this clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(n, y, theta):\n",
    "    return binom(n, y) * theta**y * (1 - theta) ** (n - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0, 1, 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "y = 8\n",
    "\n",
    "a = 25\n",
    "b = 25\n",
    "\n",
    "prior = stats.beta(a, b)\n",
    "posterior = stats.beta(y + a, n - y + b)\n",
    "likelihoods = [likelihood(n, y, p) for p in x_grid]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(x_grid, prior.pdf(x_grid), label=\"prior, $p(\\\\theta)$\", linewidth=5)\n",
    "ax.plot(\n",
    "    x_grid, posterior.pdf(x_grid), label=\"posterior, $p(\\\\theta \\mid D)$\", linewidth=5\n",
    ")\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(\n",
    "    x_grid,\n",
    "    likelihoods,\n",
    "    label=\"likelihood, $p(D \\mid \\\\theta )$\",\n",
    "    linewidth=5,\n",
    "    color=\"green\",\n",
    ")\n",
    "ax.axvline(y / n, color=\"red\", label=\"MLE\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax2.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: simulate\n",
    "\n",
    "Here we show the change in posterior as we see more and more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_theta = 0.75\n",
    "a = 3\n",
    "b = 3\n",
    "prior = stats.beta(a, b)\n",
    "\n",
    "# Sample some data:\n",
    "num_tosses = 250\n",
    "samples = np.random.binomial(1, true_theta, num_tosses)\n",
    "print(f\"Sample has mean: {samples.mean()} and true coin bias is: {true_theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fracs = [0.01, 0.05, 0.1, 0.25, 0.5, 1]\n",
    "ns = np.round(np.array(fracs) * num_tosses).astype(int)\n",
    "\n",
    "f, ax = plt.subplots(2, 3, figsize=(18, 10))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(len(ns)):\n",
    "    ax[i].plot(x_grid, prior.pdf(x_grid), label=\"prior\", linewidth=5)\n",
    "    heads = samples[: ns[i]].sum()\n",
    "    post = stats.beta(heads + a, ns[i] - heads + b)\n",
    "    ax[i].plot(x_grid, post.pdf(x_grid), label=\"posterior\", linewidth=5)\n",
    "    ax[i].set_title(f\"Number of samples seen: {ns[i]}, heads so far: {heads}\")\n",
    "    ax[i].axvline(true_theta, color=\"grey\", linestyle=\"--\")\n",
    "    ax[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\n",
    "    f\"True $\\\\theta$: {true_theta}, actual heads in sample: {samples.sum()} => MLE {samples.mean():.2f}\",\n",
    "    fontsize=14,\n",
    "    y=1.03,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
