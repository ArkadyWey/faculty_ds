{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gotta classify 'em all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the types of Pokémon based on their attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, you have access to the vital statistics for the first 6 generations of Pokémon. Your (very open ended!) task is to see what to extent it is possible to predict the primary type (the `Type_1` field) of a Pokémon given its other vital statistics. This is a multiclass classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints and tips:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The package `sklearn` is the industry standard for ML algorithms that can be used out of the box quickly- you should use it. https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "    * Beginning with something simple like Logistic Regression or a Decision Tree is encouraged to establish a performance baseline.\n",
    "* The dataset features a large number of columns, many of which are likely redundant and will not contain any predictive power for the task at hand. \n",
    "    * Consider preselecting a small number of 'obvious' columns which your misspent youth (sorry, 'domain expertise') tells you are likely to contain a lot of predictice power. \n",
    "    * Get something up and running with these first (fewer features -> less time feature engineering) and then circle back to incorporate more features into your model once this is done.\n",
    "    * Doing this iteratively rather than 'all at once' is good from a client facing point of view (having some kind of concrete result to discuss early is always a good thing), and will also help you see what is going on more quickly.\n",
    "* The dataset consists of a mixture of categorical and continuous variables. \n",
    "    * Categorical variables will need to be converted to numeric indicator values before being passed into `sklearn` classifiers- you can use the `pandas` function `pd.get_dummies` for this.\n",
    "    * Dummying lots of categorical features with lots of possible values can quickly result in a very large, sparse, feature space. Beginning with only a small subset of features will help here.\n",
    "* Some of the columns in your dataset have null values in them:\n",
    "    * It's wise to ignore these at a first pass in order to get up and running quickly, but they might contain information that could improve your model.\n",
    "    * In this situation, does it make more sense to impute these null values with something like the mean or mode value of the non-null entries, to convert them to a dummy variable, or to incorporate them into another column somehow?\n",
    "* There are `18` different `Type_1` values. Are these values distributed evenly across the dataset? It might make sense to focus your initial efforts on identifying only a subset of these in order to avoid getting bogged down with small data issues at the start.\n",
    "* In order to evaluate the performance of your model, you will need to perform a train/test split (use `sklearn.model_selection.train_test_split`).\n",
    "    * Research what it means to stratify your train/test split with respect to the target variable. It is a good idea to do so here in order to guarantee that the performance metrics you quote are relatively stable. You should be aware that doing this in situations where you can't be sure that the class balance breakdown 'in the wild' is the same as in your dataset will result in biased estimates of the performance of your model.\n",
    "    * Train/test splits feature a frustrating tradeoff: you want as much data as possible in your train set to build the best possible model, but you also want lots of data in your test set to evaluate its performance accurately. Cross validation is a computationally intensive way to have your cake and eat it in this scenario: see `sklearn.model_selection.cross_val_score`.\n",
    "* You will need to consider how to evaluate the performance of your classifier: the accuracy score is the simplest metric to quote, but plotting a full confusion matrix will give you significantly more insight into how your model is performing and where it could be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to import more packages (i.e., numpy, sklearn packages) as required.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Magic command to make plots render inline/underneath cells in Jupyter notebooks.\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"https://s3-eu-west-1.amazonaws.com/faculty-client-teaching-materials/non-linear-algorithms/pokemon.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fpath)\n",
    "# list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Type_1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mostcommon = df[\"Type_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_types = mostcommon.iloc[:19].index\n",
    "my_features = [\n",
    "    \"Total\",\n",
    "    \"HP\",\n",
    "    \"Attack\",\n",
    "    \"Defense\",\n",
    "    \"Sp_Atk\",\n",
    "    \"Sp_Def\",\n",
    "    \"hasMegaEvolution\",\n",
    "    \"Egg_Group_1\",\n",
    "]\n",
    "my_features = [\n",
    "    \"Total\",\n",
    "    \"HP\",\n",
    "    \"Attack\",\n",
    "    \"Defense\",\n",
    "    \"Sp_Atk\",\n",
    "    \"Sp_Def\",\n",
    "    \"Egg_Group_1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"Type_1\"].isin(my_types), [\"Type_1\"] + my_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[\"Type_1\"].isin(my_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[subset][my_features]\n",
    "y = df[subset][\"Type_1\"]\n",
    "X = pd.get_dummies(X, dtype=float)\n",
    "X = X.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=0, stratify=y\n",
    ")\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "y_pred = logisticRegr.predict(x_test)\n",
    "score = logisticRegr.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
