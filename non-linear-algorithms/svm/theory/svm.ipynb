{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduce Support Vector Machines (SVM) from a theoretical perspective.\n",
    "We willl carefully motivate and formulate the mathematical model behind SVMs,\n",
    "and will show you how to train an SVM using a generic convex optimization solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by considering a simple binary classification problem.\n",
    "The set-up is as follows: we have a dataset consisting of two features\n",
    "denoted $X_1$ and $X_2$, and a label denoted by the variable\n",
    "$Y$. We'll denote by\n",
    "$\\mathbf{x}_1$, $\\mathbf{x}_2$, ..., $\\mathbf{x}_n$ a set\n",
    "of samples from the distribution of $(X_1, X_2)$, and will denote\n",
    "their labels by $y_1$, $y_2$, ..., $y_n$. For reasons that will\n",
    "become apparent later, the labels will be assumed to be taking\n",
    "values in $\\{-1, 1\\}$; the *positive class* will refer to the data points\n",
    "with label $1$, and the *negative class* will refer to the data points\n",
    "with label $-1$. We'll use $S_+$ to denote the set of samples with label\n",
    "$1$ and $S_-$ to denote the set of samples with label $-1$.\n",
    "\n",
    "In the first part of this tutorial, our toy dataset will consist of\n",
    "20 samples generated from a 2D normal random\n",
    "variable with mean vector $(1, 0)$ and covariance matrix\n",
    "$$\\begin{pmatrix}\n",
    "0.05 & 0 \\\\\n",
    "0 & 0.05\n",
    "\\end{pmatrix},$$\n",
    "for the positive class, and the negative class will consist of\n",
    "20 samples generated from a 2D normal random variable with mean\n",
    "vector $(0, 1)$ and the same covariance matrix as for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_plus_dist = scipy.stats.multivariate_normal(\n",
    "    mean=np.array([1, 0]), cov=np.array([[0.05, 0], [0, 0.05]])\n",
    ")\n",
    "\n",
    "Y_plus_sample = Y_plus_dist.rvs(n_samples_per_class)\n",
    "\n",
    "Y_minus_dist = scipy.stats.multivariate_normal(\n",
    "    mean=np.array([0, 1]), cov=np.array([[0.05, 0], [0, 0.05]])\n",
    ")\n",
    "\n",
    "Y_minus_sample = Y_minus_dist.rvs(n_samples_per_class)\n",
    "\n",
    "features = np.concatenate([Y_plus_sample, Y_minus_sample], axis=0)\n",
    "\n",
    "labels = np.ones(shape=(2 * n_samples_per_class, 1))\n",
    "labels[n_samples_per_class:] = -1\n",
    "\n",
    "data = np.concatenate([features, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x=data[:20, 0], y=data[:20, 1], c=\"green\", label=\"$Y = 1$\")\n",
    "plt.scatter(x=data[20:, 0], y=data[20:, 1], c=\"red\", label=\"$Y = -1$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.xlim((-0.5, 1.5))\n",
    "plt.ylim((-0.5, 1.5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like one should be able to separate this dataset *linearly*,\n",
    "i.e. that one should be able to find a straight line which separates the\n",
    "two classes perfectly. In mathematical terms, this means\n",
    "that there should exist coefficients $w_1$, $w_2$ and $b$\n",
    "such that $Y = 1$ if $w_1X_1 + w_2X_2 + b > 0$ and\n",
    "$Y = -1$ if $w_1X_1 + w_2X_2 + b < 0$. Let's have a go at\n",
    "drawing a few possible separating lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.linspace(start=-2, stop=2, num=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=X_1,\n",
    "    label=\"$X_2 = X_1$\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"solid\",\n",
    ")\n",
    "line2 = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=2 * X_1 + 0.3,\n",
    "    label=\"$X_2 = 2X_1$+0.3\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"dashed\",\n",
    ")\n",
    "line3 = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=0.1 * X_1 + 0.5,\n",
    "    label=\"$X_2 = 0.1X_1$+0.5\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"dotted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(x=data[:20, 0], y=data[:20, 1], c=\"green\", label=\"$Y = 1$\")\n",
    "plt.scatter(x=data[20:, 0], y=data[20:, 1], c=\"red\", label=\"$Y = -1$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.xlim((-0.5, 1.5))\n",
    "plt.ylim((-0.5, 1.5))\n",
    "\n",
    "ax.add_line(line1)\n",
    "ax.add_line(line2)\n",
    "ax.add_line(line3)\n",
    "plt.legend(bbox_to_anchor=(1, 1), handles=[line1, line2, line3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each one of these three lines separates the data.\n",
    "However, we are intuitively drawn to choose the solid line over the other\n",
    "two as a separator of the classes. This is because we expect it\n",
    "to generalize better on unseen data: the dashed and dotted lines both\n",
    "seem to overfit to the boundary of the red class. Let us generate\n",
    "a few more datapoints to verify this assumption on this particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_extra_samples_per_class = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_plus_extra_sample = Y_plus_dist.rvs(n_extra_samples_per_class)\n",
    "\n",
    "Y_minus_extra_sample = Y_minus_dist.rvs(n_extra_samples_per_class)\n",
    "\n",
    "features_extra = np.concatenate(\n",
    "    [Y_plus_extra_sample, Y_minus_extra_sample], axis=0\n",
    ")\n",
    "\n",
    "labels_extra = np.ones(shape=(2 * n_extra_samples_per_class, 1))\n",
    "labels_extra[n_extra_samples_per_class:] = -1\n",
    "\n",
    "data_extra = np.concatenate([features_extra, labels_extra], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=X_1,\n",
    "    label=\"$X_2 = X_1$\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"solid\",\n",
    ")\n",
    "line2 = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=2 * X_1 + 0.3,\n",
    "    label=\"$X_2 = 2X_1$+0.3\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"dashed\",\n",
    ")\n",
    "line3 = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=0.1 * X_1 + 0.5,\n",
    "    label=\"$X_2 = 0.1X_1$+0.5\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"dotted\",\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(\n",
    "    x=data_extra[:n_extra_samples_per_class, 0],\n",
    "    y=data_extra[:n_extra_samples_per_class, 1],\n",
    "    c=\"green\",\n",
    "    label=\"$Y = 1$\",\n",
    ")\n",
    "plt.scatter(\n",
    "    x=data_extra[n_extra_samples_per_class:, 0],\n",
    "    y=data_extra[n_extra_samples_per_class:, 1],\n",
    "    c=\"red\",\n",
    "    label=\"$Y = -1$\",\n",
    ")\n",
    "\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.xlim((-0.5, 1.5))\n",
    "plt.ylim((-0.5, 1.5))\n",
    "ax.add_line(line1)\n",
    "ax.add_line(line2)\n",
    "ax.add_line(line3)\n",
    "plt.legend(bbox_to_anchor=(1, 1), handles=[line1, line2, line3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like our intuition was correct! After resampling $200$ data\n",
    "points from both distributions, the solid line is the only one to\n",
    "keep a perfect accuracy score. A Support Vector Machine\n",
    "(also known as a Support Vector Classifier, if we wish to emphasise that it\n",
    "is being used for classification purposes)\n",
    "attempts to find the line that separates the data in the best possible way,\n",
    "just as we did. This assumes an appropriate definition for what\n",
    "a \"good separation\" means. In the case of SVMs, the best separating line\n",
    "is the one which is as \"far away from the boundary of either class as possible\".\n",
    "The dashed and dotted lines, being \"too close\" to the boundary of the classes,\n",
    "are not desirable. In the rest of this post, we'll strive to translate this\n",
    "observation into mathematical terms. It's easy to get lost in the details\n",
    "of the algorithm, but this should not make you forget that SVMs, at their\n",
    "heart, are doing nothing more than finding the line whose distance to\n",
    "either class is as large as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperplanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst our introductory example dealt with a binary classification problem with two predictors,\n",
    "real-life problems will typically feature many more predictors. Hence, from now on, let's assume\n",
    "that we have $k$ variables $X_1$, $X_2$, ..., $X_k$ available to us in order to classify the data into two classes.\n",
    "(Rest assured that our toy examples will still have only 2 features, so as to make visualisation as easy as possible!).\n",
    "In higher dimensions, it no longer makes no sense to speak of a \"straight line\" separating the data.\n",
    "The mathematical objects which generalize 2D lines in higher dimensional space \n",
    "are called a *hyperplanes*. A hyperplane in $k$-dimensional space is the surface defined by the equation\n",
    "$$w_0 X_0 + w_1 X_1 + \\cdots + w_{k-1}X_{k-1} + b = 0.$$\n",
    "You'll notice that when $k=2$ this is the straight line in 2D space that we know and love,\n",
    "and when $k=3$ this is simply a plane in 3-dimensional space. This equation can be\n",
    "written in terms of scalar products of vectors: if $\\mathbf{x} = (X_1, X_2, \\ldots, X_k)$\n",
    "then this is equivalent to\n",
    "$$\\mathbf{w}\\cdot \\mathbf{x} + b = 0.$$\n",
    "From now on, we'll use $\\Pi$ to denote the surface defined by the above equation.\n",
    "There is a natural interpretation for the coefficients of a hyperplane: \n",
    "$$\\mathbf{w} = (w_0, w_1, \\ldots, w_{k-1})$$\n",
    "is a vector which is *normal* (i.e. perpendicular) to $\\Pi$, and\n",
    "$$b = - \\mathbf{w}\\cdot\\mathbf{v_0},$$\n",
    "where $\\cdot$ denotes the scalar product in $k$-dimensional space\n",
    "and $\\mathbf{v_0}$ is any vector belonging to $\\Pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/fig1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances\n",
    "\n",
    "If we are to compute the hyperplane which is \"furthest away from the class boundaries\", we had better define\n",
    "what we mean by the distance between a set of data points and a hyperplane!\n",
    "The *distance* $d(\\mathbf{x}, \\Pi)$ between a data point $\\mathbf{x}$ and a hyperplane $\\Pi$\n",
    "is defined to be the $l^2$-norm of $\\mathbf{x} - \\mathbf{x_\\perp}$ where $\\mathbf{x_\\perp}$ is the orthogonal\n",
    "projection of $\\mathbf{x}$ onto $\\Pi$. After some algebra, we have:\n",
    "$$d(\\mathbf{x}, \\Pi)^2 = \\frac{(\\mathbf{w}\\cdot\\mathbf{x} + b)^2}{\\|\\mathbf{w}\\|^2}.$$\n",
    "This is consistent with the equation defining $\\Pi$: the set of\n",
    "vectors $\\mathbf{x}$ belonging to $\\Pi$ is those for which $d(\\mathbf{x}, \\Pi) = 0$.\n",
    "\n",
    "Then, the distance between a set of vectors $S$ and $\\Pi$ is defined to be the distance between $\\Pi$ and the data point\n",
    "in $S$ which is closest to $\\Pi$, i.e.\n",
    "$$d(S, \\Pi) = \\min_{\\mathbf{x} \\in S} d(\\mathbf{x}, \\Pi).$$\n",
    "\n",
    "We now have a mathematically sound definition of *distance* which a computer algorithm\n",
    "will be able to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/fig2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linearly separable case\n",
    "As we mentioned in the introduction, a support vector classifier aims to find the hyperplane $\\Pi$ which\n",
    "separates the two classes whilst maximizing the distance between the hyperplane and the boundary of either class.\n",
    "In mathematical terms, separating the data means finding $\\mathbf{w}$ and $b$ such that\n",
    "$$\\mathbf{w}\\cdot \\mathbf{x} + b > 0$$\n",
    "for members of the positive class, and\n",
    "$$\\mathbf{w}\\cdot \\mathbf{x} + b < 0$$\n",
    "for members of the negative class. To require the hyperplane to\n",
    "be as far away from the boundary of either class means finding $\\mathbf{w}$ and $b$ such that\n",
    "$$\\min\\{ d(S_+, \\Pi), d(S_-, \\Pi) \\}$$\n",
    "is as large as possible. It is not be too hard to convince oneself\n",
    "that for the optimal plane $\\Pi$ it must be the case that $d(S_+, \\Pi) = d(S_-, \\Pi)$. (Recall that $S_+$ and $S_-$ denote the vectors with labels $1$ and $-1$, respectively.) Indeed, if this were not the case, then we could always reduce the imbalance between the two distances by translating the plane along $\\mathbf{w}$ in the appropriate direction. This would then increase $\\min\\{ d(S_+, \\Pi), d(S_-, \\Pi) \\}$, a contradiction. This has the consequence that if $\\mathbf{x_+} \\in S_+$ is closest to $\\Pi$ and $\\mathbf{x_-} \\in S_-$ is closest to $\\Pi$ then\n",
    "$$\\mathbf{w}\\cdot\\mathbf{x_+} + b = -(\\mathbf{w}\\cdot\\mathbf{x_-} + b).$$\n",
    "By multiplying the equation $\\mathbf{w}\\cdot\\mathbf{x} + b = 0$ by a positive constant if needed, we may assume that $\\mathbf{w}\\cdot\\mathbf{x_+} + b = 1$ for the vectors\n",
    "in $S_+$ which are closest to $\\Pi$ and $\\mathbf{w}\\cdot\\mathbf{x_-} + b = -1$ for the vectors in $S_-$ which are closest\n",
    "to $\\Pi$. The square of the distance between $\\Pi$ and these vectors, and hence between $\\Pi$ and either one of the two classes, is then simply $1 / \\|w\\|^2$. Accordingly, we may restate the support vector classification problem as follows: Find $\\mathbf{w}$ and $b$ so that $\\|\\mathbf{w}\\|^2$ is as small as possible subject to $\\mathbf{w} \\cdot \\mathbf{x} + b \\geq 1$  for each  $\\mathbf{x} \\in S_+$, and\n",
    "$\\mathbf{w} \\cdot \\mathbf{x} + b \\leq -1$  for each  $\\mathbf{x} \\in S_-$. Since we cleverly chose our labels to be $+1$ for members of the positive class and $-1$ for members of the negative class, we may simplify this further into\n",
    "\n",
    "**Support Vector Machine, Hard Margin**: Minimize $$\\frac{1}{2}\\|\\mathbf{w}\\|^2$$\n",
    "subject to\n",
    "$$y_i\\left(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\right) -1 \\geq 0 \\text{ for }1\\leq i\\leq n.$$\n",
    "where $y_i$ is the $i$-th label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(It will be clearer in the next section why we called this the \"hard margin\" version of the SVM optimization problem.)\n",
    "This is a *convex* quadratic optimization problem. It is not necessary to know exactly what this means; but\n",
    "convexity makes the optimization problem easier to solve since it implies that a local minimum must also be a global minimum.\n",
    "Note that the factor of $1/2$ is included for convenience.\n",
    "In what follows we'll use the `cvxopt` package to train an SVM\n",
    "on our synthetically generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt.solvers import qp\n",
    "from cvxopt import matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic programming solver from the `cvxopt` package expects the quadratic problem\n",
    "to be posed in a slightly different form; see http://cvxopt.org/userguide/coneprog.html#quadratic-programming\n",
    "for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = matrix(np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = matrix(np.array([0.0, 0.0, 0.0]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = matrix(\n",
    "    -data[:, 2].reshape((-1, 1))\n",
    "    * np.concatenate([data[:, :2], np.ones((data.shape[0], 1))], axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = matrix(-np.ones(shape=(data.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solution = qp(P, q, G, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeffs = np.array(solution[\"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of the separating hyperplane found by the SVM algorithm is $2.28X_1 - 1.06X_2-0.53 = 0$.\n",
    "Let's find the coordinates of the vectors that are closest to this hyperplane. These vectors are called the\n",
    "*support vectors* of the separating hyperplane. They have the important property that removing all the\n",
    "other elements from $S_+$ and $S_-$ would leave the optimization problem unchanged in that the\n",
    "optimal separating hyperplane would still remain the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xplus = data[\n",
    "    (np.abs(np.matmul(data[:, :2], coeffs[:2]) + coeffs[2] - 1) < tol).reshape(\n",
    "        -1\n",
    "    ),\n",
    "    :2,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xminus = data[\n",
    "    (np.abs(np.matmul(data[:, :2], coeffs[:2]) + coeffs[2] + 1) < tol).reshape(\n",
    "        -1\n",
    "    ),\n",
    "    :2,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot our findings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesvc = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=-coeffs[0] / coeffs[1] * X_1 - coeffs[2] / coeffs[1],\n",
    "    label=\"$\\mathbf{w}\\cdot\\mathbf{x} + b = 0$\",\n",
    "    color=\"#e88127\",\n",
    "    linestyle=\"solid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesvcplus = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=-coeffs[0] / coeffs[1] * X_1 - coeffs[2] / coeffs[1] - 1,\n",
    "    label=\"$\\mathbf{w}\\cdot\\mathbf{x} + b = 1$\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"dotted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesvcminus = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=-coeffs[0] / coeffs[1] * X_1 - coeffs[2] / coeffs[1] + 1,\n",
    "    label=\"$\\mathbf{w}\\cdot\\mathbf{x} + b = -1$\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"dashed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.axis(\"equal\")\n",
    "plt.scatter(\n",
    "    x=data[:n_samples_per_class, 0],\n",
    "    y=data[:n_samples_per_class, 1],\n",
    "    c=\"green\",\n",
    "    label=\"$Y = 1$\",\n",
    ")\n",
    "plt.scatter(\n",
    "    x=data[n_samples_per_class:, 0],\n",
    "    y=data[n_samples_per_class:, 1],\n",
    "    c=\"red\",\n",
    "    label=\"$Y = -1$\",\n",
    ")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.xlim((-0.5, 1.5))\n",
    "plt.ylim((-0.5, 1.5))\n",
    "\n",
    "sv = plt.scatter(\n",
    "    x=np.concatenate([xplus[:, 0], xminus[:, 0]]),\n",
    "    y=np.concatenate([xplus[:, 1], xminus[:, 1]]),\n",
    "    marker=\"*\",\n",
    "    s=250,\n",
    "    label=\"Support Vectors\",\n",
    ")\n",
    "ax.add_line(linesvc)\n",
    "ax.add_line(linesvcplus)\n",
    "ax.add_line(linesvcminus)\n",
    "\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1, 1), handles=[linesvc, sv, linesvcplus, linesvcminus]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The region between the planes defined by $\\mathbf{w}\\cdot\\mathbf{x} + b = -1$ and $\\mathbf{w}\\cdot\\mathbf{x} + b = 1$ is called the *margin*.\n",
    "The *width* of the margin is $2/\\| \\mathbf{w} \\|$; this is the distance between the two planes defining the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The non-separable case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we (implicitly) assumed that the training dataset was linearly separable.\n",
    "This need not be the case, and it certainly is not for the two classes\n",
    "of vectors plotted in red and green below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_plus_insep_dist = scipy.stats.multivariate_normal(\n",
    "    mean=np.array([1, 0]), cov=np.array([[0.5, 0], [0, 0.5]])\n",
    ")\n",
    "\n",
    "Y_plus_insep_sample = Y_plus_insep_dist.rvs(n_samples_per_class)\n",
    "\n",
    "Y_minus_insep_dist = scipy.stats.multivariate_normal(\n",
    "    mean=np.array([0, 1]), cov=np.array([[0.5, 0], [0, 0.5]])\n",
    ")\n",
    "\n",
    "Y_minus_insep_sample = Y_minus_insep_dist.rvs(n_samples_per_class)\n",
    "\n",
    "features_insep = np.concatenate(\n",
    "    [Y_plus_insep_sample, Y_minus_insep_sample], axis=0\n",
    ")\n",
    "\n",
    "labels_insep = np.ones(shape=(2 * n_samples_per_class, 1))\n",
    "labels_insep[n_samples_per_class:] = -1\n",
    "\n",
    "data_insep = np.concatenate([features_insep, labels_insep], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    x=data_insep[:20, 0], y=data_insep[:20, 1], c=\"green\", label=\"$Y = 1$\"\n",
    ")\n",
    "plt.scatter(\n",
    "    x=data_insep[20:, 0], y=data_insep[20:, 1], c=\"red\", label=\"$Y = -1$\"\n",
    ")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.xlim((-0.5, 1.5))\n",
    "plt.ylim((-0.5, 1.5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, this is not the end for Support Vector Machines.\n",
    "Instead of enforcing a *hard* margin by requiring $y_i\\left(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\right) -1 \\geq 0$\n",
    "for each member of our dataset, let  us allow this inequality to be violated by some of them,\n",
    "but let us make sure that we penalize the algorithm whenever it decides to do this. This is\n",
    "called a *soft* margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine, Soft Margin**: Minimize $$\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\left(\\sum_{i=1}^n \\xi_i\\right)^k$$\n",
    "subject to\n",
    "$$\\xi_i \\geq 0 \\text{ for } 1 \\leq i \\leq n \\text{, and}$$\n",
    "$$y_i\\left(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\right) -1 \\geq -\\xi_i \\text{ for } 1 \\leq i \\leq n.$$\n",
    "where $y_i$ is the $i$-th label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $C>0$ and $k\\geq 1$ are constant hyperparameters. Typically one chooses $k=1$ as it simplifies the optimization problem (more on this below),\n",
    "and $C$ is tuned through cross-validation. Choosing a smaller value of $C$ makes the margin softer: we are more tolerant to violations\n",
    "of the margin. The variables $\\xi_i$ are called *slack variables*. When our algorithm chooses to assign a non-zero positive value to a slack\n",
    "variable, the margin is violated by the corresponding data point, and it becomes a support vector. \n",
    "\n",
    "Note that in practice this is always the form in which support vector machines are trained. Not only does this allow us to deal with the\n",
    "linearly inseparable case, but it can also be beneficial even when the dataset is linearly separable! The reason for this is that\n",
    "we are allowing the algorithm to look for a larger margin provided the incurred penalty is not too large. This means that\n",
    "we are overfitting less to the particular values of the vectors closest to the separating hyperplane.\n",
    "\n",
    "There is a good theoretical reason why the optimal separating plane for our initial linearly separable toy example is defined \n",
    "by the equation $X_2 = X_1$ (exercise for the reader!). However, the line we previously found had a slope roughly equal to 2.\n",
    "Can the soft margin algorithm come to the rescue? Let's try it out with $k=1$ and $C=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.zeros(shape=(2 * n_samples_per_class + 3, 2 * n_samples_per_class + 3))\n",
    "P[0, 0] = 1\n",
    "P[1, 1] = 1\n",
    "P = matrix(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = C * np.ones(shape=(2 * n_samples_per_class + 3, 1))\n",
    "q[0] = 0.0\n",
    "q[1] = 0.0\n",
    "q[2] = 0.0\n",
    "q = matrix(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = -data[:, 2].reshape((-1, 1)) * np.concatenate(\n",
    "    [data[:, :2], np.ones((data.shape[0], 1))], axis=1\n",
    ")\n",
    "G = np.concatenate([G, -np.identity(2 * n_samples_per_class)], axis=1)\n",
    "G_positive_penalty = np.concatenate(\n",
    "    [\n",
    "        np.zeros(shape=(2 * n_samples_per_class, 3)),\n",
    "        -np.identity(2 * n_samples_per_class),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "G = np.concatenate([G, G_positive_penalty], axis=0)\n",
    "G = matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.concatenate(\n",
    "    [-np.ones(shape=(data.shape[0], 1)), np.zeros(shape=(data.shape[0], 1))],\n",
    "    axis=0,\n",
    ")\n",
    "h = matrix(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solution = qp(P, q, G, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeffs = np.array(solution[\"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeffs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separating hyperplane is now determined by the equation $0.91X_1 - 0.9X_2 + 0.01$, i.e. $X_2 = 1.008X_1 -0.019$.\n",
    "This is much closer to the optimal separating line, defined by $X_2 = X_1$, than the separating line we obtained by\n",
    "enforcing a hard margin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsupport = data[\n",
    "    (np.abs(np.matmul(data[:, :2], coeffs[:2]) + coeffs[2]) < 1).reshape(-1),\n",
    "    :2,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesvc = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=-coeffs[0] / coeffs[1] * X_1 - coeffs[2] / coeffs[1],\n",
    "    label=\"$\\mathbf{w}\\cdot\\mathbf{x} + b = 0$\",\n",
    "    color=\"#e88127\",\n",
    "    linestyle=\"solid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesvcplus = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=-coeffs[0] / coeffs[1] * X_1 - coeffs[2] / coeffs[1] - 1,\n",
    "    label=\"$\\mathbf{w}\\cdot\\mathbf{x} + b = 1$\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"dotted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesvcminus = matplotlib.lines.Line2D(\n",
    "    xdata=X_1,\n",
    "    ydata=-coeffs[0] / coeffs[1] * X_1 - coeffs[2] / coeffs[1] + 1,\n",
    "    label=\"$\\mathbf{w}\\cdot\\mathbf{x} + b = -1$\",\n",
    "    color=\"#9b9b9b\",\n",
    "    linestyle=\"dashed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(x=data[:20, 0], y=data[:20, 1], c=\"green\", label=\"$Y = 1$\")\n",
    "plt.scatter(x=data[20:, 0], y=data[20:, 1], c=\"red\", label=\"$Y = -1$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.xlim((-0.5, 1.5))\n",
    "plt.ylim((-0.5, 1.5))\n",
    "sv = plt.scatter(\n",
    "    x=xsupport[:, 0],\n",
    "    y=xsupport[:, 1],\n",
    "    marker=\"*\",\n",
    "    s=250,\n",
    "    label=\"Support Vectors\",\n",
    ")\n",
    "ax.add_line(linesvc)\n",
    "ax.add_line(linesvcplus)\n",
    "ax.add_line(linesvcminus)\n",
    "\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1, 1), handles=[linesvc, sv, linesvcplus, linesvcminus]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you separate this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are datasets for which enforcing a linear separation boundary is just a terrible idea,\n",
    "regardless of how soft we let the margin be. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_minus = np.random.uniform(low=0.0, high=2.0, size=(n_samples_per_class, 1))\n",
    "theta_minus = np.random.uniform(\n",
    "    low=0.0, high=2 * np.pi, size=(n_samples_per_class, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_plus = np.random.uniform(low=4.0, high=6.0, size=(n_samples_per_class, 1))\n",
    "theta_plus = np.random.uniform(\n",
    "    low=0.0, high=2 * np.pi, size=(n_samples_per_class, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_plus_insep2_sample = np.concatenate(\n",
    "    [R_plus * np.cos(theta_plus), 4 + R_plus * np.sin(theta_plus)], axis=1\n",
    ")\n",
    "Y_minus_insep2_sample = np.concatenate(\n",
    "    [R_minus * np.cos(theta_minus), 4 + R_minus * np.sin(theta_minus)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_insep2 = np.concatenate(\n",
    "    [Y_plus_insep2_sample, Y_minus_insep2_sample], axis=0\n",
    ")\n",
    "\n",
    "labels_insep2 = np.ones(shape=(2 * n_samples_per_class, 1))\n",
    "labels_insep2[n_samples_per_class:] = -1\n",
    "\n",
    "data_insep2 = np.concatenate([features_insep2, labels_insep2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    x=data_insep2[:n_samples_per_class, 0],\n",
    "    y=data_insep2[:n_samples_per_class, 1],\n",
    "    c=\"green\",\n",
    "    label=\"$Y = 1$\",\n",
    ")\n",
    "plt.scatter(\n",
    "    x=data_insep2[n_samples_per_class:, 0],\n",
    "    y=data_insep2[n_samples_per_class:, 1],\n",
    "    c=\"red\",\n",
    "    label=\"$Y = -1$\",\n",
    ")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.xlim((-6, 6))\n",
    "plt.ylim((-2, 10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that a separation boundary should be circular rather than linear.\n",
    "And yet, SVMs can still be used to separate this dataset. The trick is to\n",
    "map the data to a typically higher dimensional space first, where it is linearly separable,\n",
    "and apply the SVM algorithm in that space. As an example,\n",
    "since we strongly suspect from the picture (or our generation parameters...)\n",
    "that a boundary of the form\n",
    "$$X_1^2 + (X_2 - c_2)^2 = r^2$$\n",
    "(which is the equation of a circle centered on $(0, c_2)$ with radius $r$) should be able to separate the data, let's try the map\n",
    "$$\\Phi: (X_1, X_2) \\longrightarrow (X_1^2, X_2, X_2^2).$$\n",
    "This is what the data looks like after applying $\\Phi$ to the elements of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need the following package to make 3D plots with matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the 3D features, i.e. the map Phi\n",
    "\n",
    "data_insep2_highdim = np.concatenate(\n",
    "    [\n",
    "        (data_insep2[:, 0] ** 2).reshape(-1, 1),\n",
    "        (data_insep2[:, 1]).reshape(-1, 1),\n",
    "        (data_insep2[:, 1] ** 2).reshape(-1, 1),\n",
    "        data_insep2[:, 2].reshape(-1, 1),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(\n",
    "    data_insep2_highdim[:, 0],\n",
    "    data_insep2_highdim[:, 1],\n",
    "    data_insep2_highdim[:, 2],\n",
    "    c=data_insep2[:, 2].astype(int),\n",
    "    cmap=matplotlib.colors.ListedColormap(colors),\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"$X_1^2$\")\n",
    "ax.set_ylabel(\"$X_2$\")\n",
    "ax.set_zlabel(\"$X_2^2$\")\n",
    "ax.view_init(azim=190)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks promising! Let's find a separating hyperplane in this higher-dimensional feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.zeros(shape=(2 * n_samples_per_class + 4, 2 * n_samples_per_class + 4))\n",
    "P[0, 0] = 1\n",
    "P[1, 1] = 1\n",
    "P[2, 2] = 1\n",
    "P = matrix(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = C * np.ones(shape=(2 * n_samples_per_class + 4, 1))\n",
    "q[0] = 0.0\n",
    "q[1] = 0.0\n",
    "q[2] = 0.0\n",
    "q[3] = 0.0\n",
    "q = matrix(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = -data_insep2_highdim[:, 3].reshape((-1, 1)) * np.concatenate(\n",
    "    [data_insep2_highdim[:, :3], np.ones((data_insep2_highdim.shape[0], 1))],\n",
    "    axis=1,\n",
    ")\n",
    "G = np.concatenate([G, -np.identity(2 * n_samples_per_class)], axis=1)\n",
    "G_positive_penalty = np.concatenate(\n",
    "    [\n",
    "        np.zeros(shape=(2 * n_samples_per_class, 4)),\n",
    "        -np.identity(2 * n_samples_per_class),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "G = np.concatenate([G, G_positive_penalty], axis=0)\n",
    "G = matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.concatenate(\n",
    "    [\n",
    "        -np.ones(shape=(data_insep2_highdim.shape[0], 1)),\n",
    "        np.zeros(shape=(data_insep2_highdim.shape[0], 1)),\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "h = matrix(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solution = qp(P, q, G, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.array(solution[\"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperplane separting hyperplane found by the `cvxopt` package\n",
    "has equation $0.166X_1^2 - 1.7X_2 + 0.135X_2^2 + 0.633 = 0$.\n",
    "As a sanity check, let us plot this decision boundary back in\n",
    "2D space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "x1_cont = np.arange(-6, 6, delta)\n",
    "x2_cont = np.arange(-2, 10, delta)\n",
    "X1_cont, X2_cont = np.meshgrid(x1_cont, x2_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (\n",
    "    coeffs[0] * X1_cont ** 2\n",
    "    + coeffs[1] * X2_cont\n",
    "    + coeffs[2] * X2_cont ** 2\n",
    "    + coeffs[3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(\n",
    "    x=data_insep2[:n_samples_per_class, 0],\n",
    "    y=data_insep2[:n_samples_per_class, 1],\n",
    "    c=\"green\",\n",
    "    label=\"$Y = 1$\",\n",
    ")\n",
    "plt.scatter(\n",
    "    x=data_insep2[n_samples_per_class:, 0],\n",
    "    y=data_insep2[n_samples_per_class:, 1],\n",
    "    c=\"red\",\n",
    "    label=\"$Y = -1$\",\n",
    ")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "linesvc = plt.contour(X1_cont, X2_cont, Z, levels=[0], colors=(\"#e88127\"))\n",
    "linesvc.collections[0].set_label(\n",
    "    \"$0.166X_1^2 - 1.7X_2 + 0.135X_2^2 + 0.633 = 0$\"\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Lagrangian Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will quickly run into computational problems if our proposed map $\\Phi$\n",
    "maps the data to a very high dimensional space. In fact, we could even attempt\n",
    "to map the data to a space with an arbitrary number of dimensions, and then it's\n",
    "not clear at all that we can formulate the optimization problem in a tractable way.\n",
    "Thankfully, there is one last trick that will make this possible: to consider\n",
    "the *dual* optimization problem.\n",
    "\n",
    "Given an optimization problem of the form\n",
    "\n",
    "$$\\text{Minimize }f(\\mathbf{x})\\text{ subject to }g_i(\\mathbf{x}) \\geq 0 \\text{ for } 1\\leq i \\leq n,$$\n",
    "\n",
    "let's introduce *Lagrange multipliers* $\\lambda_i \\geq 0$, one for each constraint, and define the\n",
    "*Lagrangian* to be \n",
    "$$L(\\mathbf{\\lambda})  = \\inf_\\mathbf{x} \\left( f(\\mathbf{x}) - \\sum_{i=1}^n \\lambda_i g_i(\\mathbf{x})\\right)$$\n",
    "\n",
    "(if you don't know what $\\inf$ means, just think $\\min$). The *Lagrangian dual problem* is then defined to be\n",
    "\n",
    "$$\\text{Maximize }L(\\mathbf{\\lambda})\\text{ subject to }\\lambda_i \\geq 0 \\text{ for } 1\\leq i \\leq n.$$\n",
    "\n",
    "In this context, the original optimization problem is called the *primal* problem.\n",
    "When the optimization problem is convex (and some technical condititons are satisfied), as is the case of support vector machines, it turns out\n",
    "that these two problems are equivalent. In other words, one could solve the dual first, giving\n",
    "an optimal value for $\\mathbf{\\lambda}$, call it $\\mathbf{\\lambda}^*$; the optimal value of $\\mathbf{x}$ is then that which\n",
    "minimizes $f(\\mathbf{x}) - \\sum_{i=1}^n \\lambda^*_i g_i(\\mathbf{x})$. We won't prove here why this holds,\n",
    "but let's get an intuition for what is going on by considering a simple example. Consider the following simple\n",
    "optimization problem: minimize\n",
    "$$f(X_1, X_2) = X_1^2 + X_2^2$$\n",
    "subject to\n",
    "$$g(X_1, X_2) \\geq 0$$\n",
    "where\n",
    "$$g(X_1, X_2) = X_2 + X_1 - 1.$$\n",
    "There is only one constraint here, so we will introduce only one Lagrange multiplier $\\lambda$.\n",
    "To find the value of the Lagrangian for a fixed value of $\\lambda \\geq 0$, we must\n",
    "minimize $X_1^2 + X_2^2 - \\lambda(X_2 + X_1 - 1)$ with respect to $X_1$ and $X_2$. That's resonably easy\n",
    "(we are minimizing a second order degree polynomials in $X_1$ and minimizing a second order degree polynomials in $X_2$\n",
    "separately). The optimal values for $X_1$ is $\\lambda/2$ and likewise for $X_2$. We can therefore compute the value\n",
    "of the Lagrangian: $L(\\lambda) = \\lambda - \\lambda^2/2$. This is maximized for $\\lambda = 1$ and so we deduce that \n",
    "$X_1^2 + X_2^2$ is minimized when $X_1 = X_2 = 1/2$ with minimal value $1/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.linspace(start=-6, stop=6, num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (g + 1) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = np.array([10 for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrangian(l):\n",
    "    return l - l ** 2 / 2\n",
    "\n",
    "\n",
    "def lagrangian_line(g, l):\n",
    "    return l * g + lagrangian(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda2 = matplotlib.lines.Line2D(\n",
    "    xdata=g,\n",
    "    ydata=lagrangian_line(g, 2),\n",
    "    label=\"$f - 2g = L(2)$\",\n",
    "    color=\"#e88127\",\n",
    "    linestyle=\"dashed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = matplotlib.lines.Line2D(\n",
    "    xdata=g,\n",
    "    ydata=lagrangian_line(g, 1),\n",
    "    label=\"$f - g = L(1)$\",\n",
    "    color=\"#e88127\",\n",
    "    linestyle=\"solid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda3 = matplotlib.lines.Line2D(\n",
    "    xdata=g,\n",
    "    ydata=lagrangian_line(g, 3),\n",
    "    label=\"$f - 3g = L(3)$\",\n",
    "    color=\"#e88127\",\n",
    "    linestyle=\"dotted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.fill_between(\n",
    "    g, f, top, where=top >= f, facecolor=\"#9b9b9b\", interpolate=True\n",
    ")\n",
    "\n",
    "plt.scatter(x=[0, 0, 0], y=[lagrangian(1), lagrangian(2), lagrangian(3)])\n",
    "\n",
    "\n",
    "plt.annotate(\"$L(1)$\", xy=(0, lagrangian(1)))\n",
    "plt.annotate(\"$L(2)$\", xy=(0, lagrangian(2)))\n",
    "plt.annotate(\"$L(3)$\", xy=(0, lagrangian(3)))\n",
    "\n",
    "ax.axvline(x=0, color=\"k\")\n",
    "\n",
    "\n",
    "ax.add_line(lambda2)\n",
    "ax.add_line(lambda1)\n",
    "ax.add_line(lambda3)\n",
    "\n",
    "plt.axis(\"equal\")\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-2, 5))\n",
    "plt.legend(bbox_to_anchor=(1, 1), handles=[lambda1, lambda2, lambda3])\n",
    "\n",
    "plt.xlabel(\"$g(X_1, X_2)$\")\n",
    "plt.ylabel(\"$f(X_1, X_2)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we have shaded the set of possible values that $f$ and $g$ can take simultaneously\n",
    "in grey. We have also drawn the lines $f - \\lambda g = L(\\lambda)$ for $\\lambda \\in \\{1, 2, 3\\}$.\n",
    "As you can see, the intercepts of these lines provide lower bounds on the minimal value that $f$ can take\n",
    "subject to $g \\geq 0$. Moreover, these lower bounds converge to the minimal value of $f$ as $\\lambda$ tends to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What does it mean for a set to be convex? Can you see why the intercepts need not converge to the\n",
    "minimal value of $f$ if the shaded area is not convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the dual to Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave it as an exercise to the reader to show that the dual formulation of the\n",
    "Suppport Vector Machine, Soft Margin optimization problem when $k=1$ is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suport Vector Machine, Soft Margin, Dual Form**: Maximize\n",
    "\n",
    "$$\\sum_{i=1}^n \\lambda_i - \\frac{1}{2} \\sum_{i, j} y_iy_j\\lambda_i\\lambda_j \\mathbf{x}_i \\cdot \\mathbf{x}_j$$\n",
    "subject to:\n",
    "$$0\\leq \\lambda_i\\leq C$$\n",
    "and\n",
    "$$\\sum_{i=1}^n \\lambda_i y_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value of $\\mathbf{w}$ is then given by \n",
    "$\\mathbf{w} = \\sum_{i=1}^n \\lambda_iy_i\\mathbf{x}_i$ and $b = (1-y_i\\mathbf{x}_i\\cdot \\mathbf{w})/y_i$ for any $i$ such that $0 < \\lambda_i < C$.\n",
    "Practically, you should take the average over all such values to get a\n",
    "more numerically stable result.\n",
    "\n",
    "The dual formulation of the SVM algorithm has two key features:\n",
    "* The constraints are easier to deal with.\n",
    "* The feature space appears implicitly rather than explicitly:\n",
    "all that one needs to know about the feature space is the\n",
    "value of the scalar product between data vectors.\n",
    "\n",
    "The latter feature is very important. In primal form we would \n",
    "have had to optimize explicitly over the coordinates of $\\mathbf{w}$,\n",
    "which may be computationally very hard or impossible to do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the previous section how the dual formulation of the SVM optimization problem only required us to know the scalar product between data vectors. This allows us to consider maps $\\Phi$ mapping the data to a feature space of arbitarily large dimension before running the SVM algorithm, and we do not need to know $\\Phi$ explicitly. Rather, what we care about is the *kernel* of $\\Phi$ which is defined by\n",
    "$$K(\\mathbf{x}, \\mathbf{y}) = \\Phi(\\mathbf{x}) \\cdot \\Phi(\\mathbf{y}),$$\n",
    "and is in practice what we have to pass to an implementation of the SVM algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most natural kernels to use for training an SVM is the *polynomial* kernel.\n",
    "The polynomial kernel is defined by\n",
    "$$K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}\\cdot  \\mathbf{y} + c)^d$$\n",
    "where $d$ and $c$ are constants. Notice how for $d=2$ this naturally generalizes the approach\n",
    "we took for classifying the circular dataset.\n",
    "\n",
    "Another popular kernel is the Radial Basis Function (RBF) kernel. It is defined by\n",
    "$$K(\\mathbf{x}, \\mathbf{y}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{y} \\|}{2\\sigma^2}\\right).$$\n",
    "You should think of the RBF kernel as a similarity measure between data points\n",
    "where vectors which are close together with respect to the $l^2$-norm are similar\n",
    "(Notice how $0 \\leq K(\\mathbf{x}, \\mathbf{y}) \\leq 1$ and $K(\\mathbf{x}, \\mathbf{y}) \\to 1$ as\n",
    "$\\|\\mathbf{x} - \\mathbf{y} \\| \\to 0$). The hyperparameter $\\sigma$ controls\n",
    "the distribution of similarity as a function of distance. When you select\n",
    "a small value for $\\sigma$, only data points which are very close together are deemed to be similar.\n",
    "You should then expect to find a separating boundary which is \n",
    "more sensitive to the local topology of your dataset, and hence you could risk overfitting.\n",
    "When you select a large value of $\\sigma$, there is less of a difference in similarity\n",
    "between closer and farther data points. Your separating boundary will then be more influenced\n",
    "by long-range topological features, and hence you could risk underfitting. As usual,\n",
    "you will have to select $\\sigma$ through cross-validation! \n",
    "\n",
    "Let us revisit our \"circular\" dataset. First we'll train the same SVM as before\n",
    "using the dual formulation of the optimization problem. This amounts to defining\n",
    "a custom kernel. Then we'll try an RBF kernel as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = matrix(\n",
    "    np.matmul(\n",
    "        data_insep2_highdim[:, 3].reshape((-1, 1))\n",
    "        * data_insep2_highdim[:, :3],\n",
    "        (\n",
    "            data_insep2_highdim[:, 3].reshape((-1, 1))\n",
    "            * data_insep2_highdim[:, :3]\n",
    "        ).T,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = matrix(-np.ones(shape=(2 * n_samples_per_class, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = matrix(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            -np.identity(2 * n_samples_per_class),\n",
    "            np.identity(2 * n_samples_per_class),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    ")\n",
    "h = matrix(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            np.zeros(shape=(2 * n_samples_per_class, 1)),\n",
    "            C * np.ones(shape=(2 * n_samples_per_class, 1)),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    ")\n",
    "A = matrix(data_insep2_highdim[:, 3].reshape((1, -1)))\n",
    "b = matrix(np.zeros(shape=(1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solution = qp(P, q, G, h, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.sum(\n",
    "    np.array(solution[\"x\"])\n",
    "    * data_insep2_highdim[:, 3].reshape((-1, 1))\n",
    "    * data_insep2_highdim[:, :3],\n",
    "    axis=0,\n",
    ").reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = (np.array(solution[\"x\"]) > 0.001) & (\n",
    "    np.abs(np.array(solution[\"x\"])) < 0.1 - 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (\n",
    "    -data_insep2_highdim[:, 3].reshape((-1, 1))\n",
    "    * (data_insep2_highdim[:, :3] @ w)\n",
    "    + 1\n",
    ") / data_insep2_highdim[:, 3].reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Z[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = np.mean(Z[select])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the plane with equation $0.17X_1^2 - 1.08 X_2 + 0.13 X_2^2 + 0.68 = 0$,\n",
    "and everything checks out :) Now let's use an RBF kernel instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(x, y, sigma):\n",
    "    return np.exp(-np.sum((x - y) ** 2, axis=1) / (2 * sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.zeros(shape=(2 * n_samples_per_class, 2 * n_samples_per_class))\n",
    "for i in range(2 * n_samples_per_class):\n",
    "    for j in range(2 * n_samples_per_class):\n",
    "        P[i, j] = K(\n",
    "            data_insep2[i, :2].reshape((1, -1)),\n",
    "            data_insep2[j, :2].reshape((1, -1)),\n",
    "            sigma,\n",
    "        )\n",
    "P = matrix(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = matrix(-np.ones(shape=(2 * n_samples_per_class, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = matrix(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            -np.identity(2 * n_samples_per_class),\n",
    "            np.identity(2 * n_samples_per_class),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    ")\n",
    "h = matrix(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            np.zeros(shape=(2 * n_samples_per_class, 1)),\n",
    "            C * np.ones(shape=(2 * n_samples_per_class, 1)),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    ")\n",
    "A = matrix(data_insep2[:, 2].reshape((1, -1)))\n",
    "b = matrix(np.zeros(shape=(1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solution = qp(P, q, G, h, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is not very optimized :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([[1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([x_test for i in range(2 * n_samples_per_class)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = (np.array(solution[\"x\"]) > 0.001) & (\n",
    "    np.array(solution[\"x\"]) < C - 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "for i in range(2 * n_samples_per_class):\n",
    "    if select[i]:\n",
    "        x = np.repeat(\n",
    "            data_insep2[i, :2].reshape(1, -1),\n",
    "            repeats=2 * n_samples_per_class,\n",
    "            axis=0,\n",
    "        )\n",
    "        b.append(\n",
    "            (\n",
    "                -data_insep2[i, 2]\n",
    "                * np.sum(\n",
    "                    np.array(solution[\"x\"]).reshape(-1)\n",
    "                    * data_insep2[:, 2]\n",
    "                    * K(data_insep2[:, :2], x, sigma)\n",
    "                )\n",
    "                + 1\n",
    "            )\n",
    "            / data_insep2[i, 2]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.mean(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = np.repeat(x.reshape((1, -1)), repeats=2 * n_samples_per_class, axis=0)\n",
    "    return (\n",
    "        np.sum(\n",
    "            K(data_insep2[:, :2], x, sigma=sigma)\n",
    "            * data_insep2[:, 2]\n",
    "            * np.array(solution[\"x\"]).reshape(-1)\n",
    "        )\n",
    "        + b\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "x1_cont = np.arange(-6, 6, delta)\n",
    "x2_cont = np.arange(-2, 10, delta)\n",
    "X1_cont, X2_cont = np.meshgrid(x1_cont, x2_cont)\n",
    "Z = np.zeros(shape=(240, 240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(240):\n",
    "    for j in range(240):\n",
    "        Z[i, j] = f(np.array([X1_cont[i, j], X2_cont[i, j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(\n",
    "    x=data_insep2[:n_samples_per_class, 0],\n",
    "    y=data_insep2[:n_samples_per_class, 1],\n",
    "    c=\"green\",\n",
    "    label=\"$Y = 1$\",\n",
    ")\n",
    "plt.scatter(\n",
    "    x=data_insep2[n_samples_per_class:, 0],\n",
    "    y=data_insep2[n_samples_per_class:, 1],\n",
    "    c=\"red\",\n",
    "    label=\"$Y = -1$\",\n",
    ")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "linesvc = plt.contour(X1_cont, X2_cont, Z, levels=[0], colors=(\"#e88127\"))\n",
    "linesvc.collections[0].set_label(\"RBF boundary\")\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Investigate the effect of $\\sigma$ by varying its value. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "This tutorial is largely based on the material found in https://www.di.ens.fr/~mallat/papiers/svmtutorial.pdf.\n",
    "Many thanks to Ben G, Laurence and Milica for reviewing an earlier version of this post. Credits to Laurence for beautiful `fig1` and `fig2`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
