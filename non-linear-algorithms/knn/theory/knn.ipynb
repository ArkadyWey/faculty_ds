{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighours (KNN) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(\n",
    "    filename=\"images/image_0.jpg\",\n",
    "    width=500,\n",
    "    height=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbours (KNN)** is an algorithm that belongs to the group of supervised algorithms and can be used to solve classification and regression problems. In the case of classification, the label of unseen data is assigned by observing the neighbours that surround that data (the closest K labelled points) and take the label of majority vote (the most prevalent label is assigned to the unseen data). In the case of regression, the average of the values of K nearest neighours is assigned to the unseen data. This algorithm is used in variety of applications such as healthcare, finance, political science, etc. For example, in finance it can be applied to predict the credit rating of customers or in the case of political science, whether a person will vote or not for some political party.\n",
    "\n",
    "\n",
    "**K-Nearest Neighbours (KNN)** is called a *non-parametric* model because there are no parameters to train. Sometimes it is called *lazy learner* because it doesn’t learn a discriminative function from the training data but “memorizes” the training dataset instead. Note that a lazy learner does not have a training phase and the prediction step in **KNN** is relatively expensive. Each time the prediction needs to be made the algorihtm searches for the nearest neighbour(s) in the entire training set. \n",
    "\n",
    "In order to classify the new point, **KNN** needs the following: training data, predefined number of neighbours (*K*) and a distance metric. For example, imagine that you have training points and you have a point (marked as a black cross on the Figure below) for which you want to define a class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\n",
    "    filename=\"images/image_1.png\",\n",
    "    width=250,\n",
    "    height=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Theoretical Approach: How does KNN work?\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters that first need to be defined are: K - number of neighbours and D - distance metric. Let's assume that the training data can be represented as $\\left \\{ x_{i}, y_{i} \\right \\}$, where $x_{i}$ is a set of attributes that describes the training data point, $y_{i}$ is its class label, and $i=\\left \\{ 1,...,I \\right \\}$ where $I$ is the total number of training data points. The unlabelled data point is represented as $x$ and the following steps need to be taken to assign it a label:\n",
    "\n",
    "* Calculate its distance $D(x,x_{i})$ to all training data points according to some pre-agreed distance metric.\n",
    "   \n",
    "   \n",
    "* Seelect K training data points (closest neighbours) and their class labels which have the shortest distance from the unlabelled point $\\left \\{ x, y \\right \\}$.\n",
    "   \n",
    "   \n",
    "* To determine the class of this unlabelled point $y$, take the most frequent class in K neighbourhood (a majority vote)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Figure shows how the unlabelled data point (marked as black cross) has been classified in case the number of neighbours K is 1, 2 and 3. It can be seen that when K equals 3, the label of majority vote (marked as green circles) has been assigned to be the label of the data of intereset (black cross). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\n",
    "    filename=\"images/image_2.png\",\n",
    "    width=500,\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various distance metrics that can be used when applying **KNN** algorihtm. The common choice for numeric data is the Euclidean distance, while in case of categorical data the Hamming distance is used, but there are other distances that can be used as well. Here are some of the available distance metrics:\n",
    "\n",
    "* Euclidean distance: $\\sqrt{\\sum_{i=1}^{n}(a_{i} - b_{i})^{2}}$ \n",
    "\n",
    "\n",
    "* Manhattan distance: $\\sum_{i=1}^{n}\\left |a_{i} - b_{i} \\right |$ \n",
    "\n",
    "\n",
    "* Chebyshev distance: $\\underset{i}{max}(a_{i}-b_{i})$\n",
    "\n",
    "\n",
    "* Minkowski distance $\\left [\\sum_{i=1}^{n}(a_{i}-b_{i})^{p}  \\right ]^{p}$\n",
    "\n",
    "\n",
    "* Hamming distance $\\sum_{i=1}^{n}1_{a_{i}\\neq b_{i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the distance metrics the second parameter that needs to be defiend is the number of neighbours K. It is well known that choosing low values of K makes algorithm more sensitive (complex model) to outliers (the noise in our data) which can cause overfitting, while selecting higher K values makes algorithm more resilient to outliers (more voters participate in prediction) and can lead to underfitting. So, how to know which distance and K to use? Perform **cross-validation** which is explained in the upcoming example.\n",
    "\n",
    "\n",
    "**Cross-validation** is a smart way to find out the optimal K value. It estimates the validation error rate by holding out a subset of the training set from the model building process. Cross-validation (let's say 10 fold validation) involves randomly dividing the training set into 10 groups, or folds, of approximately equal size. 90% data is used to train the model and remaining 10% is used to validate it. The misclassification rate is then computed on the 10% validation data. This procedure is repeated 10 times. Different group of observations are treated as a validation set each of the 10 times. It results in 10 estimates of the validation error which are then averaged out.\n",
    "\n",
    "\n",
    "K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbour algorithm. \n",
    "\n",
    "\n",
    "The number of neighbours in KNN is a hyperparameter that you need to choose at the time of model building. Research has shown that there is no optimal number of neighbours that is suitable for all kinds of data sets, as each dataset has it's own requirements. In the case of a small number of neighbours, the noise will have a higher influence on the result, while a large number of neighbours make it computationally expensive. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Pros and Cons\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Pros**:\n",
    "\n",
    "* Easy to understand (use geometric arguments, no equations).\n",
    "\n",
    "\n",
    "* No assumptions about data (when there is little or no prior knowledge about the distribution data, non-parametric KNN can be useful in case of nonlinear data).\n",
    "\n",
    "\n",
    "* Used to solve both classification and regression problems. It can be used to solve multi-class problems as well.\n",
    "\n",
    "\n",
    "* Sometimes competitive with state of the art classifiers.\n",
    "\n",
    "\n",
    "\n",
    "**KNN Cons**:\n",
    "\n",
    "\n",
    "* Performance highly dependent on K and distance metrics. Computattionally expensive: memory - high memory requirement to store all the training data, and time - needs to compute the distance to all training data.\n",
    "\n",
    "\n",
    "* Inefficient if needed to classify large data sets (prediction stage might be slow).\n",
    "\n",
    "\n",
    "* Accuracy degraded by noisy/irrelevant attributes. Sensitive to irrelevant features and the scale of the data (not suitable for large dimensional data).\n",
    "\n",
    "\n",
    "**Some considerations:**\n",
    "\n",
    "\n",
    "Consider using PCA to reduce dimensions - KNN performs better with a lower number of features than a large number of features. In order to cope with the problem of highly dimenisonal data, Principal Component Analysis (PCA) or feature selection can be applied. Also, another important requirement is a feature scaling (data normalisation) - it might happen that units of measure differ between features, which contributes that those features dominate in their contribution to the distance measure, thus unabling estimator to learn from other features correctly as expected. An example would be, a feature that represents a hight of the person (*cms*), and a feature that refers to the weight of the person (*kgs). The height will have greater influence on distance calculation than the weight of a person. Some of the methods used to standardise the data are: \n",
    "\n",
    "* **Min-Max Normalisation**: $x^{'}= \\frac{x-min(x)}{max(x)-min(x)}$,\n",
    "\n",
    "\n",
    "* **Mean Normalisation**: $x^{'}= \\frac{x-average(x)}{max(x)-min(x)}$, \n",
    "\n",
    "where $x$ and $x^{'}$ are the original and normalised values, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Pratical Approach: Case Study\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to see the practical approach of using KNN algorithm to solve classification problem, that is to **predict whether a patient has a diabetes or not**. Firstly,  we apply data cleaning, for example coping with missing data, feature scaling, and we familiarise with the features in diabetes dataset, for example plotting distributions of the features. This is described in **Data Preparation** section. The second section, called **Hyper-parameter Tuning** shows some ways how to select the values for number of neighbours K, such as for example using cross-validation. Finally, the third section, **Train and Test KNN** is reserved for training and testing KNN classifier. Training dataset is used to generate predictions, while the test dataset is used to evaluate the performance of KNN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Predict whether the patient has diabetes based on diagnostic measures.\n",
    "\n",
    "**Dataset**: The dataset is composed of *medical predictor variables* (features) and one *target variable* (class label). The data is collected for female patients of Pima Indian hertiage that are at least 21 years old, and originally it is from the National Institute of Diabetes and Digestive and Kidney Diseases. Here is the brief description of *medical predictor variable* and *target variable*:\n",
    "\n",
    "* **Pregnancies**: Number of pregnancies the patient has had\n",
    "\n",
    "* **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "\n",
    "* **BloodPressure**: Diastolic blood pressure (mm Hg)\n",
    "\n",
    "* **SkinThickness**: Triceps skin fold thickness (mm)\n",
    "\n",
    "* **Insulin**: 2-Hour serum insulin (mu U/ml)\n",
    "\n",
    "* **BMI**: Body mass index (weight in kg/(height in m)^2)\n",
    "\n",
    "* **DiabetesPedigreeFunction**: Diabetes pedigree function\n",
    "\n",
    "* **Age**: Age (years)\n",
    "\n",
    "* **Outcome**: Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "df = pd.read_csv(\"https://s3-eu-west-1.amazonaws.com/faculty-client-teaching-materials/non-linear-algorithms/diabetes_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of dataframe.\n",
    "print(\"dimension of diabetes data: {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 8 columns represent the features and the last column\n",
    "# represent the target label.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column name ‘Outcome’ to ‘diabetes’ and set 1 to be 'Yes',\n",
    "# and 0  to be 'No'.\n",
    "df = df.rename({\"Outcome\": \"Diabetes\"}, axis=1)\n",
    "df[\"Diabetes\"] = df[\"Diabetes\"].replace(1, \"Yes\")\n",
    "df[\"Diabetes\"] = df[\"Diabetes\"].replace(0, \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of target variable.\n",
    "diabetes = pd.DataFrame(df[\"Diabetes\"].value_counts())\n",
    "\n",
    "plt.figure()\n",
    "diabetes.plot(color=\"#FA7268\", kind=\"bar\")\n",
    "plt.xlabel(\"Diabetes\")\n",
    "plt.ylabel(\"Number of outcomes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the classes are fairly divided, which means there is no problem of unbalanced classes in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values.\n",
    "df[\"Glucose\"].fillna(df[\"Glucose\"].mean(), inplace=True)\n",
    "df[\"BloodPressure\"].fillna(df[\"BloodPressure\"].median(), inplace=True)\n",
    "df[\"SkinThickness\"].fillna(df[\"SkinThickness\"].median(), inplace=True)\n",
    "df[\"Insulin\"].fillna(df[\"Insulin\"].mean(), inplace=True)\n",
    "df[\"BMI\"].fillna(df[\"BMI\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check statistics.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns[:8]\n",
    "plt.subplots(figsize=(18, 15))\n",
    "length = len(columns)\n",
    "\n",
    "# Plot distribution of each feature.\n",
    "for i, j in zip(columns, range(length)):\n",
    "\n",
    "    plt.subplot((length / 2), 3, j + 1)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "    df[i].hist(color=\"#FA7268\", bins=20, edgecolor=\"black\")\n",
    "    plt.title(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lens summary.\n",
    "ls = lens.summarise(df, scheduler=\"sync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a lens summary into an explorer object.\n",
    "explorer = lens.explore(ls)\n",
    "explorer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of feature statistic.\n",
    "explorer.column_details(\"DiabetesPedigreeFunction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation between each variables.\n",
    "explorer.correlation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As shown in Figure below, age is highly correlated with pregnancies (0,61).\n",
    "# Also, glucose is highly correlated with insulin (0.4) and\n",
    "# skin thickness is correlated with BMI (0.55).\n",
    "# explorer.correlation_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply feature scaling.\n",
    "rescaled_features = StandardScaler().fit_transform(\n",
    "    df[\n",
    "        [\n",
    "            \"Glucose\",\n",
    "            \"Insulin\",\n",
    "            \"BMI\",\n",
    "            \"Age\",\n",
    "            \"BloodPressure\",\n",
    "            \"SkinThickness\",\n",
    "            \"Pregnancies\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(\n",
    "    data=rescaled_features,\n",
    "    columns=df[\n",
    "        [\n",
    "            \"Glucose\",\n",
    "            \"Insulin\",\n",
    "            \"BMI\",\n",
    "            \"Age\",\n",
    "            \"BloodPressure\",\n",
    "            \"SkinThickness\",\n",
    "            \"Pregnancies\",\n",
    "        ]\n",
    "    ].columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.concat(\n",
    "    [features, df[[\"DiabetesPedigreeFunction\", \"Diabetes\"]]], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get fetaures of the dataframe.\n",
    "X = df_.drop(labels=\"Diabetes\", axis=1)\n",
    "\n",
    "# Get target labels of the dataframe.\n",
    "Y = df_[\"Diabetes\"].values\n",
    "\n",
    "# Split data into train and test datasets.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, random_state=22, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select K using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Number of neighbours is a parameter to be tuned.\n",
    "n_neighbors = 90\n",
    "num_neighbors = list(range(1, n_neighbors + 1))\n",
    "\n",
    "# List that stores mean of cv scores for K neighbors.\n",
    "cv_scores = []\n",
    "\n",
    "# Loop over different values of K.\n",
    "for i in range(1, len(num_neighbors) + 1):\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "\n",
    "    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results.\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.plot(num_neighbors, cv_scores)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Neighbors (K)\")\n",
    "plt.ylabel(\"Cross-Validated Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Best accuracy is {} with number of neighbours = {}\".format(\n",
    "        max(cv_scores), cv_scores.index(max(cv_scores)) + 1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KNN classifier.\n",
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on training data.\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the unlabeled data.\n",
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy score is: {}\".format(accuracy_score(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://learn.sherlockml.com/tutorials/knn\n",
    "\n",
    "\n",
    "* https://docs.sherlockml.com/libraries/lens/getting_started.html\n",
    "\n",
    "\n",
    "* https://www.youtube.com/watch?v=k_7gMp5wh5A&list=PLBv09BD7ez_68OwSB97WXyIOvvI5nqi-3\n",
    "\n",
    "\n",
    "* https://scikit-learn.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
