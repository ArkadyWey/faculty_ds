{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting income from census data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge you will be working with the popular machine learning dataset from the 1994 Adult Census database. Given a set of socio-economic attributes you will try to predict if the annual income of an individual is less than or greater than $50,000. \n",
    "\n",
    "The following attributes are present in the dataset: \n",
    "<ol> \n",
    "<li><b>age</b>: continuous.</li>\n",
    "<li><b>workclass</b>: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.</li>\n",
    "<li><b>fnlwgt</b>: continuous.</li>\n",
    "<li><b>education</b>: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.</li>\n",
    "<li><b>education-num</b>: numerical version of education.</li>\n",
    "<li><b>marital-status</b>: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.</li>\n",
    "<li><b>occupation</b>: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.</li>\n",
    "<li><b>relationship</b>: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.</li>\n",
    "<li><b>race</b>: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.</li>\n",
    "<li><b>sex</b>: Female, Male.</li>\n",
    "<li><b>capital-gain</b>: continuous.</li>\n",
    "<li><b>capital-loss</b>: continuous.</li>\n",
    "<li><b>hours-per-week</b>: continuous.</li>\n",
    "<li><b>native-country</b>: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.</li>\n",
    "<li><b> income </b>: >50K, <=50K.</li>\n",
    "</ol>\n",
    "\n",
    "Citation:\n",
    "<ul>\n",
    "<li>This dataset has been taken from repository Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.</li>\n",
    "<li>Ron Kohavi, \"Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\", Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996.</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to import more packages (i.e., numpy, sklearn packages) as required.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = 'https://s3-eu-west-1.amazonaws.com/fellowship-teaching-materials/data-practical/adult.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the csv file into a DataFrame\n",
    "df = pd.read_csv(path_, header='infer', index_col=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to perform some quality checks on the data, e.g., missing values, duplications etc. Try to find out some basic insights about the data so that you can make more informed decisions about the machine learning task.\n",
    "</n>\n",
    "\n",
    "The Pandas data analysis library has many built-in functions that facilitate faster and easier data manipulation and exploration. \n",
    "</n>\n",
    "\n",
    "For creating plots you can use any python plotting library that you are familiar with, e.g., matplotlib, seaborn, Pandas also has its own built in plotting functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for missing values or duplicate rows using pandas and remove accordingly if any are found\n",
    "print(f'Null values: {df.isnull().values.sum()}')\n",
    "print(f'Number of duplicated rows: {sum(df.duplicated(keep=\"first\"))}')\n",
    "df.drop_duplicates(keep=\"first\",inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get some basic statistics on your continuous variables using pandas\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms for your continuous variables\n",
    "# You can use any python plot library for this\n",
    "# Below we make use of the built-in Pandas plotting functions\n",
    "cont_cols = ['age', 'fnlwgt', 'educationNum', 'capitalGain', 'capitalLoss','hoursPerWeek']\n",
    "fig, axes = plt.subplots(ncols=2,nrows=3, figsize=[20,20])\n",
    "axes = np.ravel(axes)\n",
    "for idx, col in enumerate(cont_cols):\n",
    "    df[col].plot(kind='hist', density=True, ax=axes[idx], title=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar charts of you categorical variables\n",
    "# It might be useful to compare the values of each category given the target variable (income)\n",
    "\n",
    "# set as categories\n",
    "cat_cols = ['workclass', 'education', 'maritalStatus', 'occupation', 'relationship', 'race', 'sex', 'nativeCountry']\n",
    "df[cat_cols] = df[cat_cols].astype(\"category\")\n",
    "\n",
    "# this time we plot with seaborn to try an alternative approach\n",
    "fig, axes = plt.subplots(ncols=2,nrows=4, figsize=[20,20])\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    sns.countplot(x=cat_cols[idx], hue='income', data=df, ax=ax)\n",
    "    if cat_cols[idx] != 'sex':\n",
    "        ax.xaxis.set_tick_params(rotation=70)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What have you learned from these plots? Do you already have some insights about which demographics are more likely to earn over $50k? Are there any features that seem redundant, uninformative or unuseable for any other reason? What about the target variable, income?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the class balance of your data. \n",
    "df.income.value_counts().plot(kind='bar')\n",
    "plt.xticks(rotation=360);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know a little more about our data it is time to preprocess it for our classification task. Consider which feature engineering steps you will need to take to ensure that the data is in the right format, for example, how should categorical variables be treated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe that will be used for training ML model\n",
    "df_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the continuous variables that you wish to keep as features into the new dataframe\n",
    "# consider if you would like to threshold any of these into binary variables\n",
    "df_data[cont_cols] = df[cont_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the categorical variables you want to the new dataframe\n",
    "# they need to be converted into numerical values and one-hot-encoded (again, pandas has built in functions for this)\n",
    "df_data['sex'] = df['sex'].copy()\n",
    "df_data = pd.get_dummies(df_data, columns=['sex'], drop_first=True)\n",
    "cat_cols.remove('sex')\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# educationNum seems to be a numeric encoding of the education category\n",
    "# we can therefore drop the latter.\n",
    "print(df[['educationNum','education']].value_counts())\n",
    "cat_cols.remove('education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and encode the rest of the categorical columns\n",
    "df_data[cat_cols] = df[cat_cols].copy()\n",
    "df_data = pd.get_dummies(df_data, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, add the output variable with one-hot-encoding. \n",
    "df_data['income'] = df['income'].copy()\n",
    "# make sure to set drop_first=True to avoid creating unnecessary columns.\n",
    "df_data = pd.get_dummies(df_data, columns=['income'], drop_first=True)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test, validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin selecting and optimising a machine learning model, you should split your data into train, test (and maybe validation) sets. \n",
    "\n",
    "In some cases, you may only need a training and a validation set. For example, perhaps the test data has been held out from the beginning, as in some competitions. You may also choose to just use a train/test split and utilise cross validation methods on your training data. \n",
    "\n",
    "The exact ratios for each dataset will depend on the amount of available data and specifics of the problem but an 80/20 train/test split is a good rule of thumb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train/test sets and separate the features from the target. \n",
    "X = df_data.drop('income_ >50K', axis=1)\n",
    "y = df_data['income_ >50K']\n",
    "# fixing your random state ensures you will get the same split every time you run this line.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many classification algorithms that could be used for this problem. It is up to you to decide which methods are most suitable for this binary classification task given what you have learned about the data so far.\n",
    "\n",
    "In general [sklearn](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) can be used to quickly test different types of model. We suggest using cross validation to compare the performance of a few classifiers on the training data, without worrying too much about hyperparameter tuning at this stage. \n",
    "\n",
    "Try to pick at least 3 models that are different in some significant way. Depending on which models you choose, you may need some extra preprocessing steps, e.g., normalising the data.\n",
    "\n",
    "You will need to consider what the important performance metrics are for a classification problem, and use these to decide which model is best for the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train different models using cross validation\n",
    "# model 1\n",
    "y_svc = cross_val_predict(SVC(), X_train, y_train, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "y_dtc = cross_val_predict(DecisionTreeClassifier(), X_train, y_train, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 3\n",
    "y_rfc = cross_val_predict(RandomForestClassifier(), X_train, y_train, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(y_pred, y_true):\n",
    "    \"\"\"Returns a dict containing the accuracy, precision, recall, and f1 score of \n",
    "    a model prediction.\"\"\"\n",
    "    # compute performance metrics of model\n",
    "    performance = {}\n",
    "\n",
    "    performance['accuracy'] = round(metrics.accuracy_score(y_true, y_pred) * 100, 2)\n",
    "    performance['precision'] = round(metrics.precision_score(y_true, y_pred), 2)\n",
    "    performance['recall'] = round(metrics.recall_score(y_true, y_pred), 2)\n",
    "    performance['f1score'] = round(metrics.f1_score(y_true, y_pred), 2)\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the performance of the models\n",
    "performance_dict = {}\n",
    "performance_dict['SVC'] = get_performance(y_svc, y_train)\n",
    "performance_dict['DecisionTree'] = get_performance(y_dtc, y_train)\n",
    "performance_dict['RandomForest'] = get_performance(y_rfc, y_train)\n",
    "df_performance = pd.DataFrame(performance_dict)\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these initial results, which model do you think is best to proceed with? \n",
    "\n",
    "Do you have any thoughts about why a certain model might be performing better at this problem than another. \n",
    "\n",
    "What are the limitations of each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select your best model from the above and see if you can increase its performance using hyper parameter tuning. You may find this [link](https://scikit-learn.org/stable/modules/grid_search.html) helpful. Depending on your model, doing an exhaustive grid search might take a very long time. Consider limiting your grid size by either selecting one or two of the hyperparameters that you think are most important or searching over small value range for each hyper parameter. Alternatively, you could try a randomised grid search to speed things up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a grid search on your hyperparameter space.\n",
    "param_grid = {'n_estimators': [100, 500], \n",
    "              'max_features': ['auto', 'sqrt'], \n",
    "              'min_samples_split': [5, 10]} # probably too small a grid for any significant improvements\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "tuned_model = GridSearchCV(clf, param_grid, cv=5, n_jobs=4)\n",
    "tuned_model.fit(X_train, y_train)\n",
    "print(f'Best Parameter values: {tuned_model.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the performance of your baseline model and the tuned model on the test set. Why is it imporant to compare performance on held out data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_baseline = RandomForestClassifier()\n",
    "clf_baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance metrics\n",
    "y_test_baseline = clf_baseline.predict(X_test)\n",
    "y_test_tuned = tuned_model.predict(X_test)\n",
    "performance_dict['RFC_test_baseline'] = get_performance(y_test_baseline, y_test)\n",
    "performance_dict['RFC_test_tuned'] = get_performance(y_test_tuned, y_test)\n",
    "df_performance = pd.DataFrame(performance_dict)\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC vs precision-recall\n",
    "\n",
    "Draw the precision-recall curve and ROC curve for the classifiers and calculate the area under the curve in both cases. Which curve do you think is more appropriate for this problem and how might the choice effect your evaluation of the model? (<b>Hint</b>: consider your class balance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_baseline = clf_baseline.predict_proba(X_test)[:,1]\n",
    "y_proba_tuned = tuned_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_b, recall_b, thresholds_b = metrics.precision_recall_curve(y_test, y_proba_baseline)\n",
    "precision_t, recall_t, thresholds_t = metrics.precision_recall_curve(y_test, y_proba_tuned)\n",
    "fpr_b, tpr_b, thresholds = metrics.roc_curve(y_test, y_proba_baseline)\n",
    "fpr_t, tpr_t, thresholds = metrics.roc_curve(y_test, y_proba_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[20,10])\n",
    "axes[0].plot(recall_t, precision_t, \"-r\", label='tuned')\n",
    "axes[0].plot(recall_b, precision_b, \"-g\", label='baseline')\n",
    "axes[0].plot([0,1],[1,0], '--')\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('Precision recall curve on test data')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(fpr_t, tpr_t, \"-r\", label='tuned')\n",
    "axes[1].plot(fpr_b, tpr_b, \"-g\", label='baseline')\n",
    "axes[1].plot([0, 1], [0, 1],'--')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC curve on test data');\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Area under precision-recall curve: {metrics.auc(recall_t, precision_t)}')\n",
    "print(f'Area under ROC curve: {metrics.auc(fpr_t, tpr_t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
