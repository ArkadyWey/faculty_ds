{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nb_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The purpose of the following exercises is to gain familiarity with some core concepts underlying linear regression (as well as more sophisticated machine learning algorithms). You'll practice implementing gradient descent, coding models with the sklearn library, engineering features, and regularising. The dataset you'll use is [from Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques) and describes houses that were sold in Ames, Iowa. The modelling objective is to use available features to predict the sale price of a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages and functions you may find useful\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the housing dataset\n",
    "data = pd.read_csv(\"../housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Spend a little time exploring the dataset. What features do you think could be predictive of sale price? Are there potential issues to flag, e.g. missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Make a scatterplot of `SalePrice` vs. `LotArea`. Does there appear to be a linear relationship? Is there a transformation we can apply to make the data appear more linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.assign(\n",
    "    LogLotArea=data.LotArea.apply(np.log),\n",
    "    LogSalePrice=data.SalePrice.apply(np.log),\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(data.LogLotArea, data.LogSalePrice)\n",
    "plt.title(\"Housing Prices in Ames, Iowa\")\n",
    "plt.xlabel(\"log lot area (square feet)\")\n",
    "plt.ylabel(\"log sale price (dollars)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Divide the data into training and test sets. A train-test split of at least 90-10 is recommended. You may want to further divide the data into 80-10-10 training-validation-test sets if you'd like to tune hyperparameters or select the best model in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in data.columns if \"SalePrice\" not in col]\n",
    "X = data[features]\n",
    "Y = data.LogSalePrice\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression on one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of demonstration, we'll start by just using log `LotArea` to predict log `SalePrice`. As you saw above, the dataset offers many more features that could be useful, so a regression in just one variable may not be the best performing. Towards the end of the exercises, you'll have an opportunity to develop your own regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first pass, let's fit a regression model without bias term, i.e.\n",
    "$$log(SalePrice) = \\theta * log(LogLotArea)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** In the cell below, define the MSE loss function, the gradient of the loss function with respect to the model parameter $\\theta$, and a function that performs gradient descent. The type hints and docstrings suggest a way to implement these functions, but feel free to write them your own way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fcn(y: pd.Series, x: pd.Series, theta: float) -> float:\n",
    "    \"\"\"MSE for univariate linear regression without bias term.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: pd.Series\n",
    "        The ground truth.\n",
    "    x: pd.Series\n",
    "        Feature observations.\n",
    "    theta: float\n",
    "        The regression parameter.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The MSE for the regression on the input data.\n",
    "    \"\"\"\n",
    "    return np.sum((y - x * theta) ** 2) / len(y)\n",
    "\n",
    "\n",
    "def grad_loss_fcn(y: pd.Series, x: pd.Series, theta: float) -> float:\n",
    "    \"\"\"Gradient of the MSE loss function with respect to theta.\"\"\"\n",
    "    return -2 * np.sum(x * (y - theta * x)) / len(y)\n",
    "\n",
    "\n",
    "def gradient_descent(\n",
    "    y: pd.Series,\n",
    "    x: pd.Series,\n",
    "    init: float,\n",
    "    alpha: float,\n",
    "    steps: int,\n",
    ") -> list:\n",
    "    \"\"\"Gradient descent algorithm for a univariate linear regression without bias.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: pd.Series\n",
    "        The ground truth.\n",
    "    x: pd.Series\n",
    "        Feature observations.\n",
    "    init: float\n",
    "        Initial value of the regression parameter.\n",
    "    alpha: float\n",
    "        Learning rate.\n",
    "    steps: int\n",
    "        Number of iterations of the algorithm.\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of updated values for theta from gradient descent.\n",
    "    \"\"\"\n",
    "    theta = init\n",
    "    descent = [theta]\n",
    "\n",
    "    for n in range(steps):\n",
    "        theta -= alpha * grad_loss_fcn(y, x, theta)\n",
    "        descent.append(theta)\n",
    "\n",
    "    return descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using gradient descent function you defined above, find the optimal value for $\\theta$. You may have to experiment with the learning rate and number of steps in order to achieve convergence. Try initialising $\\theta$ at different values. Plot the loss function and plot the loss at each stage of the gradient descent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linspace(-30, 30, 100)\n",
    "loss_graph = np.vectorize(lambda theta: loss_fcn(Y_train, X_train.LogLotArea, theta))(\n",
    "    theta\n",
    ")\n",
    "\n",
    "descent = np.array(gradient_descent(Y_train, X_train.LogLotArea, -25, 10 ** (-3), 100))\n",
    "descent_values = np.vectorize(\n",
    "    lambda theta: loss_fcn(Y_train, X_train.LogLotArea, theta)\n",
    ")(descent)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(theta, loss_graph, label=\"loss function\")\n",
    "plt.scatter(descent, descent_values, c=\"#F55C5C\", label=\"descent values\")\n",
    "plt.title(\"Gradient descent\")\n",
    "plt.xlabel(\"theta\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Plot the regression you optimised by gradient descent against the data. Evaluate the performance of your model by a metric of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(7, 13, 100)\n",
    "regression_graph = descent[-1] * x\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_train.LogLotArea, Y_train, label=\"train data\")\n",
    "plt.scatter(\n",
    "    X_test.LogLotArea, Y_test, facecolors=\"none\", edgecolors=\"black\", label=\"test data\"\n",
    ")\n",
    "plt.plot(x, regression_graph, color=\"#F55C5C\", label=\"regression\")\n",
    "plt.title(\"Housing Prices in Ames, Iowa\")\n",
    "plt.xlabel(\"log lot area (square feet)\")\n",
    "plt.ylabel(\"log sale price (dollars)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance by MSE\n",
    "train_mse = loss_fcn(Y_train, X_train.LogLotArea, descent[-1])\n",
    "test_mse = loss_fcn(Y_test, X_test.LogLotArea, descent[-1])\n",
    "train_mse, test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's great that you can implement linear regression from scratch, but in practice, you'll probably use a library like `sklearn`. The process of fitting and making predictions is essentially the same with `sklearn` no matter what the modelling task is.\n",
    "1. Instantiate the model class with whatever hyperparameters you choose.\n",
    "2. Call the fit method on the training data.\n",
    "3. Call the predict method on the test data.\n",
    "\n",
    "When in doubt about how to use anything in `sklearn`, refer to the documentation or google your question!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use the `LinearRegression` class from `sklearn` to regress log `SalePrice` onto log `LotArea`, and compare the model to the one you developed above. Note that default in `sklearn` is to fit a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression.fit(X_train.LogLotArea.values.reshape(-1, 1), Y_train)\n",
    "\n",
    "x = np.linspace(7, 13, 100)\n",
    "preds = regression.coef_ * x + regression.intercept_\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_train.LogLotArea, Y_train, label=\"train data\")\n",
    "plt.scatter(\n",
    "    X_test.LogLotArea, Y_test, facecolors=\"none\", edgecolors=\"black\", label=\"test data\"\n",
    ")\n",
    "plt.plot(x, preds, color=\"#F55C5C\", label=\"regression\")\n",
    "plt.title(\"Housing Prices in Ames, Iowa\")\n",
    "plt.xlabel(\"log lot area (square feet)\")\n",
    "plt.ylabel(\"log sale price (dollars)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = mean_squared_error(\n",
    "    Y_train, regression.predict(X_train.LogLotArea.values.reshape(-1, 1))\n",
    ")\n",
    "test_mse = mean_squared_error(\n",
    "    Y_test, regression.predict(X_test.LogLotArea.values.reshape(-1, 1))\n",
    ")\n",
    "train_mse, test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** In training the model with an MSE loss function, we assume that the residuals are normally distributed. Assess whether this is a reasonable assumption for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regression.predict(X_train.LogLotArea.values.reshape(-1, 1))\n",
    "residuals = Y_train - y_pred\n",
    "\n",
    "normal = norm(loc=0, scale=residuals.std())\n",
    "x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(residuals, bins=20, density=True, label=\"train residuals\")\n",
    "plt.plot(x, normal.pdf(x), label=\"normal dist\", c=\"#F55C5C\", linewidth=1.5)\n",
    "plt.title(\"Prediction error on training set\")\n",
    "plt.xlabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can get a better fit by introducing higher powers of log `LotArea` as features. The form of the regression will then be\n",
    "$$ log(SalePrice) = \\theta_0 + \\theta_1 * log(LotArea) + \\dots + \\theta_n * log(LotArea)^n$$\n",
    "for some $n \\geq 1$. With higher order features, there's a risk of overfitting the model to the training data, so we should consider using a regularisation method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Engineer polynomial features in log `LotArea` and fit a linear regression. When working with multiple features, it's good practice to scale them before fitting a model. You may want to use the `Ridge` or `Lasso` class from `sklearn` to regularise the fit and then experiment with the regularisation parameter. Compare the contribution of each feature to the model prediction. Evaluate your model's performance, comparing to the performance of the model you developed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale log LotArea and add higher order features\n",
    "\n",
    "\n",
    "def add_poly_features(data: pd.DataFrame, feature: str, max_power: int) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "    for n in range(2, max_power + 1):\n",
    "        df.loc[:, f\"{feature}{n}\"] = df[feature] ** n\n",
    "    return df\n",
    "\n",
    "\n",
    "MAX_POWER = 9\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train = add_poly_features(X_train, \"LogLotArea\", MAX_POWER)\n",
    "x_train = x_train[[col for col in x_train if \"LogLotArea\" in col]]\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "x_test = add_poly_features(X_test, \"LogLotArea\", MAX_POWER)\n",
    "x_test = x_test[[col for col in x_test if \"LogLotArea\" in col]]\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression with varying levels of L2 regularisation.\n",
    "regression_dict = {}\n",
    "for i in range(-5, 6):\n",
    "    regression = Ridge(alpha=10**i)\n",
    "    regression.fit(x_train, Y_train)\n",
    "    regression_dict[i] = regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame({\"LogLotArea\": np.linspace(7, 13, 100)})\n",
    "x = add_poly_features(x, \"LogLotArea\", MAX_POWER)\n",
    "x = scaler.transform(x)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_train[:, 0], Y_train, label=\"train data\")\n",
    "plt.scatter(\n",
    "    x_test[:, 0], Y_test, facecolors=\"none\", edgecolors=\"black\", label=\"test data\"\n",
    ")\n",
    "\n",
    "for i, regression in regression_dict.items():\n",
    "    preds = regression.intercept_\n",
    "    for n in range(0, MAX_POWER):\n",
    "        preds += regression.coef_[n] * x[:, n]\n",
    "    plt.plot(x[:, 0], preds, label=f\"alpha=10^{i}\")\n",
    "\n",
    "plt.title(\"Housing Prices in Ames, Iowa\")\n",
    "plt.xlabel(\"log lot area (square feet)\")\n",
    "plt.ylabel(\"log sale price (dollars)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = regression_dict[-5]\n",
    "\n",
    "train_mse = mean_squared_error(\n",
    "    Y_train,\n",
    "    regression.predict(x_train),\n",
    ")\n",
    "test_mse = mean_squared_error(\n",
    "    Y_test,\n",
    "    regression.predict(x_test),\n",
    ")\n",
    "train_mse, test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension: develop your own regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now that we've pretty much exhausted the modelling possibilities with just one feature, develop your own regression model with all the features in the dataset at your disposal. Compare the performance of your model to the peformance of the models we produced above. Keep in mind the following tips as you train and evaluate your model.\n",
    "* Consider one-hot-encoding categorical features.\n",
    "* Features on different scales should be transformed so that they're comparable.\n",
    "* If tuning a hyperparameter like the regularisation parameter, it's best practice to further divide your training set into training and validation sets or to cross validate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
