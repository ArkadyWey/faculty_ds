{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nb_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the following exercises, you'll practice key skills in developing a logistic regression classifier. The dataset you'll use is from the classic [Titanic competition on Kaggle](https://www.kaggle.com/competitions/titanic/overview), containing information about passengers aboard the Titanic. The modelling objective is to use available features to predict whether a passenger survived or perished in the disaster. \n",
    "\n",
    "The format of this exercise notebook is similar to that of the linear regression notebook. You'll start by drawing your own decision boundary through feature space and then defining a logistic regression classifier. You'll then practice using the `sklearn` implementation of logistic regression and engineering more complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages and functions you may find useful\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the titanic dataset\n",
    "data = pd.read_csv(\"../titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Spend a little time exploring the dataset. What features do you think are predictive of survival? Are there potential issues to flag, e.g. missing data? Imbalanced classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Make a scatterplot of `Age` vs `Fare`, with points colored by survival status. Where would you draw a decision boundary? Consider applying a log transformation to `Fare` as a preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=\"Age\")\n",
    "data = data.assign(LogFare=np.log(data.Fare + 10))  # Add 10 to avoid taking log(0)\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(x=data.LogFare, y=data.Age, hue=data.Survived)\n",
    "plt.title(\"Titanic survival\")\n",
    "plt.xlabel(\"log transformed fare\")\n",
    "plt.ylabel(\"age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Divide the data into training and test sets. A train-test split of at least 90-10 is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[col for col in data.columns if col != \"Survived\"]]\n",
    "Y = data.Survived\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression on two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualise the decsion boundary in feature space, we'll limit the number of features to two: `Age` and `Fare`, transformed as above. If we choose a linear decision boundary, our regression will be of the form\n",
    "$$P(Survived=1|Age, Fare, \\theta) = \\big( 1 + exp(\\theta_0 + \\theta_1 * Age + \\theta_2 * log(Fare) \\big)^{-1}.$$\n",
    "You can then decide on an operating point to binarise the model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing a decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Based on the plot above, draw your own decision boundary, linear or otherwise. Then define a logistic regression classifier based on your decision boundary and a choice of operating point. Evaluate the classification accuracy of your model. Does it outperform the naive baseline model that just predicts no one survived?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(\n",
    "    x_1: pd.Series,\n",
    "    x_2: pd.Series,\n",
    "    slope: float,\n",
    "    intercept: float,\n",
    "    operating_point: float = 0.5,\n",
    "    return_proba: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"A logistic regression classifier in two variables.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_1: pd.Series\n",
    "        Feature 1 observations.\n",
    "    x_2: float\n",
    "        Feature 2 observations.\n",
    "    slope: float\n",
    "        Slope of the linear parametisation of x_2 by x_1 on\n",
    "        the decision boundary.\n",
    "    intercept: float\n",
    "        Intercept of the linear parameterisation of x_2 by x_1 on\n",
    "        the decision boundary.\n",
    "    operating_point: float, default 0.5\n",
    "        Threshold for turning probability into a binary output.\n",
    "    return_proba: bool, default False\n",
    "        Whether to return probability or a binary output.\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Class label 0 or 1.\n",
    "    \"\"\"\n",
    "    linear_fcn = -x_2 + slope * x_1 + intercept\n",
    "    proba = (1 + np.exp(-linear_fcn)) ** (-1)\n",
    "\n",
    "    if return_proba:\n",
    "        return proba\n",
    "    return (proba >= operating_point).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLOPE = 30\n",
    "INTERCEPT = -95\n",
    "\n",
    "x = np.linspace(0, 7, 100)\n",
    "decision_boundary = SLOPE * x + INTERCEPT\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(x=X_train.LogFare, y=X_train.Age, hue=Y_train)\n",
    "plt.plot(x, decision_boundary, color=\"black\")\n",
    "plt.title(\"Titanic survival\")\n",
    "plt.xlabel(\"log transformed fare\")\n",
    "plt.ylabel(\"age\")\n",
    "plt.ylim((0, 80))\n",
    "plt.xlim((2, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    num_wrong = np.sum(np.absolute(y_true - y_pred))\n",
    "    return 1 - num_wrong / len(y_true)\n",
    "\n",
    "\n",
    "train_accuracy = calc_accuracy(\n",
    "    Y_train, logistic_regression(X_train.LogFare, X_train.Age, SLOPE, INTERCEPT)\n",
    ")\n",
    "test_accuracy = calc_accuracy(\n",
    "    Y_test, logistic_regression(X_test.LogFare, X_test.Age, SLOPE, INTERCEPT)\n",
    ")\n",
    "train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the naive baseline model\n",
    "train_accuracy = np.sum(Y_train == 0) / len(Y_train)\n",
    "test_accuracy = np.sum(Y_test == 0) / len(Y_test)\n",
    "train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use the `sklearn` class `LogisticRegression` to fit a logistic regression model on `Age` and log transformed `Fare`, scaling the features beforehand if necessary. Plot the decision boundary from the fit. Evaluate the classification accuracy of the model and compare to the performance of your model above. Note that the default logistic regression in `sklearn` is L2 regularised, so you may want to experiment with different levels of regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "features = [\"Age\", \"LogFare\"]\n",
    "\n",
    "x_train = scaler.fit_transform(X_train[features])\n",
    "x_test = scaler.transform(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "logistic_reg = LogisticRegression()\n",
    "logistic_reg.fit(x_train, Y_train)\n",
    "\n",
    "coef = logistic_reg.coef_\n",
    "intercept = logistic_reg.intercept_\n",
    "coef, intercept\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "decision_boundary = -(coef[0, 1] * x + intercept[0]) / coef[0, 0]\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(x=x_train[:, 1], y=x_train[:, 0], hue=Y_train)\n",
    "plt.plot(x, decision_boundary, c=\"black\", label=\"decision boundary\")\n",
    "plt.ylim((-0.1, 1.1))\n",
    "plt.title(\"Titanic survival\")\n",
    "plt.xlabel(\"log transformed fare\")\n",
    "plt.ylabel(\"age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = logistic_reg.score(x_train, Y_train)\n",
    "test_accuracy = logistic_reg.score(x_test, Y_test)\n",
    "train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial logistic regression\n",
    "\n",
    "We may be able to achieve higher accuracy by introducing higher order features. But as always with a more complex model, we risk overfitting the model to the training data, so we may have to adjust the strength of regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Engineer polynomial features in log `Fare` and/or `Age` and fit a logistic regression. You may want to experiment with the regularisation method and the strength of the regularisation parameter. Compare the contribution of each feature to the model prediction. Evaluate your model's performance, comparing to the performance of the model you developed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add higher order features and scale\n",
    "\n",
    "\n",
    "def add_poly_features(data: pd.DataFrame, feature: str, max_power: int) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "    for n in range(2, max_power + 1):\n",
    "        df.loc[:, f\"{feature}{n}\"] = df[feature] ** n\n",
    "    return df\n",
    "\n",
    "\n",
    "MAX_POWER = 11\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features = [\"Age\", \"LogFare\"]\n",
    "\n",
    "x_train = add_poly_features(X_train[features], \"LogFare\", MAX_POWER)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "x_test = add_poly_features(X_test[features], \"LogFare\", MAX_POWER)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and plot decision boundary\n",
    "\n",
    "regression = LogisticRegression(penalty=\"none\")\n",
    "regression.fit(x_train, Y_train)\n",
    "coef = regression.coef_\n",
    "intercept = regression.intercept_\n",
    "\n",
    "\n",
    "x = pd.DataFrame({\"Age\": np.zeros((100,)), \"LogFare\": np.linspace(2, 7, 100)})\n",
    "x = add_poly_features(x, \"LogFare\", MAX_POWER)\n",
    "x = scaler.transform(x)\n",
    "decision_boundary = -intercept[0] / coef[0, 0]\n",
    "for n in range(1, MAX_POWER + 1):\n",
    "    decision_boundary += -(coef[0, n] / coef[0, 0]) * x[:, n]\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(x=x_train[:, 1], y=x_train[:, 0], hue=Y_train)\n",
    "plt.plot(x[:, 1], decision_boundary, c=\"black\", label=\"decision boundary\")\n",
    "plt.ylim((-0.1, 1.2))\n",
    "plt.title(\"Titanic survival\")\n",
    "plt.xlabel(\"log transformed fare\")\n",
    "plt.ylabel(\"age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = regression.score(x_train, Y_train)\n",
    "test_accuracy = regression.score(x_test, Y_test)\n",
    "train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension: develop your own regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now that we've demonstrated the basics of logitstic regression, develop your own model with all the features in the dataset at your disposal. Compare the performance of your model to the peformance of the models we produced above. Keep in mind the following tips as you train and evaluate your model.\n",
    "* Consider one-hot-encoding categorical features.\n",
    "* Features on different scales should be transformed so that they're comparable.\n",
    "* If tuning a hyperparameter like the regularisation parameter, it's best practice to further divide your training set into training and validation sets or to cross validate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
